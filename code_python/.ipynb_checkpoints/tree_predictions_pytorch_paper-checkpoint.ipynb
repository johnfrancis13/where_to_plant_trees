{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bff69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in necessary packages\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "from osgeo import gdal\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "\n",
    "from torchinfo import summary\n",
    "import gc\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchmetrics.classification import BinaryJaccardIndex\n",
    "from torchmetrics import MeanAbsoluteError\n",
    "from torchmetrics.functional import dice_score\n",
    "from scipy.ndimage import convolve\n",
    "from tqdm.auto import tqdm # progress bar\n",
    "from timeit import default_timer as timer\n",
    "def print_train_time(start:float,\n",
    "                    end:float,\n",
    "                    device: torch.device= None):\n",
    "    total_time=end-start\n",
    "    print(f\"Train time on {device} : {total_time:.3f} seconds\")\n",
    "    return total_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a71874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in 2017 data for training\n",
    "folder=\"2017_cleaned/\" # 9533 files\n",
    "filelist_new = []\n",
    "\n",
    "# Load the images, and append them to a list.\n",
    "for filepath in os.listdir(folder):\n",
    "    if filepath.endswith((\".npy\")):\n",
    "        #print(filepath)\n",
    "        tempfile=folder+'/{0}'.format(filepath)\n",
    "        filelist_new.append(tempfile)\n",
    "\n",
    "len(filelist_new) # 9533 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopper function\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "# Split images into train/test\n",
    "filelist_train ,filelist_test = train_test_split(filelist_new,test_size=0.2,random_state=42)\n",
    "len(filelist_train) , len(filelist_test) # (7626, 1907)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a705132",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Dataloaders using the file paths ###\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create two DataLoaders, one for training and one for test\n",
    "class allbands_dataset_train(Dataset):\n",
    "    def __init__(self,filelist_train, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filelist (string): List with all of the file paths\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.filelist = filelist_train           \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filelist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()    \n",
    "            \n",
    "        # Generate data\n",
    "        dataset = np.load(self.filelist[idx])\n",
    "        \n",
    "        # X\n",
    "        X=dataset[:14] # separate out the band values\n",
    "\n",
    "        # canopy_height,tree/not tree,ndvi\n",
    "        out_tree_height = dataset[14]         \n",
    "        out_tree_mask = dataset[15]\n",
    "        \n",
    "        preds=[out_tree_height, out_tree_mask]\n",
    "        \n",
    "        return [X,preds]\n",
    "    \n",
    "class allbands_dataset_test(Dataset):\n",
    "    def __init__(self,filelist_test, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filelist (string): List with all of the file paths\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.filelist = filelist_test           \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filelist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()    \n",
    "        # Generate data\n",
    "        dataset = np.load(self.filelist[idx])\n",
    "        \n",
    "        # X\n",
    "        X=dataset[:14] # separate out the band values\n",
    "\n",
    "        # canopy_height,tree/not tree,ndvi\n",
    "        out_tree_height = dataset[14]         \n",
    "        out_tree_mask = dataset[15]\n",
    "        \n",
    "        preds=[out_tree_height, out_tree_mask]\n",
    "        \n",
    "        return [X,preds]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c827e8c3",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "- 6 models are run according to the paper; each task is run individually, together in a multi-task framework, with a manual vs auto loss stragey, and for all layers shared or only the encoder layers shared\n",
    "\n",
    "1. Multi Task Auto Loss (All Shared)\n",
    "2. Single Task Height\n",
    "3. Single Task Tree Cover\n",
    "4. Multi Task Manual Loss (All Shared)\n",
    "5. Multi Task Manual Loss (Not Shared Decoder)\n",
    "6. Multi Task Auto Loss Model (Not Shared Decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a1d07",
   "metadata": {},
   "source": [
    "# Multi Task Auto Loss (All Shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc3edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE UNET MODEL \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def defineUNetModel_fullyshared():\n",
    "    def double_conv0(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )        \n",
    "    \n",
    "    def double_conv(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def double_conv2(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    class UNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            self.dconv_down1 = double_conv0(14, 32)\n",
    "            self.dconv_down2 = double_conv(32, 64)\n",
    "            self.dconv_down3 = double_conv(64, 128)\n",
    "            self.dconv_down4 = nn.Sequential(\n",
    "                nn.Conv2d(128, 256, 3, padding=\"same\"),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.Dropout(p=0.2),\n",
    "                nn.Conv2d(256, 256, 3, padding=\"same\"),\n",
    "                nn.LeakyReLU(inplace=True))\n",
    "\n",
    "            self.maxpool = nn.MaxPool2d(2)\n",
    "            self.maxpool3 = nn.MaxPool2d(3)\n",
    "            \n",
    "            ## - looks to be the best solution, and appears to match old keras code\n",
    "            self.upsample1 = nn.ConvTranspose2d(256, 128, 2, stride=3, padding=0, output_padding=1)\n",
    "            self.upsample2 = nn.ConvTranspose2d(128, 64, 2, stride=2, padding=0)            \n",
    "            self.upsample3 = nn.ConvTranspose2d(64, 32, 2, stride=2, padding=0)\n",
    "            \n",
    "            \n",
    "            self.dconv_up3 = double_conv2(128+128, 128)\n",
    "            self.dconv_up2 = double_conv2(64 + 64, 64)\n",
    "            self.dconv_up1 = double_conv2(32 + 32, 32)\n",
    "\n",
    "            # Need 3 separate layers, otherwise they are all based on the same weight\n",
    "            self.conv_last1 = nn.Conv2d(32, 1, 1)\n",
    "            self.conv_last2 = nn.Conv2d(32, 1, 1)\n",
    "            self.conv_last3 = nn.Conv2d(32, 1, 1)\n",
    "            self.linear = nn.MaxPool2d(2)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            conv1 = self.dconv_down1(x)\n",
    "            x = self.maxpool(conv1)\n",
    "\n",
    "            conv2 = self.dconv_down2(x)\n",
    "            x = self.maxpool(conv2)\n",
    "\n",
    "            conv3 = self.dconv_down3(x)\n",
    "            x = self.maxpool3(conv3)\n",
    "\n",
    "            x = self.dconv_down4(x)\n",
    "            #x = self.maxpool(conv4)\n",
    "\n",
    "            x = self.upsample1(x)\n",
    "            x = torch.cat([x, conv3], dim=1)\n",
    "            x = self.dconv_up3(x)\n",
    "\n",
    "            x = self.upsample2(x)\n",
    "            x = torch.cat([x, conv2], dim=1)\n",
    "            x = self.dconv_up2(x)\n",
    "\n",
    "            x = self.upsample3(x)\n",
    "            x = torch.cat([x, conv1], dim=1)\n",
    "            x = self.dconv_up1(x)\n",
    "\n",
    "\n",
    "            out_tree_height = self.conv_last1(x) # looks like i don't need any additional activation here for linear\n",
    "            out_tree_mask = self.sigmoid(self.conv_last2(x))        \n",
    "            \n",
    "            return [out_tree_height, out_tree_mask]\n",
    "    model=UNet()\n",
    "    return model\n",
    "\n",
    "torch.manual_seed(42)\n",
    "unet_1 = defineUNetModel_fullyshared().to(device)\n",
    "unet_1, summary(unet_1,input_size= (1,14,240,240))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0b8f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto loss strategy\n",
    "#https://github.com/yaringal/multi-task-learning-example/blob/master/multi-task-learning-example-pytorch.ipynb\n",
    "log_var_a = torch.zeros((1,), requires_grad=True)\n",
    "log_var_b = torch.zeros((1,), requires_grad=True)\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "bce_loss = nn.BCELoss()\n",
    "\n",
    "# Remade my own version of the loss function\n",
    "def loss_criterion(y_pred, y_true, log_vars):\n",
    "    loss = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        precision = torch.exp(-log_vars[i])\n",
    "        if i==0:\n",
    "            diff = mse(y_pred[i], y_true[i])\n",
    "        else:\n",
    "            diff = bce_loss(y_pred[i], y_true[i])\n",
    "        loss += torch.sum(precision * diff + log_vars[i], -1)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "params_all = ([p for p in unet_1.parameters()] + [log_var_a] + [log_var_b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde6665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loss/optimizer/metrics\n",
    "# Opitimiser\n",
    "optimizer = optim.Adam(params=params_all,\n",
    "                     lr=.001)\n",
    "# Metrics\n",
    "mean_absolute_error = MeanAbsoluteError().to(device)\n",
    "iou_score = BinaryJaccardIndex().to(device)\n",
    "len(filelist_train),len(filelist_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d6789",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's \n",
    "    validation loss is less than the previous least less, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, best_valid_loss=float('inf')\n",
    "    ):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        \n",
    "    def __call__(\n",
    "        self, current_valid_loss, \n",
    "        epoch, model\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            torch.save(model.state_dict(), 'models/pytorch_paper_final/pytorch_mtloss_allshared.pt')\n",
    "\n",
    "save_best_model = SaveBestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53029963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### GOING TO TEST RUNNING MANY EPOCHS WITH A SMALLER PORTION OF MY DATA ###\n",
    "\n",
    "# Run a Train/Test Loop now\n",
    "# Build the training Loop (and a testing loop)\n",
    "torch.manual_seed(42)\n",
    "epochs = 100\n",
    "\n",
    "# Instatiate datasets/loaders\n",
    "my_dataset_train = allbands_dataset_train(filelist_train=filelist_train)\n",
    "my_dataset_test = allbands_dataset_test(filelist_test=filelist_test)\n",
    "\n",
    "my_dataloader_train = DataLoader(my_dataset_train, batch_size=16,shuffle=True, num_workers=0)\n",
    "my_dataloader_test = DataLoader(my_dataset_test, batch_size=16,shuffle=False, num_workers=0)\n",
    "\n",
    "log_var_a=log_var_a.to(device)\n",
    "log_var_b=log_var_b.to(device)\n",
    "\n",
    "batch_size= 16\n",
    "\n",
    "# track individual losses\n",
    "height_loss= []\n",
    "tmask_loss= []\n",
    "\n",
    "# track individual metrics\n",
    "height_mae= []\n",
    "tmask_iou= []\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.90483741803) \n",
    "early_stopper = EarlyStopper(patience=8, min_delta=0) # stop early if training loss does not improve after 10 epochs\n",
    "\n",
    "train_time_start_on_cpu = timer()\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1} out of {epochs}\")\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Loop through training batch data\n",
    "    for i_batch, sample_batched in enumerate(my_dataloader_train):\n",
    "        # Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X,Y= sample_batched\n",
    "        X= X.to(device)\n",
    "        Y[0]=Y[0].to(device) # probably a better way to do this....\n",
    "        Y[1]=Y[1].to(device)\n",
    "        \n",
    "        X = Variable(X.float().cuda())\n",
    "        Y[0] = Variable(Y[0].float().cuda())\n",
    "        Y[1] = Variable(Y[1].float().cuda())\n",
    "\n",
    "        unet_1.train()\n",
    "        # Forward Pass\n",
    "        pred_tree_height, pred_tree_mask = unet_1(X)\n",
    "        \n",
    "        # Calc loss (per batch) \n",
    "        loss = loss_criterion([pred_tree_height.squeeze(), pred_tree_mask.squeeze()],\n",
    "                             [Y[0],Y[1]],\n",
    "                             [log_var_a, log_var_b])\n",
    "        \n",
    "        train_loss += loss.item() # accumulate train loss\n",
    "        \n",
    "        # perform backpropagation on the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # performm gradient descent\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_batch % 200 == 0:\n",
    "            precision1 = torch.exp(-log_var_a)\n",
    "            diff1 = mse(pred_tree_height.squeeze(), Y[0])\n",
    "            th_loss = torch.sum(precision1 * diff1 + log_var_a, -1)\n",
    "        \n",
    "            precision2 = torch.exp(-log_var_b)\n",
    "            diff2 = bce_loss(pred_tree_mask.squeeze(), Y[1])\n",
    "            tm_loss = torch.sum(precision2 * diff2 + log_var_b, -1)\n",
    "        \n",
    "            \n",
    "            print(f\"Batch {i_batch+1} out of {len(my_dataloader_train)} completed.\", loss.item(),th_loss.item(),tm_loss.item())\n",
    "            height_loss.append(th_loss.item())\n",
    "            tmask_loss.append(tm_loss.item())\n",
    "\n",
    "    # Divide total train loss by length of dataloader\n",
    "    train_loss /= (len(my_dataset_train)/batch_size)\n",
    "        \n",
    "    ### Testing\n",
    "    test_loss, test_height_mae, test_tree_iou = 0,0,0\n",
    "    \n",
    "    unet_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        for i_batch, sample_batched in enumerate(my_dataloader_test):\n",
    "            X,Y= sample_batched\n",
    "            X= X.to(device)\n",
    "            \n",
    "            Y[0]=Y[0].to(device) # probably a better way to do this....\n",
    "            Y[1]=Y[1].to(device)\n",
    "            \n",
    "            X = Variable(X.float().cuda())\n",
    "            Y[0] = Variable(Y[0].float().cuda())\n",
    "            Y[1] = Variable(Y[1].float().cuda())\n",
    "\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_tree_height, pred_tree_mask = unet_1(X)\n",
    "            \n",
    "            # loss accumulate\n",
    "            test_loss += loss_criterion([pred_tree_height.squeeze(), pred_tree_mask.squeeze()],\n",
    "                                     [Y[0],Y[1].squeeze()],\n",
    "                                      [log_var_a, log_var_b]).item()\n",
    "\n",
    "            test_loss += loss.item() # accumulate train loss\n",
    "            \n",
    "            #Tree Height MAE\n",
    "            test_height_mae += mean_absolute_error(torch.squeeze(pred_tree_height),Y[0].squeeze())\n",
    "            \n",
    "            #Tree Mask IOU\n",
    "            test_tree_iou += iou_score(torch.squeeze(pred_tree_mask),Y[1].squeeze().type(torch.LongTensor).to(device))\n",
    "            \n",
    "                \n",
    "        # get loss per batch\n",
    "        test_loss /= (len(my_dataset_test)/batch_size)\n",
    "         \n",
    "        # get mae per batch\n",
    "        test_height_mae /= (len(my_dataset_test)/batch_size)\n",
    "        \n",
    "        # get iou1 per batch\n",
    "        test_tree_iou /= (len(my_dataset_test)/batch_size)\n",
    "        \n",
    "        \n",
    "        # save metrics\n",
    "        height_mae.append(test_height_mae.item())\n",
    "        tmask_iou.append(test_tree_iou.item())\n",
    "\n",
    "    lr=optimizer.param_groups[0][\"lr\"]    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Tree Height MAE: {test_height_mae:.5f} | Test Tree Mask IOU: {test_tree_iou:.5f} | Learning Rate: {lr:.10f}\")\n",
    "    save_best_model(test_loss, epoch, unet_1)\n",
    "    if epoch>1:\n",
    "        scheduler.step()  # every 10 decay learning rate\n",
    "    \n",
    "    if early_stopper.early_stop(test_loss):             \n",
    "        break\n",
    "\n",
    "train_time_end_on_cpu = timer()    \n",
    "total_train_time_on_cpu= print_train_time(start=train_time_start_on_cpu,\n",
    "                                          end=train_time_end_on_cpu,\n",
    "                                          device=str(next(unet_1.parameters()).device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5126642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in model\n",
    "unet_1 = defineUNetModel_fullyshared().to(device)\n",
    "unet_1.load_state_dict(torch.load(\"models/pytorch_paper_final/pytorch_mtloss_allshared.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9469eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# caluclate Tree Height MAE\n",
    "mean_absolute_error = MeanAbsoluteError(nan_strategy='ignore').to(device)\n",
    "my_dataset_test = allbands_dataset_test(filelist_test=filelist_test)\n",
    "my_dataloader_test = DataLoader(my_dataset_test, batch_size=16,shuffle=False, num_workers=0)\n",
    "test_height_mae = []\n",
    "\n",
    "batch_size=16\n",
    "unet_1.eval()\n",
    "with torch.inference_mode():\n",
    "    for i_batch, sample_batched in enumerate(my_dataloader_test):\n",
    "        X,Y= sample_batched\n",
    "        X= X.to(device)\n",
    "        \n",
    "        Y[0]=Y[0].to(device) # probably a better way to do this....\n",
    "        Y[1]=Y[1].to(device)\n",
    "        \n",
    "        \n",
    "        X = Variable(X.float().cuda())\n",
    "        Y[0] = Variable(Y[0].float().cuda())\n",
    "        Y[1] = Variable(Y[1].float().cuda())\n",
    "        \n",
    "        # Forward pass\n",
    "        pred_tree_height, pred_tree_mask = unet_1(X)\n",
    "        \n",
    "        \n",
    "        # mask predicted tree height with the tree mask\n",
    "        pred_tree_mask = custom_replace(pred_tree_mask, .4)\n",
    "        pred_tree_height[pred_tree_height  < 0 ] = 0\n",
    "        \n",
    "        pred_tree_height = torch.squeeze(pred_tree_height)*torch.squeeze(pred_tree_mask) #0s get rid of non tree pixels\n",
    "        \n",
    "        actual_tree_height= Y[0]*torch.squeeze(pred_tree_mask)\n",
    "        \n",
    "        #Height MAE\n",
    "        test_height_mae.append(mean_absolute_error(pred_tree_height,actual_tree_height).item())\n",
    "        \n",
    "\n",
    "# with torch.inference_mode():\n",
    "#     # Get Average Height MAE\n",
    "#     test_height_mae /= (len(my_dataset_test)/batch_size)\n",
    "print(\"Tree Height MAE=\",max(test_height_mae),min(test_height_mae),len(test_height_mae),sum(test_height_mae) / len(test_height_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56be772",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# caluclate Tree Mask IoU at various thresholds\n",
    "my_dataset_test = allbands_dataset_test(filelist_test=filelist_test)\n",
    "my_dataloader_test = DataLoader(my_dataset_test, batch_size=16,shuffle=False, num_workers=0)\n",
    "\n",
    "iou_score05 = BinaryJaccardIndex(threshold=.05).to(device)\n",
    "iou_score1 = BinaryJaccardIndex(threshold=.1).to(device)\n",
    "iou_score15 = BinaryJaccardIndex(threshold=.15).to(device)\n",
    "iou_score2 = BinaryJaccardIndex(threshold=.2).to(device)\n",
    "iou_score25 = BinaryJaccardIndex(threshold=.25).to(device)\n",
    "iou_score3 = BinaryJaccardIndex(threshold=.3).to(device)\n",
    "iou_score35 = BinaryJaccardIndex(threshold=.35).to(device)\n",
    "iou_score4 = BinaryJaccardIndex(threshold=.4).to(device)\n",
    "iou_score45 = BinaryJaccardIndex(threshold=.45).to(device)\n",
    "iou_score5 = BinaryJaccardIndex(threshold=.5).to(device)\n",
    "iou_score55 = BinaryJaccardIndex(threshold=.55).to(device)\n",
    "iou_score6 = BinaryJaccardIndex(threshold=.6).to(device)\n",
    "iou_score65 = BinaryJaccardIndex(threshold=.65).to(device)\n",
    "iou_score7 = BinaryJaccardIndex(threshold=.7).to(device)\n",
    "iou_score75 = BinaryJaccardIndex(threshold=.75).to(device)\n",
    "iou_score8 = BinaryJaccardIndex(threshold=.8).to(device)\n",
    "iou_score85 = BinaryJaccardIndex(threshold=.85).to(device)\n",
    "iou_score9 = BinaryJaccardIndex(threshold=.9).to(device)\n",
    "iou_score95 = BinaryJaccardIndex(threshold=.95).to(device)\n",
    "test_tree_iou05,test_tree_iou1,test_tree_iou15,test_tree_iou2,test_tree_iou25,test_tree_iou3,test_tree_iou35 = 0,0,0,0,0,0,0\n",
    "test_tree_iou4,test_tree_iou45,test_tree_iou5,test_tree_iou55,test_tree_iou6,test_tree_iou65,test_tree_iou7 = 0,0,0,0,0,0,0\n",
    "test_tree_iou75,test_tree_iou8,test_tree_iou85,test_tree_iou9 ,test_tree_iou95 = 0,0,0,0,0\n",
    "\n",
    "\n",
    "unet_1.eval()\n",
    "with torch.inference_mode():\n",
    "    for i_batch, sample_batched in enumerate(my_dataloader_test):\n",
    "        X,Y= sample_batched\n",
    "        X= X.to(device)\n",
    "        \n",
    "        Y[0]=Y[0].to(device) # probably a better way to do this....\n",
    "        Y[1]=Y[1].to(device)\n",
    "        \n",
    "        X = Variable(X.float().cuda())\n",
    "        Y[0] = Variable(Y[0].float().cuda())\n",
    "        Y[1] = Variable(Y[1].float().cuda())\n",
    "        \n",
    "        # Forward pass\n",
    "        pred_tree_height, pred_tree_mask = unet_1(X)\n",
    "        \n",
    "        #Tree Mask IOU\n",
    "        test_tree_iou05 += iou_score05(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou1  += iou_score1(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou15 += iou_score15(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou2  += iou_score2(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou25 += iou_score25(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou3  += iou_score3(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou35 += iou_score35(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou4  += iou_score4(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou45 += iou_score45(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou5  += iou_score5(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou55 += iou_score55(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou6  += iou_score6(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou65 += iou_score65(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou7  += iou_score7(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou75 += iou_score75(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou8  += iou_score8(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou85 += iou_score85(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou9  += iou_score9(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou95 += iou_score95(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        \n",
    "    \n",
    "# get iou1 per batch\n",
    "test_tree_iou05 = test_tree_iou05/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou1  = test_tree_iou1/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou15 = test_tree_iou15/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou2  = test_tree_iou2/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou25 = test_tree_iou25/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou3  = test_tree_iou3/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou35 = test_tree_iou35/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou4  = test_tree_iou4/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou45 = test_tree_iou45/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou5  = test_tree_iou5/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou55 = test_tree_iou55/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou6  = test_tree_iou6/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou65 = test_tree_iou65/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou7  = test_tree_iou7/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou75 = test_tree_iou75/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou8  = test_tree_iou8/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou85 = test_tree_iou85/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou9  = test_tree_iou9/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou95 = test_tree_iou95/(len(my_dataset_test)/batch_size)\n",
    "\n",
    "    \n",
    "print(\"Tree IoU .05=\",test_tree_iou05)\n",
    "print(\"Tree IoU .1 =\",test_tree_iou1 )\n",
    "print(\"Tree IoU .15=\",test_tree_iou15)\n",
    "print(\"Tree IoU .2 =\",test_tree_iou2 )\n",
    "print(\"Tree IoU .25=\",test_tree_iou25)\n",
    "print(\"Tree IoU .3 =\",test_tree_iou3 )\n",
    "print(\"Tree IoU .35=\",test_tree_iou35)\n",
    "print(\"Tree IoU .4 =\",test_tree_iou4 )\n",
    "print(\"Tree IoU .45=\",test_tree_iou45)\n",
    "print(\"Tree IoU .5 =\",test_tree_iou5 )\n",
    "print(\"Tree IoU .55=\",test_tree_iou55)\n",
    "print(\"Tree IoU .6 =\",test_tree_iou6 )\n",
    "print(\"Tree IoU .65=\",test_tree_iou65)\n",
    "print(\"Tree IoU .7 =\",test_tree_iou7 )\n",
    "print(\"Tree IoU .75=\",test_tree_iou75)\n",
    "print(\"Tree IoU .8 =\",test_tree_iou8 )\n",
    "print(\"Tree IoU .85=\",test_tree_iou85)\n",
    "print(\"Tree IoU .9 =\",test_tree_iou9 )\n",
    "print(\"Tree IoU .95=\",test_tree_iou95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddaca17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot Loss, MAE, IoU\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1,30001,345),height_loss,\"rx-\",label=\"tree height loss\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1,30001,345),tmask_loss,\"bx-\",label=\"tree mask loss\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1,30,1),height_mae,\"m--\",label=\"Tree height MAE\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1,30,1),tmask_iou,\"m--\",label=\"Tree Mask IoU\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "#tmask_loss\n",
    "#len(tnvdi_loss)\n",
    "\n",
    "# tota_loss = np.add(np.array(height_loss[0::5]), np.array(tmask_loss[0::5]))\n",
    "# tota_loss = np.add(tota_loss,np.array(tnvdi_loss[0::5]))\n",
    "tota_loss = np.add(np.array(height_loss), np.array(tmask_loss))\n",
    "\n",
    "plt.plot(range(1,30001,345),tota_loss,\"m--\",label=\"total loss\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68dc056",
   "metadata": {},
   "source": [
    "# Single Task Height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80765e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE UNET MODEL \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def defineUNetModel_fullyshared():\n",
    "    def double_conv0(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )  \n",
    "    def double_conv(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def double_conv2(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    class UNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            self.dconv_down1 = double_conv0(14, 32)\n",
    "            self.dconv_down2 = double_conv(32, 64)\n",
    "            self.dconv_down3 = double_conv(64, 128)\n",
    "            self.dconv_down4 = nn.Sequential(\n",
    "                nn.Conv2d(128, 256, 3, padding=\"same\"),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.Dropout(p=0.2),\n",
    "                nn.Conv2d(256, 256, 3, padding=\"same\"),\n",
    "                nn.LeakyReLU(inplace=True))\n",
    "\n",
    "            self.maxpool = nn.MaxPool2d(2)\n",
    "            self.maxpool3 = nn.MaxPool2d(3)\n",
    "            \n",
    "            ## - looks to be the best solution, and appears to match keras code\n",
    "            self.upsample1 = nn.ConvTranspose2d(256, 128, 2, stride=3, padding=0, output_padding=1)\n",
    "            self.upsample2 = nn.ConvTranspose2d(128, 64, 2, stride=2, padding=0)            \n",
    "            self.upsample3 = nn.ConvTranspose2d(64, 32, 2, stride=2, padding=0)\n",
    "            \n",
    "            \n",
    "            self.dconv_up3 = double_conv2(128+128, 128)\n",
    "            self.dconv_up2 = double_conv2(64 + 64, 64)\n",
    "            self.dconv_up1 = double_conv2(32 + 32, 32)\n",
    "\n",
    "            # Need 3 separate layers, otherwise they are all based on the same weight\n",
    "            self.conv_last1 = nn.Conv2d(32, 1, 1)\n",
    "            self.linear = nn.MaxPool2d(2)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            conv1 = self.dconv_down1(x)\n",
    "            x = self.maxpool(conv1)\n",
    "\n",
    "            conv2 = self.dconv_down2(x)\n",
    "            x = self.maxpool(conv2)\n",
    "\n",
    "            conv3 = self.dconv_down3(x)\n",
    "            x = self.maxpool3(conv3)\n",
    "\n",
    "            x = self.dconv_down4(x)\n",
    "            #x = self.maxpool(conv4)\n",
    "\n",
    "            x = self.upsample1(x)\n",
    "            x = torch.cat([x, conv3], dim=1)\n",
    "            x = self.dconv_up3(x)\n",
    "\n",
    "            x = self.upsample2(x)\n",
    "            x = torch.cat([x, conv2], dim=1)\n",
    "            x = self.dconv_up2(x)\n",
    "\n",
    "            x = self.upsample3(x)\n",
    "            x = torch.cat([x, conv1], dim=1)\n",
    "            x = self.dconv_up1(x)\n",
    "\n",
    "\n",
    "            out_tree_height = self.conv_last1(x) # looks like i don't need any additional activation here for linear\n",
    "\n",
    "            \n",
    "            return [out_tree_height]\n",
    "    model=UNet()\n",
    "    return model\n",
    "\n",
    "torch.manual_seed(42)\n",
    "unet_1 = defineUNetModel_fullyshared().to(device)\n",
    "unet_1, summary(unet_1,input_size= (1,14,240,240))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3eb5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loss/optimizer/metrics\n",
    "mse = nn.MSELoss()\n",
    "optimizer = optim.Adam(params=unet_1.parameters(),\n",
    "                     lr=.001)\n",
    "\n",
    "# Metrics\n",
    "mean_absolute_error = MeanAbsoluteError().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e37be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's \n",
    "    validation loss is less than the previous least less, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, best_valid_loss=float('inf')\n",
    "    ):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        \n",
    "    def __call__(\n",
    "        self, current_valid_loss, \n",
    "        epoch, model\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            torch.save(model.state_dict(), 'models/pytorch_paper_final/pytorch_single_treeheight.pt')\n",
    "\n",
    "save_best_model = SaveBestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a Train/Test Loop now\n",
    "# Build the training Loop (and a testing loop)\n",
    "torch.manual_seed(42)\n",
    "epochs = 50\n",
    "\n",
    "# Instatiate datasets/loaders\n",
    "my_dataset_train = allbands_dataset_train(filelist_train=filelist_train)\n",
    "my_dataset_test = allbands_dataset_test(filelist_test=filelist_test)\n",
    "\n",
    "my_dataloader_train = DataLoader(my_dataset_train, batch_size=16,shuffle=True, num_workers=0)\n",
    "my_dataloader_test = DataLoader(my_dataset_test, batch_size=16,shuffle=False, num_workers=0)\n",
    "batch_size= 16\n",
    "\n",
    "# track individual losses\n",
    "height_loss = []\n",
    "height_mae = []\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.90483741803) \n",
    "early_stopper = EarlyStopper(patience=7, min_delta=0) # stop early if training loss does not improve after 10 epochs\n",
    "\n",
    "train_time_start_on_cpu = timer()\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1} out of {epochs}\")\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Loop through training batch data\n",
    "    for i_batch, sample_batched in enumerate(my_dataloader_train):\n",
    "        # Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X,Y= sample_batched\n",
    "        X= X.to(device)\n",
    "        Y[0]=Y[0].to(device) # probably a better way to do this....\n",
    "        \n",
    "        X = Variable(X.float().cuda())\n",
    "        Y[0] = Variable(Y[0].float().cuda())\n",
    "\n",
    "        unet_1.train()\n",
    "        # Forward Pass\n",
    "        pred_tree_height = unet_1(X)\n",
    "        \n",
    "        # Calc loss (per batch) \n",
    "        loss= mse(pred_tree_height[0].squeeze(), Y[0])\n",
    "\n",
    "        train_loss += loss.item() # accumulate train loss\n",
    "\n",
    "        # perform backpropagation on the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # performm gradient descent\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_batch % 200 == 0:\n",
    "            print(f\"Batch {i_batch+1} out of {len(my_dataloader_train)} completed.\", loss.item())\n",
    "            height_loss.append(loss.item())\n",
    "\n",
    "\n",
    "    # Divide total train loss by length of dataloader\n",
    "    train_loss /= (len(my_dataset_train)/batch_size)\n",
    "        \n",
    "    ### Testing\n",
    "    test_loss, test_height_mae = 0,0\n",
    "    \n",
    "    unet_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        for i_batch, sample_batched in enumerate(my_dataloader_test):\n",
    "            X,Y= sample_batched\n",
    "            X= X.to(device)\n",
    "            \n",
    "            Y[0]=Y[0].to(device) # probably a better way to do this....\n",
    "            \n",
    "            X = Variable(X.float().cuda())\n",
    "            Y[0] = Variable(Y[0].float().cuda())\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_tree_height= unet_1(X)\n",
    "            \n",
    "            # loss accumulate\n",
    "            test_loss= mse(pred_tree_height[0].squeeze(), Y[0])\n",
    "\n",
    "\n",
    "            test_loss += loss.item() # accumulate train loss\n",
    "            \n",
    "            #Tree Height MAE\n",
    "            test_height_mae += mean_absolute_error(torch.squeeze(pred_tree_height[0]),Y[0])\n",
    "            \n",
    "                \n",
    "        # get loss per batch\n",
    "        test_loss /= (len(my_dataset_test)/batch_size)\n",
    "         \n",
    "        # get mae per batch\n",
    "        test_height_mae /= (len(my_dataset_test)/batch_size)\n",
    "        height_mae.append(test_height_mae.item())\n",
    "    lr=optimizer.param_groups[0][\"lr\"]        \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Tree Height MAE: {test_height_mae:.5f} | Learning Rate: {lr:.10f}\")\n",
    "    save_best_model(test_loss, epoch, unet_1)\n",
    "    if epoch>1:\n",
    "        scheduler.step()  # every 10 decay learning rate\n",
    "    if early_stopper.early_stop(test_loss):             \n",
    "        break\n",
    "\n",
    "train_time_end_on_cpu = timer()    \n",
    "total_train_time_on_cpu= print_train_time(start=train_time_start_on_cpu,\n",
    "                                          end=train_time_end_on_cpu,\n",
    "                                          device=str(next(unet_1.parameters()).device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "unet_1 = defineUNetModel_fullyshared().to(device)\n",
    "unet_1.load_state_dict(torch.load(\"models/pytorch_paper_final/pytorch_single_treeheight.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad36c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss/MAE\n",
    "plt.plot(range(1,51,1),height_loss,\"rx-\",label=\"tree height loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1,76,1),height_mae,\"m--\",label=\"mae\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0606a6",
   "metadata": {},
   "source": [
    "# Single Task Tree Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de24495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE UNET MODEL \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def defineUNetModel_fullyshared():\n",
    "    def double_conv0(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )    \n",
    "    def double_conv(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def double_conv2(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    class UNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            self.dconv_down1 = double_conv0(14, 32)\n",
    "            self.dconv_down2 = double_conv(32, 64)\n",
    "            self.dconv_down3 = double_conv(64, 128)\n",
    "            self.dconv_down4 = nn.Sequential(\n",
    "                nn.Conv2d(128, 256, 3, padding=\"same\"),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.Dropout(p=0.2),\n",
    "                nn.Conv2d(256, 256, 3, padding=\"same\"),\n",
    "                nn.LeakyReLU(inplace=True))\n",
    "\n",
    "            self.maxpool = nn.MaxPool2d(2)\n",
    "            self.maxpool3 = nn.MaxPool2d(3)\n",
    "            \n",
    "            ## - looks to be the best solution, and appears to match keras code\n",
    "            self.upsample1 = nn.ConvTranspose2d(256, 128, 2, stride=3, padding=0, output_padding=1)\n",
    "            self.upsample2 = nn.ConvTranspose2d(128, 64, 2, stride=2, padding=0)            \n",
    "            self.upsample3 = nn.ConvTranspose2d(64, 32, 2, stride=2, padding=0)\n",
    "            \n",
    "            \n",
    "            self.dconv_up3 = double_conv2(128+128, 128)\n",
    "            self.dconv_up2 = double_conv2(64 + 64, 64)\n",
    "            self.dconv_up1 = double_conv2(32 + 32, 32)\n",
    "\n",
    "            # Need 3 separate layers, otherwise they are all based on the same weight\n",
    "            self.conv_last2 = nn.Conv2d(32, 1, 1)\n",
    "            self.linear = nn.MaxPool2d(2)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            conv1 = self.dconv_down1(x)\n",
    "            x = self.maxpool(conv1)\n",
    "\n",
    "            conv2 = self.dconv_down2(x)\n",
    "            x = self.maxpool(conv2)\n",
    "\n",
    "            conv3 = self.dconv_down3(x)\n",
    "            x = self.maxpool3(conv3)\n",
    "\n",
    "            x = self.dconv_down4(x)\n",
    "            #x = self.maxpool(conv4)\n",
    "\n",
    "            x = self.upsample1(x)\n",
    "            x = torch.cat([x, conv3], dim=1)\n",
    "            x = self.dconv_up3(x)\n",
    "\n",
    "            x = self.upsample2(x)\n",
    "            x = torch.cat([x, conv2], dim=1)\n",
    "            x = self.dconv_up2(x)\n",
    "\n",
    "            x = self.upsample3(x)\n",
    "            x = torch.cat([x, conv1], dim=1)\n",
    "            x = self.dconv_up1(x)\n",
    "\n",
    "            out_tree_mask = self.sigmoid(self.conv_last2(x))        \n",
    "            \n",
    "            return [out_tree_mask]\n",
    "    model=UNet()\n",
    "    return model\n",
    "\n",
    "torch.manual_seed(42)\n",
    "unet_1 = defineUNetModel_fullyshared().to(device)\n",
    "unet_1, summary(unet_1,input_size= (16,14,240,240))\n",
    "### NOTES\n",
    "# Everything lines up with my keras model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c90f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loss/optimizer/metrics\n",
    "mse = nn.MSELoss()\n",
    "bce_loss = nn.BCELoss()\n",
    "\n",
    "iou_score = BinaryJaccardIndex().to(device)\n",
    "\n",
    "optimizer = optim.Adam(params=unet_1.parameters(),\n",
    "                     lr=.001)\n",
    "\n",
    "# Metrics\n",
    "mean_absolute_error = MeanAbsoluteError().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a06a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's \n",
    "    validation loss is less than the previous least less, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, best_valid_loss=float('inf')\n",
    "    ):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        \n",
    "    def __call__(\n",
    "        self, current_valid_loss, \n",
    "        epoch, model\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            torch.save(model.state_dict(), 'models/pytorch_paper_final/pytorch_single_treemask.pt')\n",
    "\n",
    "save_best_model = SaveBestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63686632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a Train/Test Loop now\n",
    "# Build the training Loop (and a testing loop)\n",
    "torch.manual_seed(42)\n",
    "epochs = 50\n",
    "\n",
    "# Instatiate datasets/loaders\n",
    "my_dataset_train = allbands_dataset_train(filelist_train=filelist_train)\n",
    "my_dataset_test = allbands_dataset_test(filelist_test=filelist_test)\n",
    "\n",
    "my_dataloader_train = DataLoader(my_dataset_train, batch_size=16,shuffle=True, num_workers=0)\n",
    "my_dataloader_test = DataLoader(my_dataset_test, batch_size=16,shuffle=False, num_workers=0)\n",
    "batch_size= 16\n",
    "\n",
    "# track individual losses\n",
    "tmask_loss = []\n",
    "mask_iou = []\n",
    "# Loss function\n",
    "bce_loss = nn.BCELoss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.90483741803) \n",
    "early_stopper = EarlyStopper(patience=10, min_delta=0) # stop early if training loss does not improve after 10 epochs\n",
    "best_test_loss = np.inf\n",
    "\n",
    "train_time_start_on_cpu = timer()\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1} out of {epochs}\")\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Loop through training batch data\n",
    "    for i_batch, sample_batched in enumerate(my_dataloader_train):\n",
    "        # Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X,Y= sample_batched\n",
    "        X= X.to(device)\n",
    "        Y[1]=Y[1].to(device) # probably a better way to do this....\n",
    "        \n",
    "        X = Variable(X.float().cuda())\n",
    "        Y[1] = Variable(Y[1].float().cuda())\n",
    "\n",
    "        unet_1.train()\n",
    "        # Forward Pass\n",
    "        pred_tree_mask = unet_1(X)\n",
    "        \n",
    "        # Calc loss (per batch) \n",
    "        loss= bce_loss(pred_tree_mask[0].squeeze(), Y[1])\n",
    "        \n",
    "        train_loss += loss.item() # accumulate train loss\n",
    "        \n",
    "        # perform backpropagation on the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # performm gradient descent\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_batch % 200 == 0:\n",
    "            print(f\"Batch {i_batch+1} out of {len(my_dataloader_train)} completed.\", loss.item())\n",
    "            tmask_loss.append(loss.item())\n",
    "\n",
    "\n",
    "    # Divide total train loss by length of dataloader\n",
    "    train_loss /= (len(my_dataset_train)/batch_size)\n",
    "        \n",
    "    ### Testing\n",
    "    test_loss, test_mask_iou = 0,0\n",
    "    \n",
    "    unet_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        for i_batch, sample_batched in enumerate(my_dataloader_test):\n",
    "            X,Y= sample_batched\n",
    "            X= X.to(device)\n",
    "            \n",
    "            Y[1]=Y[1].to(device) # probably a better way to do this....\n",
    "            \n",
    "            X = Variable(X.float().cuda())\n",
    "            Y[1] = Variable(Y[1].float().cuda())\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_tree_mask= unet_1(X)\n",
    "            \n",
    "            # loss accumulate\n",
    "            test_loss= bce_loss(pred_tree_mask[0].squeeze(), Y[1])\n",
    "\n",
    "            test_loss += loss.item() # accumulate train loss\n",
    "            \n",
    "            #Tree Mask IoU\n",
    "            test_mask_iou += iou_score(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "            mask_iou.append(test_mask_iou.item())\n",
    "                \n",
    "        # get loss per batch\n",
    "        test_loss /= (len(my_dataset_test)/batch_size)\n",
    "         \n",
    "        # get mae per batch\n",
    "        test_mask_iou /= (len(my_dataset_test)/batch_size)\n",
    "        \n",
    "    lr=optimizer.param_groups[0][\"lr\"]        \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Tree Mask IOU: {test_mask_iou:.2f} | Learning Rate: {lr:.10f}\")\n",
    "    save_best_model(test_loss, epoch, unet_1)\n",
    "    if epoch>1:\n",
    "        scheduler.step() \n",
    "    if early_stopper.early_stop(test_loss):             \n",
    "        break\n",
    "\n",
    "        \n",
    "train_time_end_on_cpu = timer()    \n",
    "total_train_time_on_cpu= print_train_time(start=train_time_start_on_cpu,\n",
    "                                          end=train_time_end_on_cpu,\n",
    "                                          device=str(next(unet_1.parameters()).device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7417779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss/IoU\n",
    "\n",
    "plt.plot(range(1,166,1),tmask_loss,\"rx-\",label=\"tree mask loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1,51,1),mask_iou[0::132],\"m--\",label=\"tree mask iou\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d22d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in best model\n",
    "unet_1 = defineUNetModel_fullyshared().to(device)\n",
    "unet_1.load_state_dict(torch.load(\"models/pytorch_paper_final/pytorch_single_treemask.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eda09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test out different thresholds to see which gives the best IoU\n",
    "my_dataset_test = allbands_dataset_test(filelist_test=filelist_test)\n",
    "my_dataloader_test = DataLoader(my_dataset_test, batch_size=16,shuffle=False, num_workers=0)\n",
    "\n",
    "iou_score05 = BinaryJaccardIndex(threshold=.05).to(device)\n",
    "iou_score1 = BinaryJaccardIndex(threshold=.1).to(device)\n",
    "iou_score15 = BinaryJaccardIndex(threshold=.15).to(device)\n",
    "iou_score2 = BinaryJaccardIndex(threshold=.2).to(device)\n",
    "iou_score25 = BinaryJaccardIndex(threshold=.25).to(device)\n",
    "iou_score3 = BinaryJaccardIndex(threshold=.3).to(device)\n",
    "iou_score35 = BinaryJaccardIndex(threshold=.35).to(device)\n",
    "iou_score4 = BinaryJaccardIndex(threshold=.4).to(device)\n",
    "iou_score45 = BinaryJaccardIndex(threshold=.45).to(device)\n",
    "iou_score5 = BinaryJaccardIndex(threshold=.5).to(device)\n",
    "iou_score55 = BinaryJaccardIndex(threshold=.55).to(device)\n",
    "iou_score6 = BinaryJaccardIndex(threshold=.6).to(device)\n",
    "iou_score65 = BinaryJaccardIndex(threshold=.65).to(device)\n",
    "iou_score7 = BinaryJaccardIndex(threshold=.7).to(device)\n",
    "iou_score75 = BinaryJaccardIndex(threshold=.75).to(device)\n",
    "iou_score8 = BinaryJaccardIndex(threshold=.8).to(device)\n",
    "iou_score85 = BinaryJaccardIndex(threshold=.85).to(device)\n",
    "iou_score9 = BinaryJaccardIndex(threshold=.9).to(device)\n",
    "iou_score95 = BinaryJaccardIndex(threshold=.95).to(device)\n",
    "test_tree_iou05,test_tree_iou1,test_tree_iou15,test_tree_iou2,test_tree_iou25,test_tree_iou3,test_tree_iou35 = 0,0,0,0,0,0,0\n",
    "test_tree_iou4,test_tree_iou45,test_tree_iou5,test_tree_iou55,test_tree_iou6,test_tree_iou65,test_tree_iou7 = 0,0,0,0,0,0,0\n",
    "test_tree_iou75,test_tree_iou8,test_tree_iou85,test_tree_iou9 ,test_tree_iou95 = 0,0,0,0,0\n",
    "\n",
    "\n",
    "unet_1.eval()\n",
    "with torch.inference_mode():\n",
    "    for i_batch, sample_batched in enumerate(my_dataloader_test):\n",
    "        X,Y= sample_batched\n",
    "        X= X.to(device)\n",
    "        \n",
    "        Y[0]=Y[0].to(device) # probably a better way to do this....\n",
    "        Y[1]=Y[1].to(device)\n",
    "        \n",
    "        X = Variable(X.float().cuda())\n",
    "        Y[0] = Variable(Y[0].float().cuda())\n",
    "        Y[1] = Variable(Y[1].float().cuda())\n",
    "        \n",
    "        # Forward pass\n",
    "        pred_tree_mask = unet_1(X)\n",
    "        \n",
    "        #Tree Mask IOU\n",
    "        test_tree_iou05 += iou_score05(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou1  += iou_score1(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou15 += iou_score15(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou2  += iou_score2(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou25 += iou_score25(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou3  += iou_score3(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou35 += iou_score35(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou4  += iou_score4(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou45 += iou_score45(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou5  += iou_score5(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou55 += iou_score55(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou6  += iou_score6(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou65 += iou_score65(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou7  += iou_score7(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou75 += iou_score75(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou8  += iou_score8(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou85 += iou_score85(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou9  += iou_score9(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou95 += iou_score95(pred_tree_mask[0].squeeze(),Y[1].type(torch.LongTensor).to(device))\n",
    "        \n",
    "    \n",
    "# get iou1 per batch\n",
    "test_tree_iou05 = test_tree_iou05/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou1  = test_tree_iou1/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou15 = test_tree_iou15/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou2  = test_tree_iou2/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou25 = test_tree_iou25/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou3  = test_tree_iou3/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou35 = test_tree_iou35/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou4  = test_tree_iou4/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou45 = test_tree_iou45/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou5  = test_tree_iou5/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou55 = test_tree_iou55/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou6  = test_tree_iou6/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou65 = test_tree_iou65/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou7  = test_tree_iou7/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou75 = test_tree_iou75/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou8  = test_tree_iou8/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou85 = test_tree_iou85/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou9  = test_tree_iou9/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou95 = test_tree_iou95/(len(my_dataset_test)/batch_size)\n",
    "\n",
    "    \n",
    "print(\"Tree IoU .05=\",test_tree_iou05)\n",
    "print(\"Tree IoU .1 =\",test_tree_iou1 )\n",
    "print(\"Tree IoU .15=\",test_tree_iou15)\n",
    "print(\"Tree IoU .2 =\",test_tree_iou2 )\n",
    "print(\"Tree IoU .25=\",test_tree_iou25)\n",
    "print(\"Tree IoU .3 =\",test_tree_iou3 )\n",
    "print(\"Tree IoU .35=\",test_tree_iou35)\n",
    "print(\"Tree IoU .4 =\",test_tree_iou4 )\n",
    "print(\"Tree IoU .45=\",test_tree_iou45)\n",
    "print(\"Tree IoU .5 =\",test_tree_iou5 )\n",
    "print(\"Tree IoU .55=\",test_tree_iou55)\n",
    "print(\"Tree IoU .6 =\",test_tree_iou6 )\n",
    "print(\"Tree IoU .65=\",test_tree_iou65)\n",
    "print(\"Tree IoU .7 =\",test_tree_iou7 )\n",
    "print(\"Tree IoU .75=\",test_tree_iou75)\n",
    "print(\"Tree IoU .8 =\",test_tree_iou8 )\n",
    "print(\"Tree IoU .85=\",test_tree_iou85)\n",
    "print(\"Tree IoU .9 =\",test_tree_iou9 )\n",
    "print(\"Tree IoU .95=\",test_tree_iou95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c549a",
   "metadata": {},
   "source": [
    "# Muti-task Manual Loss- All Shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE UNET MODEL \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def defineUNetModel_fullyshared():\n",
    "    def double_conv0(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )  \n",
    "    def double_conv(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def double_conv2(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    class UNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            self.dconv_down1 = double_conv0(14, 32)\n",
    "            self.dconv_down2 = double_conv(32, 64)\n",
    "            self.dconv_down3 = double_conv(64, 128)\n",
    "            self.dconv_down4 = nn.Sequential(\n",
    "                nn.Conv2d(128, 256, 3, padding=\"same\"),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.Dropout(p=0.2),\n",
    "                nn.Conv2d(256, 256, 3, padding=\"same\"),\n",
    "                nn.LeakyReLU(inplace=True))\n",
    "\n",
    "            self.maxpool = nn.MaxPool2d(2)\n",
    "            self.maxpool3 = nn.MaxPool2d(3)\n",
    "            \n",
    "            ## - looks to be the best solution, and appears to match keras code\n",
    "            self.upsample1 = nn.ConvTranspose2d(256, 128, 2, stride=3, padding=0, output_padding=1)\n",
    "            self.upsample2 = nn.ConvTranspose2d(128, 64, 2, stride=2, padding=0)            \n",
    "            self.upsample3 = nn.ConvTranspose2d(64, 32, 2, stride=2, padding=0)\n",
    "            \n",
    "            \n",
    "            self.dconv_up3 = double_conv2(128+128, 128)\n",
    "            self.dconv_up2 = double_conv2(64 + 64, 64)\n",
    "            self.dconv_up1 = double_conv2(32 + 32, 32)\n",
    "\n",
    "            # Need 3 separate layers, otherwise they are all based on the same weight\n",
    "            self.conv_last1 = nn.Conv2d(32, 1, 1)\n",
    "            self.conv_last2 = nn.Conv2d(32, 1, 1)\n",
    "            self.conv_last3 = nn.Conv2d(32, 1, 1)\n",
    "            self.linear = nn.MaxPool2d(2)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            conv1 = self.dconv_down1(x)\n",
    "            x = self.maxpool(conv1)\n",
    "\n",
    "            conv2 = self.dconv_down2(x)\n",
    "            x = self.maxpool(conv2)\n",
    "\n",
    "            conv3 = self.dconv_down3(x)\n",
    "            x = self.maxpool3(conv3)\n",
    "\n",
    "            x = self.dconv_down4(x)\n",
    "            #x = self.maxpool(conv4)\n",
    "\n",
    "            x = self.upsample1(x)\n",
    "            x = torch.cat([x, conv3], dim=1)\n",
    "            x = self.dconv_up3(x)\n",
    "\n",
    "            x = self.upsample2(x)\n",
    "            x = torch.cat([x, conv2], dim=1)\n",
    "            x = self.dconv_up2(x)\n",
    "\n",
    "            x = self.upsample3(x)\n",
    "            x = torch.cat([x, conv1], dim=1)\n",
    "            x = self.dconv_up1(x)\n",
    "\n",
    "\n",
    "            out_tree_height = self.conv_last1(x) # looks like i don't need any additional activation here for linear\n",
    "            out_tree_mask = self.sigmoid(self.conv_last2(x))        \n",
    "            \n",
    "            return [out_tree_height, out_tree_mask]\n",
    "    model=UNet()\n",
    "    return model\n",
    "\n",
    "torch.manual_seed(42)\n",
    "unet_1 = defineUNetModel_fullyshared().to(device)\n",
    "unet_1, summary(unet_1,input_size= (1,14,240,240))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ee0002",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "bce_loss = nn.BCELoss()\n",
    "\n",
    "# Set up loss/optimizer/metrics\n",
    "# Opitimiser\n",
    "optimizer = optim.Adam(params=unet_1.parameters(),\n",
    "                     lr=.001)\n",
    "\n",
    "# Metrics\n",
    "mean_absolute_error = MeanAbsoluteError().to(device)\n",
    "iou_score = BinaryJaccardIndex().to(device)\n",
    "\n",
    "len(filelist_train),len(filelist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6b8d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's \n",
    "    validation loss is less than the previous least less, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, best_valid_loss=float('inf')\n",
    "    ):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        \n",
    "    def __call__(\n",
    "        self, current_valid_loss, \n",
    "        epoch, model\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            torch.save(model.state_dict(), 'models/pytorch_paper_final/pytorch_mtloss_allshared_manual.pt')\n",
    "\n",
    "save_best_model = SaveBestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad327ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a Train/Test Loop now\n",
    "# Build the training Loop (and a testing loop)\n",
    "torch.manual_seed(42)\n",
    "epochs = 50\n",
    "\n",
    "# Instatiate datasets/loaders\n",
    "my_dataset_train = allbands_dataset_train(filelist_train=filelist_train)\n",
    "my_dataset_test = allbands_dataset_test(filelist_test=filelist_test)\n",
    "\n",
    "my_dataloader_train = DataLoader(my_dataset_train, batch_size=16,shuffle=True, num_workers=0)\n",
    "my_dataloader_test = DataLoader(my_dataset_test, batch_size=16,shuffle=False, num_workers=0)\n",
    "\n",
    "batch_size= 16\n",
    "\n",
    "# track individual losses\n",
    "height_loss= []\n",
    "tmask_loss= []\n",
    "\n",
    "# track individual metrics\n",
    "height_mae= []\n",
    "tmask_iou= []\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.90483741803) \n",
    "early_stopper = EarlyStopper(patience=7, min_delta=0) # stop early if training loss does not improve after 10 epochs\n",
    "\n",
    "train_time_start_on_cpu = timer()\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1} out of {epochs}\")\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Loop through training batch data\n",
    "    for i_batch, sample_batched in enumerate(my_dataloader_train):\n",
    "        # Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X,Y= sample_batched\n",
    "        X= X.to(device)\n",
    "        Y[0]=Y[0].to(device) # probably a better way to do this....\n",
    "        Y[1]=Y[1].to(device)\n",
    "\n",
    "        \n",
    "        X = Variable(X.float().cuda())\n",
    "        Y[0] = Variable(Y[0].float().cuda())\n",
    "        Y[1] = Variable(Y[1].float().cuda())\n",
    "\n",
    "        unet_1.train()\n",
    "        # Forward Pass\n",
    "        pred_tree_height, pred_tree_mask = unet_1(X)\n",
    "        \n",
    "        # Calc loss (per batch)         \n",
    "        # CCAI Loss Weighting to replicate findings\n",
    "        loss_1= mse(pred_tree_height.squeeze(), Y[0])\n",
    "        loss_2= bce_loss(pred_tree_mask.squeeze(), Y[1])\n",
    "        loss = (loss_1*.6) + (loss_2*.5) \n",
    "        \n",
    "        train_loss += loss.item() # accumulate train loss\n",
    "        \n",
    "        # perform backpropagation on the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # performm gradient descent\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_batch % 200 == 0:\n",
    "            print(f\"Batch {i_batch+1} out of {len(my_dataloader_train)} completed.\", loss.item(),loss_1.item(),loss_2.item())\n",
    "            height_loss.append(loss_1.item())\n",
    "            tmask_loss.append(loss_2.item())\n",
    "\n",
    "    # Divide total train loss by length of dataloader\n",
    "    train_loss /= (len(my_dataset_train)/batch_size)\n",
    "        \n",
    "    ### Testing\n",
    "    test_loss, test_height_mae, test_tree_iou = 0,0,0\n",
    "    \n",
    "    unet_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        for i_batch, sample_batched in enumerate(my_dataloader_test):\n",
    "            X,Y= sample_batched\n",
    "            X= X.to(device)\n",
    "            \n",
    "            Y[0]=Y[0].to(device) # probably a better way to do this....\n",
    "            Y[1]=Y[1].to(device)\n",
    "\n",
    "            X = Variable(X.float().cuda())\n",
    "            Y[0] = Variable(Y[0].float().cuda())\n",
    "            Y[1] = Variable(Y[1].float().cuda())\n",
    "\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_tree_height, pred_tree_mask = unet_1(X)\n",
    "            \n",
    "            # loss accumulate\n",
    "            \n",
    "            # CCAI Loss Weighting to replicate findings\n",
    "            loss_1= mse(pred_tree_height.squeeze(), Y[0])\n",
    "            loss_2= bce_loss(pred_tree_mask.squeeze(), Y[1])\n",
    "            loss = (loss_1*.6) + (loss_2*.5)\n",
    "\n",
    "            test_loss += loss.item() # accumulate train loss\n",
    "            \n",
    "            #Tree Height MAE\n",
    "            test_height_mae += mean_absolute_error(torch.squeeze(pred_tree_height),Y[0])\n",
    "            \n",
    "            #Tree Mask IOU\n",
    "            test_tree_iou += iou_score(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "            \n",
    "        # get loss per batch\n",
    "        test_loss /= (len(my_dataset_test)/batch_size)\n",
    "         \n",
    "        # get mae per batch\n",
    "        test_height_mae /= (len(my_dataset_test)/batch_size)\n",
    "        \n",
    "        # get iou1 per batch\n",
    "        test_tree_iou /= (len(my_dataset_test)/batch_size)\n",
    "        \n",
    "        \n",
    "        # save metrics\n",
    "        height_mae.append(test_height_mae.item())\n",
    "        tmask_iou.append(test_tree_iou.item())\n",
    "\n",
    "    lr=optimizer.param_groups[0][\"lr\"]        \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Tree Height MAE: {test_height_mae:.5f} | Test Tree Mask IOU: {test_tree_iou:.5f} | Learning Rate: {lr:.10f}\")\n",
    "    save_best_model(test_loss, epoch, unet_1)\n",
    "    if epoch>1:\n",
    "        scheduler.step()  # every 10 decay learning rate\n",
    "    if early_stopper.early_stop(test_loss):             \n",
    "        break\n",
    "        \n",
    "        \n",
    "train_time_end_on_cpu = timer()    \n",
    "total_train_time_on_cpu= print_train_time(start=train_time_start_on_cpu,\n",
    "                                          end=train_time_end_on_cpu,\n",
    "                                          device=str(next(unet_1.parameters()).device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f47af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in best model\n",
    "unet_1 = defineUNetModel_fullyshared().to(device)\n",
    "unet_1.load_state_dict(torch.load(\"models/pytorch_paper_final/pytorch_mtloss_allshared_manual.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f30cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calc MAE\n",
    "mean_absolute_error = MeanAbsoluteError(nan_strategy='ignore').to(device)\n",
    "my_dataset_test = allbands_dataset_test(filelist_test=filelist_test)\n",
    "my_dataloader_test = DataLoader(my_dataset_test, batch_size=16,shuffle=False, num_workers=0)\n",
    "test_height_mae = []\n",
    "\n",
    "batch_size=16\n",
    "unet_1.eval()\n",
    "with torch.inference_mode():\n",
    "    for i_batch, sample_batched in enumerate(my_dataloader_test):\n",
    "        X,Y= sample_batched\n",
    "        X= X.to(device)\n",
    "        \n",
    "        Y[0]=Y[0].to(device) # probably a better way to do this....\n",
    "        Y[1]=Y[1].to(device)\n",
    "        \n",
    "        \n",
    "        X = Variable(X.float().cuda())\n",
    "        Y[0] = Variable(Y[0].float().cuda())\n",
    "        Y[1] = Variable(Y[1].float().cuda())\n",
    "        \n",
    "        \n",
    "        # Forward pass\n",
    "        pred_tree_height, pred_tree_mask = unet_1(X)\n",
    "        \n",
    "        \n",
    "        # mask predicted tree height with the tree mask\n",
    "        pred_tree_mask = custom_replace(pred_tree_mask, .4)\n",
    "        pred_tree_height[pred_tree_height  < 0 ] = 0\n",
    "        \n",
    "        pred_tree_height = torch.squeeze(pred_tree_height)*torch.squeeze(pred_tree_mask) #0s get rid of non tree pixels\n",
    "\n",
    "        \n",
    "        # now i only want to compare Y[0]\n",
    "        actual_tree_height= Y[0]*torch.squeeze(pred_tree_mask)\n",
    "\n",
    "        #Height MAE\n",
    "        test_height_mae.append(mean_absolute_error(pred_tree_height,actual_tree_height).item())\n",
    "        \n",
    "\n",
    "# with torch.inference_mode():\n",
    "#     # Get Average Height MAE\n",
    "#     test_height_mae /= (len(my_dataset_test)/batch_size)\n",
    "print(\"Tree Height MAE=\",max(test_height_mae),min(test_height_mae),len(test_height_mae),sum(test_height_mae) / len(test_height_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b5ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test various thresholds for IoU\n",
    "my_dataset_test = allbands_dataset_test(filelist_test=filelist_test)\n",
    "my_dataloader_test = DataLoader(my_dataset_test, batch_size=16,shuffle=False, num_workers=0)\n",
    "\n",
    "iou_score05 = BinaryJaccardIndex(threshold=.05).to(device)\n",
    "iou_score1 = BinaryJaccardIndex(threshold=.1).to(device)\n",
    "iou_score15 = BinaryJaccardIndex(threshold=.15).to(device)\n",
    "iou_score2 = BinaryJaccardIndex(threshold=.2).to(device)\n",
    "iou_score25 = BinaryJaccardIndex(threshold=.25).to(device)\n",
    "iou_score3 = BinaryJaccardIndex(threshold=.3).to(device)\n",
    "iou_score35 = BinaryJaccardIndex(threshold=.35).to(device)\n",
    "iou_score4 = BinaryJaccardIndex(threshold=.4).to(device)\n",
    "iou_score45 = BinaryJaccardIndex(threshold=.45).to(device)\n",
    "iou_score5 = BinaryJaccardIndex(threshold=.5).to(device)\n",
    "iou_score55 = BinaryJaccardIndex(threshold=.55).to(device)\n",
    "iou_score6 = BinaryJaccardIndex(threshold=.6).to(device)\n",
    "iou_score65 = BinaryJaccardIndex(threshold=.65).to(device)\n",
    "iou_score7 = BinaryJaccardIndex(threshold=.7).to(device)\n",
    "iou_score75 = BinaryJaccardIndex(threshold=.75).to(device)\n",
    "iou_score8 = BinaryJaccardIndex(threshold=.8).to(device)\n",
    "iou_score85 = BinaryJaccardIndex(threshold=.85).to(device)\n",
    "iou_score9 = BinaryJaccardIndex(threshold=.9).to(device)\n",
    "iou_score95 = BinaryJaccardIndex(threshold=.95).to(device)\n",
    "test_tree_iou05,test_tree_iou1,test_tree_iou15,test_tree_iou2,test_tree_iou25,test_tree_iou3,test_tree_iou35 = 0,0,0,0,0,0,0\n",
    "test_tree_iou4,test_tree_iou45,test_tree_iou5,test_tree_iou55,test_tree_iou6,test_tree_iou65,test_tree_iou7 = 0,0,0,0,0,0,0\n",
    "test_tree_iou75,test_tree_iou8,test_tree_iou85,test_tree_iou9 ,test_tree_iou95 = 0,0,0,0,0\n",
    "\n",
    "\n",
    "unet_1.eval()\n",
    "with torch.inference_mode():\n",
    "    for i_batch, sample_batched in enumerate(my_dataloader_test):\n",
    "        X,Y= sample_batched\n",
    "        X= X.to(device)\n",
    "        \n",
    "        Y[0]=Y[0].to(device) # probably a better way to do this....\n",
    "        Y[1]=Y[1].to(device)\n",
    "        \n",
    "        X = Variable(X.float().cuda())\n",
    "        Y[0] = Variable(Y[0].float().cuda())\n",
    "        Y[1] = Variable(Y[1].float().cuda())\n",
    "        \n",
    "        # Forward pass\n",
    "        pred_tree_height, pred_tree_mask = unet_1(X)\n",
    "        \n",
    "        #Tree Mask IOU\n",
    "        test_tree_iou05 += iou_score05(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou1  += iou_score1(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou15 += iou_score15(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou2  += iou_score2(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou25 += iou_score25(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou3  += iou_score3(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou35 += iou_score35(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou4  += iou_score4(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou45 += iou_score45(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou5  += iou_score5(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou55 += iou_score55(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou6  += iou_score6(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou65 += iou_score65(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou7  += iou_score7(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou75 += iou_score75(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou8  += iou_score8(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou85 += iou_score85(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou9  += iou_score9(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou95 += iou_score95(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        \n",
    "    \n",
    "# get iou1 per batch\n",
    "test_tree_iou05 = test_tree_iou05/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou1  = test_tree_iou1/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou15 = test_tree_iou15/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou2  = test_tree_iou2/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou25 = test_tree_iou25/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou3  = test_tree_iou3/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou35 = test_tree_iou35/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou4  = test_tree_iou4/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou45 = test_tree_iou45/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou5  = test_tree_iou5/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou55 = test_tree_iou55/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou6  = test_tree_iou6/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou65 = test_tree_iou65/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou7  = test_tree_iou7/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou75 = test_tree_iou75/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou8  = test_tree_iou8/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou85 = test_tree_iou85/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou9  = test_tree_iou9/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou95 = test_tree_iou95/(len(my_dataset_test)/batch_size)\n",
    "\n",
    "    \n",
    "print(\"Tree IoU .05=\",test_tree_iou05)\n",
    "print(\"Tree IoU .1 =\",test_tree_iou1 )\n",
    "print(\"Tree IoU .15=\",test_tree_iou15)\n",
    "print(\"Tree IoU .2 =\",test_tree_iou2 )\n",
    "print(\"Tree IoU .25=\",test_tree_iou25)\n",
    "print(\"Tree IoU .3 =\",test_tree_iou3 )\n",
    "print(\"Tree IoU .35=\",test_tree_iou35)\n",
    "print(\"Tree IoU .4 =\",test_tree_iou4 )\n",
    "print(\"Tree IoU .45=\",test_tree_iou45)\n",
    "print(\"Tree IoU .5 =\",test_tree_iou5 )\n",
    "print(\"Tree IoU .55=\",test_tree_iou55)\n",
    "print(\"Tree IoU .6 =\",test_tree_iou6 )\n",
    "print(\"Tree IoU .65=\",test_tree_iou65)\n",
    "print(\"Tree IoU .7 =\",test_tree_iou7 )\n",
    "print(\"Tree IoU .75=\",test_tree_iou75)\n",
    "print(\"Tree IoU .8 =\",test_tree_iou8 )\n",
    "print(\"Tree IoU .85=\",test_tree_iou85)\n",
    "print(\"Tree IoU .9 =\",test_tree_iou9 )\n",
    "print(\"Tree IoU .95=\",test_tree_iou95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7923c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all the things\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1,30001,150),height_loss,\"rx-\",label=\"tree height loss\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1,30001,150),tmask_loss,\"bx-\",label=\"tree mask loss\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1,51,1),height_mae,\"m--\",label=\"Tree height MAE\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1,51,1),tmask_iou,\"m--\",label=\"Tree Mask IoU\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10875c25",
   "metadata": {},
   "source": [
    "# Muti-task Manual Loss- Not Shared Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7677cb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a version with shared encoding paths but different encoding paths\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def defineUNetModel_partiallyshared():\n",
    "    def double_conv0(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )  \n",
    "    def double_conv(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    class UNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            self.dconv_down1 = double_conv0(14, 32)\n",
    "            self.dconv_down2 = double_conv(32, 64)\n",
    "            self.dconv_down3 = double_conv(64, 128)\n",
    "            self.dconv_down4 = nn.Sequential(\n",
    "                nn.Conv2d(128, 256, 3, padding=1),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.Dropout(p=0.2),\n",
    "                nn.Conv2d(256, 256, 3, padding=1),\n",
    "                nn.LeakyReLU(inplace=True))\n",
    "\n",
    "            self.maxpool = nn.MaxPool2d(2)\n",
    "            self.maxpool3 = nn.MaxPool2d(3)\n",
    "            \n",
    "            self.upsample1a = nn.ConvTranspose2d(256, 128, 3, stride=3, padding=0)\n",
    "            self.upsample1b = nn.ConvTranspose2d(256, 128, 3, stride=3, padding=0)\n",
    "            self.upsample1c = nn.ConvTranspose2d(256, 128, 3, stride=3, padding=0)\n",
    "            \n",
    "            self.upsample2a = nn.ConvTranspose2d(128, 64, 2, stride=2, padding=0)\n",
    "            self.upsample2b = nn.ConvTranspose2d(128, 64, 2, stride=2, padding=0)\n",
    "            self.upsample2c = nn.ConvTranspose2d(128, 64, 2, stride=2, padding=0)\n",
    "            \n",
    "            self.upsample3a = nn.ConvTranspose2d(64, 32, 2, stride=2, padding=0)\n",
    "            self.upsample3b = nn.ConvTranspose2d(64, 32, 2, stride=2, padding=0)\n",
    "            self.upsample3c = nn.ConvTranspose2d(64, 32, 2, stride=2, padding=0)\n",
    "            \n",
    "            \n",
    "            self.dconv_up3a = double_conv(128+128, 128)\n",
    "            self.dconv_up3b = double_conv(128+128, 128)\n",
    "            self.dconv_up3c = double_conv(128+128, 128)\n",
    "            \n",
    "            self.dconv_up2a = double_conv(64 + 64, 64)\n",
    "            self.dconv_up2b = double_conv(64 + 64, 64)\n",
    "            self.dconv_up2c = double_conv(64 + 64, 64)\n",
    "            \n",
    "            self.dconv_up1a = double_conv(32 + 32, 32)\n",
    "            self.dconv_up1b = double_conv(32 + 32, 32)\n",
    "            self.dconv_up1c = double_conv(32 + 32, 32)\n",
    "           # self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "\n",
    "            self.conv_lasta = nn.Conv2d(32, 1, 1)\n",
    "            self.conv_lastb = nn.Conv2d(32, 1, 1)\n",
    "            self.conv_lastc = nn.Conv2d(32, 1, 1)\n",
    "                    \n",
    "            self.linear = nn.MaxPool2d(2)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "        def forward(self, x):\n",
    "            conv1 = self.dconv_down1(x)\n",
    "            x = self.maxpool(conv1)\n",
    "\n",
    "            conv2 = self.dconv_down2(x)\n",
    "            x = self.maxpool(conv2)\n",
    "\n",
    "            conv3 = self.dconv_down3(x)\n",
    "            x = self.maxpool3(conv3)\n",
    "\n",
    "            encoder_end = self.dconv_down4(x)\n",
    "            #x = self.maxpool(conv4)\n",
    "            \n",
    "            # Now the model should split into three branches\n",
    "\n",
    "            # Tree Height\n",
    "            x1 = self.upsample1a(encoder_end)\n",
    "            x1 = torch.cat([x1, conv3], dim=1)\n",
    "            x1 = self.dconv_up3a(x1)\n",
    "\n",
    "            x1 = self.upsample2a(x1)\n",
    "            x1 = torch.cat([x1, conv2], dim=1)\n",
    "            x1 = self.dconv_up2a(x1)\n",
    "\n",
    "            x1 = self.upsample3a(x1)\n",
    "            x1 = torch.cat([x1, conv1], dim=1)\n",
    "            x1 = self.dconv_up1a(x1)\n",
    "            out_tree_height = self.conv_lasta(x1) # looks like i don't need any additional activation here for linear\n",
    "\n",
    "            # Tree Mask\n",
    "            x2 = self.upsample1b(encoder_end)\n",
    "            x2 = torch.cat([x2, conv3], dim=1)\n",
    "            x2 = self.dconv_up3b(x2)\n",
    "\n",
    "            x2 = self.upsample2b(x2)\n",
    "            x2 = torch.cat([x2, conv2], dim=1)\n",
    "            x2 = self.dconv_up2b(x2)\n",
    "\n",
    "            x2 = self.upsample3b(x2)\n",
    "            x2 = torch.cat([x2, conv1], dim=1)\n",
    "            x2 = self.dconv_up1b(x2)\n",
    "            out_tree_mask = self.sigmoid(self.conv_lastb(x2))            \n",
    "     \n",
    "            \n",
    "            return [out_tree_height, out_tree_mask]\n",
    "    model=UNet()\n",
    "    return model\n",
    "\n",
    "torch.manual_seed(42)\n",
    "unet_1 = defineUNetModel_partiallyshared().to(device)\n",
    "unet_1, summary(unet_1,input_size= (1,14,240,240))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7552fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loss/optimizer/metrics\n",
    "mse = nn.MSELoss()\n",
    "bce_loss = nn.BCELoss()\n",
    "\n",
    "# Opitimiser\n",
    "optimizer = optim.Adam(params=unet_1.parameters(),\n",
    "                       lr=.001)\n",
    "# Metrics\n",
    "mean_absolute_error = MeanAbsoluteError().to(device)\n",
    "iou_score = BinaryJaccardIndex().to(device)\n",
    "\n",
    "len(filelist_train),len(filelist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's \n",
    "    validation loss is less than the previous least less, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, best_valid_loss=float('inf')\n",
    "    ):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        \n",
    "    def __call__(\n",
    "        self, current_valid_loss, \n",
    "        epoch, model\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            torch.save(model.state_dict(), 'models/pytorch_paper_final/pytorch_mtloss_partshared_manual.pt')\n",
    "\n",
    "save_best_model = SaveBestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa338d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a Train/Test Loop now\n",
    "# Build the training Loop (and a testing loop)\n",
    "torch.manual_seed(42)\n",
    "epochs = 75\n",
    "\n",
    "# Instatiate datasets/loaders\n",
    "my_dataset_train = allbands_dataset_train(filelist_train=filelist_train)\n",
    "my_dataset_test = allbands_dataset_test(filelist_test=filelist_test)\n",
    "\n",
    "my_dataloader_train = DataLoader(my_dataset_train, batch_size=16,shuffle=True, num_workers=0)\n",
    "my_dataloader_test = DataLoader(my_dataset_test, batch_size=16,shuffle=False, num_workers=0)\n",
    "batch_size= 16\n",
    "\n",
    "# track individual losses\n",
    "height_loss= []\n",
    "tmask_loss= []\n",
    "\n",
    "# track individual metrics\n",
    "height_mae= []\n",
    "tmask_iou= []\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.90483741803) \n",
    "early_stopper = EarlyStopper(patience=8, min_delta=0) # stop early if training loss does not improve after 10 epochs\n",
    "\n",
    "train_time_start_on_cpu = timer()\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1} out of {epochs}\")\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Loop through training batch data\n",
    "    for i_batch, sample_batched in enumerate(my_dataloader_train):\n",
    "        # Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X,Y= sample_batched\n",
    "        X= X.to(device)\n",
    "        Y[0]=Y[0].to(device) # probably a better way to do this....\n",
    "        Y[1]=Y[1].to(device)\n",
    "        \n",
    "        X = Variable(X.float().cuda())\n",
    "        Y[0] = Variable(Y[0].float().cuda())\n",
    "        Y[1] = Variable(Y[1].float().cuda())\n",
    "\n",
    "        unet_1.train()\n",
    "        # Forward Pass\n",
    "        pred_tree_height, pred_tree_mask = unet_1(X)\n",
    "        \n",
    "        # Calc loss (per batch) \n",
    "        #CCAI Loss Weighting to replicate findings\n",
    "        loss_1= mse(pred_tree_height.squeeze(), Y[0])\n",
    "        loss_2= bce_loss(pred_tree_mask.squeeze(), Y[1])\n",
    "        loss = (loss_1*.6) + (loss_2*.5)\n",
    "\n",
    "        train_loss += loss.item() # accumulate train loss\n",
    "\n",
    "        # perform backpropagation on the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # performm gradient descent\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_batch % 200 == 0:          \n",
    "            print(f\"Batch {i_batch+1} out of {len(my_dataloader_train)} completed.\", loss.item(),loss_1.item(),loss_2.item())\n",
    "            height_loss.append(loss_1.item())\n",
    "            tmask_loss.append(loss_2.item())\n",
    "\n",
    "    # Divide total train loss by length of dataloader\n",
    "    train_loss /= (len(my_dataset_train)/batch_size)\n",
    "        \n",
    "    ### Testing\n",
    "    test_loss, test_height_mae, test_tree_iou = 0,0,0\n",
    "    \n",
    "    unet_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        for i_batch, sample_batched in enumerate(my_dataloader_test):\n",
    "            X,Y= sample_batched\n",
    "            X= X.to(device)\n",
    "            \n",
    "            Y[0]=Y[0].to(device) # probably a better way to do this....\n",
    "            Y[1]=Y[1].to(device)\n",
    "            \n",
    "            X = Variable(X.float().cuda())\n",
    "            Y[0] = Variable(Y[0].float().cuda())\n",
    "            Y[1] = Variable(Y[1].float().cuda())\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_tree_height, pred_tree_mask = unet_1(X)\n",
    "            \n",
    "            # loss accumulate\n",
    "            \n",
    "            #CCAI Loss Weighting to replicate findings\n",
    "            loss_1= mse(pred_tree_height.squeeze(), Y[0])\n",
    "            loss_2= bce_loss(pred_tree_mask.squeeze(), Y[1])\n",
    "            loss = (loss_1*.6) + (loss_2*.5)\n",
    "\n",
    "            test_loss += loss.item() # accumulate train loss\n",
    "            \n",
    "            #Tree Height MAE\n",
    "            test_height_mae += mean_absolute_error(torch.squeeze(pred_tree_height),Y[0])\n",
    "            \n",
    "            #Tree Mask IOU\n",
    "            test_tree_iou += iou_score(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "            \n",
    "            \n",
    "                \n",
    "        # get loss per batch\n",
    "        test_loss /= (len(my_dataset_test)/batch_size)\n",
    "         \n",
    "        # get mae per batch\n",
    "        test_height_mae /= (len(my_dataset_test)/batch_size)\n",
    "        \n",
    "        # get iou1 per batch\n",
    "        test_tree_iou /= (len(my_dataset_test)/batch_size)\n",
    "        \n",
    "        \n",
    "        # save metrics\n",
    "        height_mae.append(test_height_mae.item())\n",
    "        tmask_iou.append(test_tree_iou.item())\n",
    "\n",
    "    lr=optimizer.param_groups[0][\"lr\"]        \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Tree Height MAE: {test_height_mae:.5f} | Test Tree Mask IOU: {test_tree_iou:.5f} | Learning Rate: {lr:.10f}\")\n",
    "    save_best_model(test_loss, epoch, unet_1)\n",
    "    if epoch>1:\n",
    "        scheduler.step()  # every 10 decay learning rate\n",
    "    if early_stopper.early_stop(test_loss):             \n",
    "        break\n",
    "        \n",
    "        \n",
    "train_time_end_on_cpu = timer()    \n",
    "total_train_time_on_cpu= print_train_time(start=train_time_start_on_cpu,\n",
    "                                          end=train_time_end_on_cpu,\n",
    "                                          device=str(next(unet_1.parameters()).device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da938476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the best model\n",
    "unet_1 = defineUNetModel_partiallyshared().to(device)\n",
    "unet_1.load_state_dict(torch.load(\"models/pytorch_paper_final/pytorch_mtloss_partshared_manual.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa0b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# test various thresholds for IoU\n",
    "batch=16\n",
    "my_dataset_test = allbands_dataset_test(filelist_test=filelist_test)\n",
    "my_dataloader_test = DataLoader(my_dataset_test, batch_size=16,shuffle=False, num_workers=0)\n",
    "\n",
    "iou_score05 = BinaryJaccardIndex(threshold=.05).to(device)\n",
    "iou_score1 = BinaryJaccardIndex(threshold=.1).to(device)\n",
    "iou_score15 = BinaryJaccardIndex(threshold=.15).to(device)\n",
    "iou_score2 = BinaryJaccardIndex(threshold=.2).to(device)\n",
    "iou_score25 = BinaryJaccardIndex(threshold=.25).to(device)\n",
    "iou_score3 = BinaryJaccardIndex(threshold=.3).to(device)\n",
    "iou_score35 = BinaryJaccardIndex(threshold=.35).to(device)\n",
    "iou_score4 = BinaryJaccardIndex(threshold=.4).to(device)\n",
    "iou_score45 = BinaryJaccardIndex(threshold=.45).to(device)\n",
    "iou_score5 = BinaryJaccardIndex(threshold=.5).to(device)\n",
    "iou_score55 = BinaryJaccardIndex(threshold=.55).to(device)\n",
    "iou_score6 = BinaryJaccardIndex(threshold=.6).to(device)\n",
    "iou_score65 = BinaryJaccardIndex(threshold=.65).to(device)\n",
    "iou_score7 = BinaryJaccardIndex(threshold=.7).to(device)\n",
    "iou_score75 = BinaryJaccardIndex(threshold=.75).to(device)\n",
    "iou_score8 = BinaryJaccardIndex(threshold=.8).to(device)\n",
    "iou_score85 = BinaryJaccardIndex(threshold=.85).to(device)\n",
    "iou_score9 = BinaryJaccardIndex(threshold=.9).to(device)\n",
    "iou_score95 = BinaryJaccardIndex(threshold=.95).to(device)\n",
    "test_tree_iou05,test_tree_iou1,test_tree_iou15,test_tree_iou2,test_tree_iou25,test_tree_iou3,test_tree_iou35 = 0,0,0,0,0,0,0\n",
    "test_tree_iou4,test_tree_iou45,test_tree_iou5,test_tree_iou55,test_tree_iou6,test_tree_iou65,test_tree_iou7 = 0,0,0,0,0,0,0\n",
    "test_tree_iou75,test_tree_iou8,test_tree_iou85,test_tree_iou9 ,test_tree_iou95 = 0,0,0,0,0\n",
    "\n",
    "\n",
    "unet_1.eval()\n",
    "with torch.inference_mode():\n",
    "    for i_batch, sample_batched in enumerate(my_dataloader_test):\n",
    "        X,Y= sample_batched\n",
    "        X= X.to(device)\n",
    "        \n",
    "        Y[0]=Y[0].to(device) # probably a better way to do this....\n",
    "        Y[1]=Y[1].to(device)\n",
    "        \n",
    "        X = Variable(X.float().cuda())\n",
    "        Y[0] = Variable(Y[0].float().cuda())\n",
    "        Y[1] = Variable(Y[1].float().cuda())\n",
    "        \n",
    "        # Forward pass\n",
    "        pred_tree_height, pred_tree_mask = unet_1(X)\n",
    "        \n",
    "        #Tree Mask IOU\n",
    "        test_tree_iou05 += iou_score05(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou1  += iou_score1(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou15 += iou_score15(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou2  += iou_score2(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou25 += iou_score25(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou3  += iou_score3(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou35 += iou_score35(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou4  += iou_score4(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou45 += iou_score45(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou5  += iou_score5(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou55 += iou_score55(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou6  += iou_score6(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou65 += iou_score65(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou7  += iou_score7(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou75 += iou_score75(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou8  += iou_score8(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou85 += iou_score85(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou9  += iou_score9(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou95 += iou_score95(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        \n",
    "    \n",
    "# get iou1 per batch\n",
    "test_tree_iou05 = test_tree_iou05/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou1  = test_tree_iou1/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou15 = test_tree_iou15/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou2  = test_tree_iou2/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou25 = test_tree_iou25/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou3  = test_tree_iou3/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou35 = test_tree_iou35/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou4  = test_tree_iou4/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou45 = test_tree_iou45/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou5  = test_tree_iou5/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou55 = test_tree_iou55/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou6  = test_tree_iou6/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou65 = test_tree_iou65/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou7  = test_tree_iou7/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou75 = test_tree_iou75/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou8  = test_tree_iou8/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou85 = test_tree_iou85/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou9  = test_tree_iou9/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou95 = test_tree_iou95/(len(my_dataset_test)/batch_size)\n",
    "\n",
    "    \n",
    "print(\"Tree IoU .05=\",test_tree_iou05)\n",
    "print(\"Tree IoU .1 =\",test_tree_iou1 )\n",
    "print(\"Tree IoU .15=\",test_tree_iou15)\n",
    "print(\"Tree IoU .2 =\",test_tree_iou2 )\n",
    "print(\"Tree IoU .25=\",test_tree_iou25)\n",
    "print(\"Tree IoU .3 =\",test_tree_iou3 )\n",
    "print(\"Tree IoU .35=\",test_tree_iou35)\n",
    "print(\"Tree IoU .4 =\",test_tree_iou4 )\n",
    "print(\"Tree IoU .45=\",test_tree_iou45)\n",
    "print(\"Tree IoU .5 =\",test_tree_iou5 )\n",
    "print(\"Tree IoU .55=\",test_tree_iou55)\n",
    "print(\"Tree IoU .6 =\",test_tree_iou6 )\n",
    "print(\"Tree IoU .65=\",test_tree_iou65)\n",
    "print(\"Tree IoU .7 =\",test_tree_iou7 )\n",
    "print(\"Tree IoU .75=\",test_tree_iou75)\n",
    "print(\"Tree IoU .8 =\",test_tree_iou8 )\n",
    "print(\"Tree IoU .85=\",test_tree_iou85)\n",
    "print(\"Tree IoU .9 =\",test_tree_iou9 )\n",
    "print(\"Tree IoU .95=\",test_tree_iou95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba111ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot Losses/Metrics\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1,30001,203),height_loss,\"rx-\",label=\"tree height loss\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1,30001,203),tmask_loss,\"bx-\",label=\"tree mask loss\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1,50,1),height_mae,\"m--\",label=\"Tree height MAE\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1,50,1),tmask_iou,\"m--\",label=\"Tree Mask IoU\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "tota_loss = np.add(np.array(height_loss), np.array(tmask_loss))\n",
    "\n",
    "plt.plot(range(1,30001,203),tota_loss,\"m--\",label=\"total loss\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc59eb5",
   "metadata": {},
   "source": [
    "# Muti-task Auto Loss- Not Shared Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a6315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a version with shared encoding paths but different encoding paths\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def defineUNetModel_partiallyshared():\n",
    "    def double_conv0(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )  \n",
    "    def double_conv(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    class UNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            self.dconv_down1 = double_conv0(14, 32)\n",
    "            self.dconv_down2 = double_conv(32, 64)\n",
    "            self.dconv_down3 = double_conv(64, 128)\n",
    "            self.dconv_down4 = nn.Sequential(\n",
    "                nn.Conv2d(128, 256, 3, padding=1),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.Dropout(p=0.2),\n",
    "                nn.Conv2d(256, 256, 3, padding=1),\n",
    "                nn.LeakyReLU(inplace=True))\n",
    "\n",
    "            self.maxpool = nn.MaxPool2d(2)\n",
    "            self.maxpool3 = nn.MaxPool2d(3)\n",
    "            \n",
    "            self.upsample1a = nn.ConvTranspose2d(256, 128, 3, stride=3, padding=0)\n",
    "            self.upsample1b = nn.ConvTranspose2d(256, 128, 3, stride=3, padding=0)\n",
    "            self.upsample1c = nn.ConvTranspose2d(256, 128, 3, stride=3, padding=0)\n",
    "            \n",
    "            self.upsample2a = nn.ConvTranspose2d(128, 64, 2, stride=2, padding=0)\n",
    "            self.upsample2b = nn.ConvTranspose2d(128, 64, 2, stride=2, padding=0)\n",
    "            self.upsample2c = nn.ConvTranspose2d(128, 64, 2, stride=2, padding=0)\n",
    "            \n",
    "            self.upsample3a = nn.ConvTranspose2d(64, 32, 2, stride=2, padding=0)\n",
    "            self.upsample3b = nn.ConvTranspose2d(64, 32, 2, stride=2, padding=0)\n",
    "            self.upsample3c = nn.ConvTranspose2d(64, 32, 2, stride=2, padding=0)\n",
    "            \n",
    "            \n",
    "            self.dconv_up3a = double_conv(128+128, 128)\n",
    "            self.dconv_up3b = double_conv(128+128, 128)\n",
    "            self.dconv_up3c = double_conv(128+128, 128)\n",
    "            \n",
    "            self.dconv_up2a = double_conv(64 + 64, 64)\n",
    "            self.dconv_up2b = double_conv(64 + 64, 64)\n",
    "            self.dconv_up2c = double_conv(64 + 64, 64)\n",
    "            \n",
    "            self.dconv_up1a = double_conv(32 + 32, 32)\n",
    "            self.dconv_up1b = double_conv(32 + 32, 32)\n",
    "            self.dconv_up1c = double_conv(32 + 32, 32)\n",
    "           # self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "\n",
    "            self.conv_lasta = nn.Conv2d(32, 1, 1)\n",
    "            self.conv_lastb = nn.Conv2d(32, 1, 1)\n",
    "            self.conv_lastc = nn.Conv2d(32, 1, 1)\n",
    "                    \n",
    "            self.linear = nn.MaxPool2d(2)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "        def forward(self, x):\n",
    "            conv1 = self.dconv_down1(x)\n",
    "            x = self.maxpool(conv1)\n",
    "\n",
    "            conv2 = self.dconv_down2(x)\n",
    "            x = self.maxpool(conv2)\n",
    "\n",
    "            conv3 = self.dconv_down3(x)\n",
    "            x = self.maxpool3(conv3)\n",
    "\n",
    "            encoder_end = self.dconv_down4(x)\n",
    "            \n",
    "            # Now the model should split into three branches\n",
    "\n",
    "            # Tree Height\n",
    "            x1 = self.upsample1a(encoder_end)\n",
    "            x1 = torch.cat([x1, conv3], dim=1)\n",
    "            x1 = self.dconv_up3a(x1)\n",
    "\n",
    "            x1 = self.upsample2a(x1)\n",
    "            x1 = torch.cat([x1, conv2], dim=1)\n",
    "            x1 = self.dconv_up2a(x1)\n",
    "\n",
    "            x1 = self.upsample3a(x1)\n",
    "            x1 = torch.cat([x1, conv1], dim=1)\n",
    "            x1 = self.dconv_up1a(x1)\n",
    "            out_tree_height = self.conv_lasta(x1) # looks like i don't need any additional activation here for linear\n",
    "\n",
    "            # Tree Mask\n",
    "            x2 = self.upsample1b(encoder_end)\n",
    "            x2 = torch.cat([x2, conv3], dim=1)\n",
    "            x2 = self.dconv_up3b(x2)\n",
    "\n",
    "            x2 = self.upsample2b(x2)\n",
    "            x2 = torch.cat([x2, conv2], dim=1)\n",
    "            x2 = self.dconv_up2b(x2)\n",
    "\n",
    "            x2 = self.upsample3b(x2)\n",
    "            x2 = torch.cat([x2, conv1], dim=1)\n",
    "            x2 = self.dconv_up1b(x2)\n",
    "            out_tree_mask = self.sigmoid(self.conv_lastb(x2))                   \n",
    "            \n",
    "            return [out_tree_height, out_tree_mask]\n",
    "    model=UNet()\n",
    "    return model\n",
    "\n",
    "torch.manual_seed(42)\n",
    "unet_1 = defineUNetModel_partiallyshared().to(device)\n",
    "unet_1, summary(unet_1,input_size= (1,14,240,240))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5522aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/yaringal/multi-task-learning-example/blob/master/multi-task-learning-example-pytorch.ipynb\n",
    "log_var_a = torch.zeros((1,), requires_grad=True)\n",
    "log_var_b = torch.zeros((1,), requires_grad=True)\n",
    "mse = nn.MSELoss()\n",
    "bce_loss = nn.BCELoss()\n",
    "\n",
    "# Remade my own version of the loss function\n",
    "def loss_criterion(y_pred, y_true, log_vars):\n",
    "    loss = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        precision = torch.exp(-log_vars[i])\n",
    "        if i==0:\n",
    "            diff = mse(y_pred[i], y_true[i])\n",
    "        else:\n",
    "            diff = bce_loss(y_pred[i], y_true[i])\n",
    "        loss += torch.sum(precision * diff + log_vars[i], -1)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "params_all = ([p for p in unet_1.parameters()] + [log_var_a] + [log_var_b])\n",
    "\n",
    "\n",
    "# Set up loss/optimizer/metrics\n",
    "# Opitimiser\n",
    "optimizer = optim.Adam(params=params_all,\n",
    "                     lr=.001)\n",
    "\n",
    "# optimizer = optim.Adam(params=unet_1.parameters(),\n",
    "#                      lr=.001)\n",
    "\n",
    "# Metrics\n",
    "mean_absolute_error = MeanAbsoluteError().to(device)\n",
    "iou_score = BinaryJaccardIndex().to(device)\n",
    "\n",
    "\n",
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's \n",
    "    validation loss is less than the previous least less, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, best_valid_loss=float('inf')\n",
    "    ):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        \n",
    "    def __call__(\n",
    "        self, current_valid_loss, \n",
    "        epoch, model\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            torch.save(model.state_dict(), 'models/pytorch_paper_final/pytorch_mtloss_partshared.pt')\n",
    "\n",
    "save_best_model = SaveBestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eafaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a Train/Test Loop now\n",
    "# Build the training Loop (and a testing loop)\n",
    "torch.manual_seed(42)\n",
    "epochs = 50\n",
    "\n",
    "# Instatiate datasets/loaders\n",
    "my_dataset_train = allbands_dataset_train(filelist_train=filelist_train)\n",
    "my_dataset_test = allbands_dataset_test(filelist_test=filelist_test)\n",
    "\n",
    "my_dataloader_train = DataLoader(my_dataset_train, batch_size=16,shuffle=True, num_workers=0)\n",
    "my_dataloader_test = DataLoader(my_dataset_test, batch_size=16,shuffle=False, num_workers=0)\n",
    "log_var_a=log_var_a.to(device)\n",
    "log_var_b=log_var_b.to(device)\n",
    "\n",
    "batch_size= 16\n",
    "\n",
    "# track individual losses\n",
    "height_loss= []\n",
    "tmask_loss= []\n",
    "\n",
    "# track individual metrics\n",
    "height_mae= []\n",
    "tmask_iou= []\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.90483741803) \n",
    "early_stopper = EarlyStopper(patience=10, min_delta=0) # stop early if training loss does not improve after 10 epochs\n",
    "\n",
    "train_time_start_on_cpu = timer()\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1} out of {epochs}\")\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Loop through training batch data\n",
    "    for i_batch, sample_batched in enumerate(my_dataloader_train):\n",
    "        # Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X,Y= sample_batched\n",
    "        X= X.to(device)\n",
    "        Y[0]=Y[0].to(device) # probably a better way to do this....\n",
    "        Y[1]=Y[1].to(device)\n",
    "        \n",
    "        X = Variable(X.float().cuda())\n",
    "        Y[0] = Variable(Y[0].float().cuda())\n",
    "        Y[1] = Variable(Y[1].float().cuda())\n",
    "        \n",
    "        unet_1.train()\n",
    "        # Forward Pass\n",
    "        pred_tree_height, pred_tree_mask = unet_1(X)\n",
    "        \n",
    "        # Calc loss (per batch) \n",
    "        loss = loss_criterion([pred_tree_height.squeeze(), pred_tree_mask.squeeze()],\n",
    "                             [Y[0],Y[1]],\n",
    "                             [log_var_a, log_var_b])\n",
    "\n",
    "        train_loss += loss.item() # accumulate train loss\n",
    "\n",
    "        # perform backpropagation on the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # performm gradient descent\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_batch % 200 == 0:\n",
    "            precision1 = torch.exp(-log_var_a)\n",
    "            diff1 = mse(pred_tree_height.squeeze(), Y[0])\n",
    "            th_loss = torch.sum(precision1 * diff1 + log_var_a, -1)\n",
    "        \n",
    "            precision2 = torch.exp(-log_var_b)\n",
    "            diff2 = bce_loss(pred_tree_mask.squeeze(), Y[1])\n",
    "            tm_loss = torch.sum(precision2 * diff2 + log_var_b, -1)\n",
    "        \n",
    "            \n",
    "            print(f\"Batch {i_batch+1} out of {len(my_dataloader_train)} completed.\", loss.item(),th_loss.item(),tm_loss.item())\n",
    "            height_loss.append(th_loss.item())\n",
    "            tmask_loss.append(tm_loss.item())\n",
    "\n",
    "\n",
    "    # Divide total train loss by length of dataloader\n",
    "    train_loss /= (len(my_dataset_train)/batch_size)\n",
    "        \n",
    "    ### Testing\n",
    "    test_loss, test_height_mae, test_tree_iou, test_ndvi_iou = 0,0,0,0\n",
    "    \n",
    "    unet_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        for i_batch, sample_batched in enumerate(my_dataloader_test):\n",
    "            X,Y= sample_batched\n",
    "            X= X.to(device)\n",
    "            \n",
    "            Y[0]=Y[0].to(device) # probably a better way to do this....\n",
    "            Y[1]=Y[1].to(device)\n",
    "            \n",
    "            X = Variable(X.float().cuda())\n",
    "            Y[0] = Variable(Y[0].float().cuda())\n",
    "            Y[1] = Variable(Y[1].float().cuda())\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_tree_height, pred_tree_mask = unet_1(X)\n",
    "            \n",
    "            # loss accumulate\n",
    "            test_loss += loss_criterion([pred_tree_height.squeeze(), pred_tree_mask.squeeze()],\n",
    "                                     [Y[0],Y[1]],\n",
    "                                      [log_var_a, log_var_b]).item()\n",
    "            \n",
    "            test_loss += loss.item() # accumulate train loss\n",
    "            \n",
    "            #Tree Height MAE\n",
    "            test_height_mae += mean_absolute_error(torch.squeeze(pred_tree_height),Y[0])\n",
    "            \n",
    "            #Tree Mask IOU\n",
    "            test_tree_iou += iou_score(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "            \n",
    "            \n",
    "        # get loss per batch\n",
    "        test_loss /= (len(my_dataset_test)/batch_size)\n",
    "         \n",
    "        # get mae per batch\n",
    "        test_height_mae /= (len(my_dataset_test)/batch_size)\n",
    "        \n",
    "        # get iou1 per batch\n",
    "        test_tree_iou /= (len(my_dataset_test)/batch_size)\n",
    "\n",
    "        \n",
    "        # save metrics\n",
    "        height_mae.append(test_height_mae.item())\n",
    "        tmask_iou.append(test_tree_iou.item())\n",
    "    \n",
    "    lr=optimizer.param_groups[0][\"lr\"]        \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Tree Height MAE: {test_height_mae:.5f} | Test Tree Mask IOU: {test_tree_iou:.5f} | Learning Rate: {lr:.10f}\")\n",
    "    save_best_model(test_loss, epoch, unet_1)\n",
    "    if epoch>1:\n",
    "        scheduler.step()  # every 10 decay learning rate\n",
    "    if early_stopper.early_stop(test_loss):             \n",
    "        break\n",
    "        \n",
    "train_time_end_on_cpu = timer()    \n",
    "total_train_time_on_cpu= print_train_time(start=train_time_start_on_cpu,\n",
    "                                          end=train_time_end_on_cpu,\n",
    "                                          device=str(next(unet_1.parameters()).device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c56599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in best model\n",
    "unet_1 = defineUNetModel_partiallyshared().to(device)\n",
    "unet_1.load_state_dict(torch.load(\"models/pytorch_paper_final/pytorch_mtloss_partshared.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15bb164",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calc Mae\n",
    "mean_absolute_error = MeanAbsoluteError(nan_strategy='ignore').to(device)\n",
    "my_dataset_test = allbands_dataset_test(filelist_test=filelist_test)\n",
    "my_dataloader_test = DataLoader(my_dataset_test, batch_size=16,shuffle=False, num_workers=0)\n",
    "test_height_mae = []\n",
    "\n",
    "batch_size=16\n",
    "unet_1.eval()\n",
    "with torch.inference_mode():\n",
    "    for i_batch, sample_batched in enumerate(my_dataloader_test):\n",
    "        X,Y= sample_batched\n",
    "        X= X.to(device)\n",
    "        \n",
    "        Y[0]=Y[0].to(device) # probably a better way to do this....\n",
    "        Y[1]=Y[1].to(device)\n",
    "        \n",
    "        \n",
    "        X = Variable(X.float().cuda())\n",
    "        Y[0] = Variable(Y[0].float().cuda())\n",
    "        Y[1] = Variable(Y[1].float().cuda())\n",
    "        \n",
    "        # mask actual tree height with the tree mask\n",
    "        #https://stackoverflow.com/questions/58521595/masking-tensor-of-same-shape-in-pytorch\n",
    "        \n",
    "        # Forward pass\n",
    "        pred_tree_height, pred_tree_mask = unet_1(X)\n",
    "        \n",
    "        \n",
    "        # mask predicted tree height with the tree mask\n",
    "        pred_tree_mask = custom_replace(pred_tree_mask, .4)\n",
    "        pred_tree_height[pred_tree_height  < 0 ] = 0\n",
    "        \n",
    "        pred_tree_height = torch.squeeze(pred_tree_height)*torch.squeeze(pred_tree_mask) #0s get rid of non tree pixels\n",
    "#        mask_tensor1 =pred_tree_height>0\n",
    "        \n",
    "        # now i only want to compare Y[0]\n",
    "        #actual_tree_height= Y[0]*Y[1]\n",
    "        actual_tree_height= Y[0]*torch.squeeze(pred_tree_mask)\n",
    "        #mask_tensor2 =actual_tree_height>0\n",
    "        \n",
    "        # get values where either are identified as trees\n",
    "        #mask_tensor= torch.logical_and(pred_tree_mask, mask_tensor2)\n",
    "        \n",
    "        #actual_vals=torch.masked_select(actual_tree_height, mask_tensor)\n",
    "        #pred_vals=torch.masked_select(torch.squeeze(pred_tree_height),mask_tensor)\n",
    "        \n",
    "        #Height MAE\n",
    "        #test_height_mae += mean_absolute_error(pred_vals,actual_vals)\n",
    "        test_height_mae.append(mean_absolute_error(pred_tree_height,actual_tree_height).item())\n",
    "        \n",
    "\n",
    "# with torch.inference_mode():\n",
    "#     # Get Average Height MAE\n",
    "#     test_height_mae /= (len(my_dataset_test)/batch_size)\n",
    "print(\"Tree Height MAE=\",max(test_height_mae),min(test_height_mae),len(test_height_mae),sum(test_height_mae) / len(test_height_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8260ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# test IoU thresholds\n",
    "my_dataset_test = allbands_dataset_test(filelist_test=filelist_test)\n",
    "my_dataloader_test = DataLoader(my_dataset_test, batch_size=16,shuffle=False, num_workers=0)\n",
    "\n",
    "iou_score05 = BinaryJaccardIndex(threshold=.05).to(device)\n",
    "iou_score1 = BinaryJaccardIndex(threshold=.1).to(device)\n",
    "iou_score15 = BinaryJaccardIndex(threshold=.15).to(device)\n",
    "iou_score2 = BinaryJaccardIndex(threshold=.2).to(device)\n",
    "iou_score25 = BinaryJaccardIndex(threshold=.25).to(device)\n",
    "iou_score3 = BinaryJaccardIndex(threshold=.3).to(device)\n",
    "iou_score35 = BinaryJaccardIndex(threshold=.35).to(device)\n",
    "iou_score4 = BinaryJaccardIndex(threshold=.4).to(device)\n",
    "iou_score45 = BinaryJaccardIndex(threshold=.45).to(device)\n",
    "iou_score5 = BinaryJaccardIndex(threshold=.5).to(device)\n",
    "iou_score55 = BinaryJaccardIndex(threshold=.55).to(device)\n",
    "iou_score6 = BinaryJaccardIndex(threshold=.6).to(device)\n",
    "iou_score65 = BinaryJaccardIndex(threshold=.65).to(device)\n",
    "iou_score7 = BinaryJaccardIndex(threshold=.7).to(device)\n",
    "iou_score75 = BinaryJaccardIndex(threshold=.75).to(device)\n",
    "iou_score8 = BinaryJaccardIndex(threshold=.8).to(device)\n",
    "iou_score85 = BinaryJaccardIndex(threshold=.85).to(device)\n",
    "iou_score9 = BinaryJaccardIndex(threshold=.9).to(device)\n",
    "iou_score95 = BinaryJaccardIndex(threshold=.95).to(device)\n",
    "test_tree_iou05,test_tree_iou1,test_tree_iou15,test_tree_iou2,test_tree_iou25,test_tree_iou3,test_tree_iou35 = 0,0,0,0,0,0,0\n",
    "test_tree_iou4,test_tree_iou45,test_tree_iou5,test_tree_iou55,test_tree_iou6,test_tree_iou65,test_tree_iou7 = 0,0,0,0,0,0,0\n",
    "test_tree_iou75,test_tree_iou8,test_tree_iou85,test_tree_iou9 ,test_tree_iou95 = 0,0,0,0,0\n",
    "\n",
    "\n",
    "unet_1.eval()\n",
    "with torch.inference_mode():\n",
    "    for i_batch, sample_batched in enumerate(my_dataloader_test):\n",
    "        X,Y= sample_batched\n",
    "        X= X.to(device)\n",
    "        \n",
    "        Y[0]=Y[0].to(device) # probably a better way to do this....\n",
    "        Y[1]=Y[1].to(device)\n",
    "        \n",
    "        X = Variable(X.float().cuda())\n",
    "        Y[0] = Variable(Y[0].float().cuda())\n",
    "        Y[1] = Variable(Y[1].float().cuda())\n",
    "        \n",
    "        # Forward pass\n",
    "        pred_tree_height, pred_tree_mask = unet_1(X)\n",
    "        \n",
    "        #Tree Mask IOU\n",
    "        test_tree_iou05 += iou_score05(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou1  += iou_score1(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou15 += iou_score15(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou2  += iou_score2(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou25 += iou_score25(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou3  += iou_score3(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou35 += iou_score35(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou4  += iou_score4(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou45 += iou_score45(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou5  += iou_score5(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou55 += iou_score55(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou6  += iou_score6(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou65 += iou_score65(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou7  += iou_score7(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou75 += iou_score75(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou8  += iou_score8(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou85 += iou_score85(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou9  += iou_score9(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        test_tree_iou95 += iou_score95(torch.squeeze(pred_tree_mask),Y[1].type(torch.LongTensor).to(device))\n",
    "        \n",
    "    \n",
    "# get iou1 per batch\n",
    "test_tree_iou05 = test_tree_iou05/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou1  = test_tree_iou1/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou15 = test_tree_iou15/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou2  = test_tree_iou2/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou25 = test_tree_iou25/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou3  = test_tree_iou3/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou35 = test_tree_iou35/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou4  = test_tree_iou4/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou45 = test_tree_iou45/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou5  = test_tree_iou5/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou55 = test_tree_iou55/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou6  = test_tree_iou6/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou65 = test_tree_iou65/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou7  = test_tree_iou7/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou75 = test_tree_iou75/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou8  = test_tree_iou8/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou85 = test_tree_iou85/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou9  = test_tree_iou9/(len(my_dataset_test)/batch_size)\n",
    "test_tree_iou95 = test_tree_iou95/(len(my_dataset_test)/batch_size)\n",
    "\n",
    "    \n",
    "print(\"Tree IoU .05=\",test_tree_iou05)\n",
    "print(\"Tree IoU .1 =\",test_tree_iou1 )\n",
    "print(\"Tree IoU .15=\",test_tree_iou15)\n",
    "print(\"Tree IoU .2 =\",test_tree_iou2 )\n",
    "print(\"Tree IoU .25=\",test_tree_iou25)\n",
    "print(\"Tree IoU .3 =\",test_tree_iou3 )\n",
    "print(\"Tree IoU .35=\",test_tree_iou35)\n",
    "print(\"Tree IoU .4 =\",test_tree_iou4 )\n",
    "print(\"Tree IoU .45=\",test_tree_iou45)\n",
    "print(\"Tree IoU .5 =\",test_tree_iou5 )\n",
    "print(\"Tree IoU .55=\",test_tree_iou55)\n",
    "print(\"Tree IoU .6 =\",test_tree_iou6 )\n",
    "print(\"Tree IoU .65=\",test_tree_iou65)\n",
    "print(\"Tree IoU .7 =\",test_tree_iou7 )\n",
    "print(\"Tree IoU .75=\",test_tree_iou75)\n",
    "print(\"Tree IoU .8 =\",test_tree_iou8 )\n",
    "print(\"Tree IoU .85=\",test_tree_iou85)\n",
    "print(\"Tree IoU .9 =\",test_tree_iou9 )\n",
    "print(\"Tree IoU .95=\",test_tree_iou95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c825df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Losses/Metrics\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1,30001,203),height_loss,\"rx-\",label=\"tree height loss\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1,30001,203),tmask_loss,\"bx-\",label=\"tree mask loss\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1,50,1),height_mae,\"m--\",label=\"Tree height MAE\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1,50,1),tmask_iou,\"m--\",label=\"Tree Mask IoU\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "tota_loss = np.add(np.array(height_loss), np.array(tmask_loss))\n",
    "\n",
    "plt.plot(range(1,30001,203),tota_loss,\"m--\",label=\"total loss\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f52f397",
   "metadata": {},
   "source": [
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ce497",
   "metadata": {},
   "source": [
    "# Run Inference on 2021 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a346c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load In final chosen models\n",
    "\n",
    "# Set up a version with shared encoding paths but different encoding paths\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def defineUNetModel_partiallyshared():\n",
    "#https://github.com/usuyama/pytorch-unet/blob/master/pytorch_unet.py\n",
    "    def double_conv0(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=\"same\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )  \n",
    "    def double_conv(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    class UNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            self.dconv_down1 = double_conv0(14, 32)\n",
    "            self.dconv_down2 = double_conv(32, 64)\n",
    "            self.dconv_down3 = double_conv(64, 128)\n",
    "            self.dconv_down4 = nn.Sequential(\n",
    "                nn.Conv2d(128, 256, 3, padding=1),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.Dropout(p=0.2),\n",
    "                nn.Conv2d(256, 256, 3, padding=1),\n",
    "                nn.LeakyReLU(inplace=True))\n",
    "\n",
    "            self.maxpool = nn.MaxPool2d(2)\n",
    "            self.maxpool3 = nn.MaxPool2d(3)\n",
    "            \n",
    "            self.upsample1a = nn.ConvTranspose2d(256, 128, 3, stride=3, padding=0)\n",
    "            self.upsample1b = nn.ConvTranspose2d(256, 128, 3, stride=3, padding=0)\n",
    "            self.upsample1c = nn.ConvTranspose2d(256, 128, 3, stride=3, padding=0)\n",
    "            \n",
    "            self.upsample2a = nn.ConvTranspose2d(128, 64, 2, stride=2, padding=0)\n",
    "            self.upsample2b = nn.ConvTranspose2d(128, 64, 2, stride=2, padding=0)\n",
    "            self.upsample2c = nn.ConvTranspose2d(128, 64, 2, stride=2, padding=0)\n",
    "            \n",
    "            self.upsample3a = nn.ConvTranspose2d(64, 32, 2, stride=2, padding=0)\n",
    "            self.upsample3b = nn.ConvTranspose2d(64, 32, 2, stride=2, padding=0)\n",
    "            self.upsample3c = nn.ConvTranspose2d(64, 32, 2, stride=2, padding=0)\n",
    "            \n",
    "            \n",
    "            self.dconv_up3a = double_conv(128+128, 128)\n",
    "            self.dconv_up3b = double_conv(128+128, 128)\n",
    "            self.dconv_up3c = double_conv(128+128, 128)\n",
    "            \n",
    "            self.dconv_up2a = double_conv(64 + 64, 64)\n",
    "            self.dconv_up2b = double_conv(64 + 64, 64)\n",
    "            self.dconv_up2c = double_conv(64 + 64, 64)\n",
    "            \n",
    "            self.dconv_up1a = double_conv(32 + 32, 32)\n",
    "            self.dconv_up1b = double_conv(32 + 32, 32)\n",
    "            self.dconv_up1c = double_conv(32 + 32, 32)\n",
    "           # self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "\n",
    "            self.conv_lasta = nn.Conv2d(32, 1, 1)\n",
    "            self.conv_lastb = nn.Conv2d(32, 1, 1)\n",
    "            self.conv_lastc = nn.Conv2d(32, 1, 1)\n",
    "                    \n",
    "            self.linear = nn.MaxPool2d(2)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "        def forward(self, x):\n",
    "            conv1 = self.dconv_down1(x)\n",
    "            x = self.maxpool(conv1)\n",
    "\n",
    "            conv2 = self.dconv_down2(x)\n",
    "            x = self.maxpool(conv2)\n",
    "\n",
    "            conv3 = self.dconv_down3(x)\n",
    "            x = self.maxpool3(conv3)\n",
    "\n",
    "            encoder_end = self.dconv_down4(x)\n",
    "            #x = self.maxpool(conv4)\n",
    "            \n",
    "            # Now the model should split into three branches\n",
    "\n",
    "            # Tree Height\n",
    "            x1 = self.upsample1a(encoder_end)\n",
    "            x1 = torch.cat([x1, conv3], dim=1)\n",
    "            x1 = self.dconv_up3a(x1)\n",
    "\n",
    "            x1 = self.upsample2a(x1)\n",
    "            x1 = torch.cat([x1, conv2], dim=1)\n",
    "            x1 = self.dconv_up2a(x1)\n",
    "\n",
    "            x1 = self.upsample3a(x1)\n",
    "            x1 = torch.cat([x1, conv1], dim=1)\n",
    "            x1 = self.dconv_up1a(x1)\n",
    "            out_tree_height = self.conv_lasta(x1) # looks like i don't need any additional activation here for linear\n",
    "\n",
    "            # Tree Mask\n",
    "            x2 = self.upsample1b(encoder_end)\n",
    "            x2 = torch.cat([x2, conv3], dim=1)\n",
    "            x2 = self.dconv_up3b(x2)\n",
    "\n",
    "            x2 = self.upsample2b(x2)\n",
    "            x2 = torch.cat([x2, conv2], dim=1)\n",
    "            x2 = self.dconv_up2b(x2)\n",
    "\n",
    "            x2 = self.upsample3b(x2)\n",
    "            x2 = torch.cat([x2, conv1], dim=1)\n",
    "            x2 = self.dconv_up1b(x2)\n",
    "            out_tree_mask = self.sigmoid(self.conv_lastb(x2))            \n",
    "     \n",
    "            \n",
    "            return [out_tree_height, out_tree_mask]\n",
    "    model=UNet()\n",
    "    return model\n",
    "\n",
    "\n",
    "unet_1 = defineUNetModel_partiallyshared().to(device)\n",
    "unet_1.load_state_dict(torch.load(\"models/pytorch_paper_final/pytorch_mtloss_partshared_manual.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759be313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepaths for inferences images\n",
    "inputPath=\"2021_predictions\" # 12972\n",
    "filelist = []\n",
    "\n",
    "# Load the images, and append them to a list.\n",
    "for filepath in os.listdir(inputPath):\n",
    "    if filepath.endswith((\".tif\")):\n",
    "    #print(filepath)\n",
    "        tempfile=inputPath+'/{0}'.format(filepath)\n",
    "        filelist.append(tempfile)\n",
    "\n",
    "len(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_replace(tensor, cutpoint):\n",
    "    res = tensor.clone()\n",
    "    res[tensor>=cutpoint] = 1\n",
    "    res[tensor<cutpoint] = 0\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# i need to get each tif, append the new raster layers, remove the bands i dont need anymore (maybe keep rgb), and then save\n",
    "inputPath=\"D:/final_data/2021_predictions/\"\n",
    "\n",
    "driver = gdal.GetDriverByName(\"GTiff\")\n",
    "driver.Register()\n",
    "i=0\n",
    "# Load the images, and append them to a list.\n",
    "for filepath in os.listdir(inputPath):\n",
    "    if filepath.endswith((\".tif\")):\n",
    "        print(filepath)\n",
    "        i=i+1\n",
    "        print(i)\n",
    "        images = []\n",
    "        dataset = gdal.Open(inputPath+'/{0}'.format(filepath))\n",
    "        gt = dataset.GetGeoTransform()\n",
    "        proj = dataset.GetProjection()\n",
    "        \n",
    "        image = dataset.ReadAsArray()  # Returned image is a NumPy array with shape (16, 60, 60) for example.\n",
    "        # predict values based on the two different models\n",
    "        images.append(image)\n",
    "        image = np.stack(images, axis= 0)\n",
    "        X=image[:,:14,:,:].copy() # separate out the band values\n",
    "        X[X  < .0000001] = 0\n",
    "        X = np.transpose(X, axes=[0, 2, 3, 1])\n",
    "        \n",
    "        # normalize values of the input data to 0,1\n",
    "        X = X/X.max(axis=(3),keepdims=True)\n",
    "        \n",
    "        X = np.transpose(X, axes=[0,3,1,2])\n",
    "        X= torch.from_numpy(X)\n",
    "        X= X.to(device)\n",
    "        X = Variable(X.float().cuda())\n",
    "        unet_1.eval()\n",
    "        with torch.inference_mode():\n",
    "            pred_tree_height, pred_tree_mask = unet_1(X)\n",
    "\n",
    "        pred_tree_mask = np.asarray(pred_tree_mask.squeeze().cpu())\n",
    "        pred_tree_mask[pred_tree_mask  >= .4] = 1\n",
    "        pred_tree_mask[pred_tree_mask  < .4 ] = 0\n",
    "        \n",
    "        pred_tree_height = np.asarray(pred_tree_height.squeeze().cpu())*650\n",
    "        pred_tree_height[pred_tree_height  < 0 ] = 0\n",
    "        \n",
    "        outcome_data=np.stack([pred_tree_height,pred_tree_mask])\n",
    "\n",
    "        \n",
    "        # save solution\n",
    "        save_raster1 = driver.Create('2021_predictions_results_theight'+'/{0}'.format(filepath), \n",
    "                                    xsize=240, ysize=240, bands = 1)\n",
    "\n",
    "        save_raster1.SetGeoTransform(gt)\n",
    "        save_raster1.SetProjection(proj) \n",
    "        \n",
    "        outband_1 = save_raster1.GetRasterBand(1)\n",
    "        outband_1.WriteArray(outcome_data[0].astype(np.float32))\n",
    "        outband_1.SetNoDataValue(np.nan)\n",
    "        outband_1.FlushCache()\n",
    "        #outband_1 = None\n",
    "        save_raster1 = None\n",
    "        \n",
    "        # save solution\n",
    "        save_raster2 = driver.Create('2021_predictions_results_tbinary'+'/{0}'.format(filepath), \n",
    "                                    xsize=240, ysize=240, bands = 1)\n",
    "\n",
    "        save_raster2.SetGeoTransform(gt)\n",
    "        save_raster2.SetProjection(proj) \n",
    "        \n",
    "        outband_1 = save_raster2.GetRasterBand(1)\n",
    "        outband_1.WriteArray(outcome_data[1].astype(np.float32))\n",
    "        outband_1.SetNoDataValue(np.nan)\n",
    "        outband_1.FlushCache()        \n",
    "\n",
    "        #outband_5 = None\n",
    "        \n",
    "        save_raster2 = None\n",
    "\n",
    "# CPU times: total: 15min 33s\n",
    "# Wall time: 52min 3s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
