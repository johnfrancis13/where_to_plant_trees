{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb649d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in any libraries i will need\n",
    "# Import libraries i will need\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "from osgeo import gdal\n",
    "import random as python_random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import gc\n",
    "import keras\n",
    "\n",
    "#import the libraries necessary for confusion plot\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import cv2 # imports the computer vision package\n",
    "from tensorflow.keras.models import Model\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a02dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath=\"D:/final_data/2017_2\"\n",
    "filelist = []\n",
    "\n",
    "# Load the images, and append them to a list.\n",
    "for filepath in os.listdir(inputPath):\n",
    "    if filepath.endswith((\".tif\")):\n",
    "    #print(filepath)\n",
    "        tempfile=inputPath+'/{0}'.format(filepath)\n",
    "        filelist.append(tempfile)\n",
    "    \n",
    "#### 7/25/22 #######\n",
    "\n",
    "# Switching this up, 15= depth, 16=tree/no tree, 16 = NDVI... want to switch NDVI to not vegetation <0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09c41f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    \n",
    "    The jaccard distance loss is usefull for unbalanced datasets. This has been\n",
    "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
    "    gradient.\n",
    "    \n",
    "    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    \n",
    "    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
    "    @author: wassname\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.sum(K.abs(y_true * y_pred), axis=-1))\n",
    "    sum_ = K.sum(K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1))\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "def dice_metric(y_pred, y_true):\n",
    "    intersection = K.sum(K.sum(K.abs(y_true * y_pred), axis=-1))\n",
    "    union = K.sum(K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1))\n",
    "    # if y_pred.sum() == 0 and y_pred.sum() == 0:\n",
    "    #     return 1.0\n",
    "\n",
    "    return 2*intersection / union\n",
    "\n",
    "# define my own jaccard metric, should be same as IoU\n",
    "def jaccard_coef(y_true, y_pred):\n",
    "    y_true_f=K.flatten(y_true)\n",
    "    y_pred_f=K.flatten(y_pred)\n",
    "    intersection=K.sum(y_true_f + y_pred_f)\n",
    "    return (intersection +1.0)/ (K.sum(y_true_f) + K.sum(y_pred_f) - intersection +1.0)\n",
    "    \n",
    "def jaccard_coef_loss(y_true, y_pred):\n",
    "    return -jaccard_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "892cbcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=25, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, Y1, Y2, Y3 = self.__data_generation(list_IDs_temp)\n",
    "        y=[Y1,Y2,Y3]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        images = []\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            dataset = gdal.Open(ID)\n",
    "            image = dataset.ReadAsArray()  # Returned image is a NumPy array with shape (16, 60, 60) for example.\n",
    "            images.append(image)  # Append the NumPy array to the list.\n",
    "\n",
    "        all_data= np.stack(images, axis= 0)\n",
    "        all_data[all_data < .0000001] = 0\n",
    "        X=all_data[:,:14,:,:] # separate out the band values\n",
    "        X = np.transpose(X, axes=[0, 2, 3, 1])\n",
    "        # normalize values of the input data to 0,1\n",
    "        X = X/X.max(axis=(3),keepdims=1)\n",
    "        # For RGB uncomment this\n",
    "        X = X[:,:,:,:3]\n",
    "        \n",
    "        # canopy_height,tree/not tree,ndvi\n",
    "        all_data= np.stack( images, axis= 0)\n",
    "        Y = all_data[:,14:]\n",
    "        Y[Y  < .0000001] = 0\n",
    "        #Y[:,0][Y[:,0]  > 1] = 1\n",
    "        Y1=Y[:,0] # 0 for height, 1 for tree/not, 2 for NDVI \n",
    "        Y1 = Y1/Y1.max()          \n",
    "        Y2=Y[:,1]\n",
    "        Y2[Y2  >0 ] = 1 \n",
    "        all_data= np.stack( images, axis= 0)\n",
    "        Y3 = all_data[:,14:]\n",
    "        Y3[(Y3 < .0000001) & (Y3 >= 0) ] = 1\n",
    "        Y3=Y3[:,2] # 0 for height, 1 for tree/not, 2 for NDVI \n",
    "        Y3[Y3  >0 ] = 0\n",
    "        Y3[Y3  <0 ] = 1\n",
    "        \n",
    "        \n",
    "        #Y[:,0][Y[:,0]  > 1] = 1\n",
    "        #Y3=Y[:,2] # 0 for height, 1 for tree/not, 2 for NDVI \n",
    "        #Y3 = Y3/Y3.max()\n",
    "        self.X=X\n",
    "        self.Y1=Y1\n",
    "        self.Y2=Y2\n",
    "        self.Y3=Y3\n",
    "\n",
    "        return X, Y1, Y2, Y3\n",
    "    \n",
    "    \n",
    "    def get_true_values_x(self, indexes):\n",
    "        'Generate one batch of data'\n",
    "        # Return validation data for plotting \n",
    "\n",
    "        # Find list of IDs - (give input of validation indexes)\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, Y1, Y2, Y3 = self.__data_generation(list_IDs_temp)\n",
    "        y=[Y1,Y2,Y3]\n",
    "        \n",
    "        # only need to return the RGB data for plotting\n",
    "        X = X[:,:,:,0:3]\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def get_true_values_y(self, indexes):\n",
    "        'Generate one batch of data'\n",
    "        # Return validation data for plotting \n",
    "\n",
    "        # Find list of IDs - (give input of validation indexes)\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, Y1, Y2, Y3 = self.__data_generation(list_IDs_temp)\n",
    "        y=[Y1,Y2,Y3]\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1be61977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9534"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'batch_size':12,\n",
    "         'shuffle': True}\n",
    "\n",
    "# need a dictionary which has a list of training paths and a list of validations paths\n",
    "filelist_temp = filelist\n",
    "np.random.seed(14)\n",
    "mask = np.random.rand(len(filelist_temp)) <=.75\n",
    "\n",
    "training_data = np.array(filelist_temp)[mask]\n",
    "val_data = np.array(filelist_temp)[~mask]\n",
    "\n",
    "\n",
    "mydict = {}\n",
    "mydict[\"training\"] = training_data\n",
    "mydict[\"validation\"] = val_data\n",
    "\n",
    "# generators\n",
    "training_generator = DataGenerator(mydict[\"training\"], **params)\n",
    "val_generator = DataGenerator(mydict[\"validation\"], **params)\n",
    "\n",
    "len(filelist_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "294ad8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# define U-Net model architecture - All shared layers\n",
    "\n",
    "def build_unet(img_shape):\n",
    "    # input layer shape is equal to patch image size\n",
    "    inputs = layers.Input(shape=img_shape)\n",
    "\n",
    "    # rescale images from (0, 255) to (0, 1)\n",
    " #   rescale = Rescaling(scale=1. / 255, input_shape=(img_height, img_width, img_channels))(inputs)\n",
    " #   previous_block_activation = rescale  # Set aside residual\n",
    "    previous_block_activation = inputs\n",
    "\n",
    "    contraction = {}\n",
    "    # # Contraction path: Blocks 1 through 5 are identical apart from the feature depth\n",
    "    for f in [32, 64]:\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(previous_block_activation)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        contraction[f'conv{f}'] = x\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        previous_block_activation = x\n",
    "\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(previous_block_activation)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    contraction[f'conv{128}'] = x\n",
    "    x = layers.MaxPooling2D((3, 3))(x)\n",
    "    previous_block_activation = x\n",
    "        \n",
    "    c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(previous_block_activation)\n",
    "    c5 = layers.Dropout(0.2)(c5)\n",
    "    c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "    previous_block_activation = c5\n",
    "    \n",
    "    x = layers.Conv2DTranspose(128, (2, 2), strides=(3, 3), padding='same')(previous_block_activation)\n",
    "    x = layers.concatenate([x, contraction[f'conv{128}']])\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    previous_block_activation = x\n",
    "        \n",
    "    # Expansive path: Second half of the network: upsampling inputs\n",
    "    # could we use upsampling layers here instead of Conv2dTRanspose layers? might that help\n",
    "    for f in reversed([32, 64]):\n",
    "        x = layers.Conv2DTranspose(f, (2, 2), strides=(2, 2), padding='same')(previous_block_activation)\n",
    "        x = layers.concatenate([x, contraction[f'conv{f}']])\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        previous_block_activation = x\n",
    "\n",
    "    output_tree_height = layers.Conv2D(filters=1, kernel_size=(1, 1), activation=\"linear\", name=\"tree_height\")(previous_block_activation)\n",
    "\n",
    "    output_tree_binary = layers.Conv2D(filters=1, kernel_size=(1, 1), activation=\"sigmoid\", name=\"tree_binary\")(previous_block_activation)\n",
    "    \n",
    "    output_vegetation_task = layers.Conv2D(filters=1, kernel_size=(1, 1), activation=\"sigmoid\", name=\"vegetation_task\")(previous_block_activation)\n",
    "\n",
    "\n",
    "    return Model(inputs=inputs, outputs=[output_tree_height,output_tree_binary,output_vegetation_task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64b64204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 240, 240, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 240, 240, 32  896         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 240, 240, 32  0           ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 240, 240, 32  9248        ['dropout[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 120, 120, 32  0           ['conv2d_1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 120, 120, 64  18496       ['max_pooling2d[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 120, 120, 64  0           ['conv2d_2[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 120, 120, 64  36928       ['dropout_1[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 60, 60, 64)  0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 60, 60, 128)  73856       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 60, 60, 128)  0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 60, 60, 128)  147584      ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 20, 20, 128)  0          ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 20, 20, 256)  295168      ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 20, 20, 256)  0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 20, 20, 256)  590080      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 60, 60, 128)  131200     ['conv2d_7[0][0]']               \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 60, 60, 256)  0           ['conv2d_transpose[0][0]',       \n",
      "                                                                  'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 60, 60, 128)  295040      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 60, 60, 128)  0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 60, 60, 128)  147584      ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 120, 120, 64  32832      ['conv2d_9[0][0]']               \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 120, 120, 12  0           ['conv2d_transpose_1[0][0]',     \n",
      "                                8)                                'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 120, 120, 64  73792       ['concatenate_1[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 120, 120, 64  0           ['conv2d_10[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 120, 120, 64  36928       ['dropout_5[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 240, 240, 32  8224       ['conv2d_11[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 240, 240, 64  0           ['conv2d_transpose_2[0][0]',     \n",
      "                                )                                 'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 240, 240, 32  18464       ['concatenate_2[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 240, 240, 32  0           ['conv2d_12[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 240, 240, 32  9248        ['dropout_6[0][0]']              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tree_height (Conv2D)           (None, 240, 240, 1)  33          ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " tree_binary (Conv2D)           (None, 240, 240, 1)  33          ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " vegetation_task (Conv2D)       (None, 240, 240, 1)  33          ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,925,667\n",
      "Trainable params: 1,925,667\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#plot_model(model_multi_all_shared,\"multi_task_model_all_shared_final.png\" , show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# build model\n",
    "model_multi_all_shared= build_unet(img_shape=(240, 240, 3))\n",
    "model_multi_all_shared.summary()\n",
    "\n",
    "model_multi_all_shared.compile(optimizer=\"adam\", \n",
    "              loss={'tree_height': 'mse',\n",
    "                    'tree_binary': jaccard_distance_loss,\n",
    "                    'vegetation_task': jaccard_distance_loss,},\n",
    "               loss_weights={'tree_binary': .5, \n",
    "                             'tree_height': .6,\n",
    "                             'vegetation_task': .1,},\n",
    "              metrics={'tree_height': [\"mae\", 'accuracy'],\n",
    "                       'tree_binary': [tf.keras.metrics.BinaryCrossentropy(),tf.keras.metrics.MeanIoU(num_classes=2),dice_metric], \n",
    "                       'vegetation_task': [tf.keras.metrics.BinaryCrossentropy(),tf.keras.metrics.MeanIoU(num_classes=2),dice_metric]}\n",
    ")\n",
    "#print(model_multi_all_shared.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfc2a465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "597/597 [==============================] - 1446s 2s/step - loss: 40.8171 - tree_height_loss: 0.1551 - tree_binary_loss: 78.7111 - vegetation_task_loss: 13.6848 - tree_height_mae: 0.2434 - tree_height_accuracy: 0.0718 - tree_binary_binary_crossentropy: 0.7414 - tree_binary_mean_io_u: 0.5251 - tree_binary_dice_metric: 0.3459 - vegetation_task_binary_crossentropy: 0.5529 - vegetation_task_mean_io_u_1: 0.1535 - vegetation_task_dice_metric: 0.9251 - val_loss: 37.7890 - val_tree_height_loss: 0.0161 - val_tree_binary_loss: 73.6702 - val_vegetation_task_loss: 9.4429 - val_tree_height_mae: 0.0887 - val_tree_height_accuracy: 0.0767 - val_tree_binary_binary_crossentropy: 0.9054 - val_tree_binary_mean_io_u: 0.5801 - val_tree_binary_dice_metric: 0.4153 - val_vegetation_task_binary_crossentropy: 0.4752 - val_vegetation_task_mean_io_u_1: 0.1592 - val_vegetation_task_dice_metric: 0.9503\n",
      "Epoch 2/100\n",
      "597/597 [==============================] - 852s 1s/step - loss: 37.6770 - tree_height_loss: 0.0372 - tree_binary_loss: 73.4376 - vegetation_task_loss: 9.3594 - tree_height_mae: 0.1326 - tree_height_accuracy: 0.0756 - tree_binary_binary_crossentropy: 0.8633 - tree_binary_mean_io_u: 0.5803 - tree_binary_dice_metric: 0.4178 - vegetation_task_binary_crossentropy: 0.4675 - vegetation_task_mean_io_u_1: 0.2765 - vegetation_task_dice_metric: 0.9508 - val_loss: 36.8859 - val_tree_height_loss: 0.0219 - val_tree_binary_loss: 72.0525 - val_vegetation_task_loss: 8.4650 - val_tree_height_mae: 0.0906 - val_tree_height_accuracy: 0.0769 - val_tree_binary_binary_crossentropy: 0.8246 - val_tree_binary_mean_io_u: 0.5872 - val_tree_binary_dice_metric: 0.4352 - val_vegetation_task_binary_crossentropy: 0.4326 - val_vegetation_task_mean_io_u_1: 0.3727 - val_vegetation_task_dice_metric: 0.9557\n",
      "Epoch 3/100\n",
      "597/597 [==============================] - 834s 1s/step - loss: 36.7919 - tree_height_loss: 0.0333 - tree_binary_loss: 71.8499 - vegetation_task_loss: 8.4694 - tree_height_mae: 0.1260 - tree_height_accuracy: 0.0761 - tree_binary_binary_crossentropy: 0.8467 - tree_binary_mean_io_u: 0.5930 - tree_binary_dice_metric: 0.4373 - vegetation_task_binary_crossentropy: 0.4472 - vegetation_task_mean_io_u_1: 0.3493 - vegetation_task_dice_metric: 0.9557 - val_loss: 36.4946 - val_tree_height_loss: 0.0165 - val_tree_binary_loss: 71.4603 - val_vegetation_task_loss: 7.5461 - val_tree_height_mae: 0.0827 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.7644 - val_tree_binary_mean_io_u: 0.5876 - val_tree_binary_dice_metric: 0.4421 - val_vegetation_task_binary_crossentropy: 0.4101 - val_vegetation_task_mean_io_u_1: 0.4956 - val_vegetation_task_dice_metric: 0.9607\n",
      "Epoch 4/100\n",
      "597/597 [==============================] - 828s 1s/step - loss: 36.4404 - tree_height_loss: 0.0282 - tree_binary_loss: 71.1953 - vegetation_task_loss: 8.2589 - tree_height_mae: 0.1075 - tree_height_accuracy: 0.0763 - tree_binary_binary_crossentropy: 0.8731 - tree_binary_mean_io_u: 0.5993 - tree_binary_dice_metric: 0.4450 - vegetation_task_binary_crossentropy: 0.5079 - vegetation_task_mean_io_u_1: 0.4671 - vegetation_task_dice_metric: 0.9567 - val_loss: 37.1283 - val_tree_height_loss: 0.0240 - val_tree_binary_loss: 72.5846 - val_vegetation_task_loss: 8.2156 - val_tree_height_mae: 0.0965 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8159 - val_tree_binary_mean_io_u: 0.5943 - val_tree_binary_dice_metric: 0.4282 - val_vegetation_task_binary_crossentropy: 0.5845 - val_vegetation_task_mean_io_u_1: 0.7102 - val_vegetation_task_dice_metric: 0.9571\n",
      "Epoch 5/100\n",
      "597/597 [==============================] - 908s 2s/step - loss: 36.1153 - tree_height_loss: 0.0266 - tree_binary_loss: 70.6253 - vegetation_task_loss: 7.8672 - tree_height_mae: 0.1112 - tree_height_accuracy: 0.0763 - tree_binary_binary_crossentropy: 0.8644 - tree_binary_mean_io_u: 0.6054 - tree_binary_dice_metric: 0.4521 - vegetation_task_binary_crossentropy: 0.5030 - vegetation_task_mean_io_u_1: 0.5830 - vegetation_task_dice_metric: 0.9590 - val_loss: 35.3596 - val_tree_height_loss: 0.0248 - val_tree_binary_loss: 69.2878 - val_vegetation_task_loss: 7.0081 - val_tree_height_mae: 0.1273 - val_tree_height_accuracy: 0.0766 - val_tree_binary_binary_crossentropy: 0.8135 - val_tree_binary_mean_io_u: 0.6111 - val_tree_binary_dice_metric: 0.4681 - val_vegetation_task_binary_crossentropy: 0.4536 - val_vegetation_task_mean_io_u_1: 0.6623 - val_vegetation_task_dice_metric: 0.9636\n",
      "Epoch 6/100\n",
      "597/597 [==============================] - 917s 2s/step - loss: 35.5502 - tree_height_loss: 0.0199 - tree_binary_loss: 69.5905 - vegetation_task_loss: 7.4299 - tree_height_mae: 0.0981 - tree_height_accuracy: 0.0764 - tree_binary_binary_crossentropy: 0.8446 - tree_binary_mean_io_u: 0.6122 - tree_binary_dice_metric: 0.4644 - vegetation_task_binary_crossentropy: 0.4963 - vegetation_task_mean_io_u_1: 0.6636 - vegetation_task_dice_metric: 0.9613 - val_loss: 35.1902 - val_tree_height_loss: 0.0152 - val_tree_binary_loss: 68.9914 - val_vegetation_task_loss: 6.8540 - val_tree_height_mae: 0.0970 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8094 - val_tree_binary_mean_io_u: 0.6138 - val_tree_binary_dice_metric: 0.4716 - val_vegetation_task_binary_crossentropy: 0.4632 - val_vegetation_task_mean_io_u_1: 0.7096 - val_vegetation_task_dice_metric: 0.9644\n",
      "Epoch 7/100\n",
      "597/597 [==============================] - 940s 2s/step - loss: 35.4522 - tree_height_loss: 0.0211 - tree_binary_loss: 69.3927 - vegetation_task_loss: 7.4316 - tree_height_mae: 0.1016 - tree_height_accuracy: 0.0764 - tree_binary_binary_crossentropy: 0.8521 - tree_binary_mean_io_u: 0.6142 - tree_binary_dice_metric: 0.4667 - vegetation_task_binary_crossentropy: 0.5268 - vegetation_task_mean_io_u_1: 0.6992 - vegetation_task_dice_metric: 0.9613 - val_loss: 34.9917 - val_tree_height_loss: 0.0158 - val_tree_binary_loss: 68.6074 - val_vegetation_task_loss: 6.7852 - val_tree_height_mae: 0.0979 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8203 - val_tree_binary_mean_io_u: 0.6176 - val_tree_binary_dice_metric: 0.4761 - val_vegetation_task_binary_crossentropy: 0.4708 - val_vegetation_task_mean_io_u_1: 0.7244 - val_vegetation_task_dice_metric: 0.9648\n",
      "Epoch 8/100\n",
      "597/597 [==============================] - 856s 1s/step - loss: 35.0123 - tree_height_loss: 0.0295 - tree_binary_loss: 68.5949 - vegetation_task_loss: 6.9716 - tree_height_mae: 0.1102 - tree_height_accuracy: 0.0759 - tree_binary_binary_crossentropy: 0.8259 - tree_binary_mean_io_u: 0.6190 - tree_binary_dice_metric: 0.4759 - vegetation_task_binary_crossentropy: 0.4805 - vegetation_task_mean_io_u_1: 0.6592 - vegetation_task_dice_metric: 0.9638 - val_loss: 34.6655 - val_tree_height_loss: 0.0145 - val_tree_binary_loss: 67.9912 - val_vegetation_task_loss: 6.6120 - val_tree_height_mae: 0.0726 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.7873 - val_tree_binary_mean_io_u: 0.6206 - val_tree_binary_dice_metric: 0.4832 - val_vegetation_task_binary_crossentropy: 0.4640 - val_vegetation_task_mean_io_u_1: 0.7322 - val_vegetation_task_dice_metric: 0.9658\n",
      "Epoch 9/100\n",
      "597/597 [==============================] - 893s 1s/step - loss: 35.5822 - tree_height_loss: 0.0212 - tree_binary_loss: 69.6173 - vegetation_task_loss: 7.6082 - tree_height_mae: 0.1014 - tree_height_accuracy: 0.0764 - tree_binary_binary_crossentropy: 0.8806 - tree_binary_mean_io_u: 0.6141 - tree_binary_dice_metric: 0.4638 - vegetation_task_binary_crossentropy: 0.5882 - vegetation_task_mean_io_u_1: 0.7209 - vegetation_task_dice_metric: 0.9603 - val_loss: 35.2241 - val_tree_height_loss: 0.0140 - val_tree_binary_loss: 69.0869 - val_vegetation_task_loss: 6.7219 - val_tree_height_mae: 0.0784 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8527 - val_tree_binary_mean_io_u: 0.6188 - val_tree_binary_dice_metric: 0.4704 - val_vegetation_task_binary_crossentropy: 0.5086 - val_vegetation_task_mean_io_u_1: 0.7864 - val_vegetation_task_dice_metric: 0.9652\n",
      "Epoch 10/100\n",
      "597/597 [==============================] - 941s 2s/step - loss: 35.0903 - tree_height_loss: 0.0210 - tree_binary_loss: 68.7253 - vegetation_task_loss: 7.1513 - tree_height_mae: 0.0987 - tree_height_accuracy: 0.0764 - tree_binary_binary_crossentropy: 0.8604 - tree_binary_mean_io_u: 0.6202 - tree_binary_dice_metric: 0.4745 - vegetation_task_binary_crossentropy: 0.5539 - vegetation_task_mean_io_u_1: 0.7542 - vegetation_task_dice_metric: 0.9628 - val_loss: 34.6098 - val_tree_height_loss: 0.0134 - val_tree_binary_loss: 67.9244 - val_vegetation_task_loss: 6.3959 - val_tree_height_mae: 0.0798 - val_tree_height_accuracy: 0.0767 - val_tree_binary_binary_crossentropy: 0.7702 - val_tree_binary_mean_io_u: 0.6219 - val_tree_binary_dice_metric: 0.4838 - val_vegetation_task_binary_crossentropy: 0.4906 - val_vegetation_task_mean_io_u_1: 0.7960 - val_vegetation_task_dice_metric: 0.9669\n",
      "Epoch 11/100\n",
      "597/597 [==============================] - 919s 2s/step - loss: 34.7671 - tree_height_loss: 0.0177 - tree_binary_loss: 68.1071 - vegetation_task_loss: 7.0294 - tree_height_mae: 0.0936 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8482 - tree_binary_mean_io_u: 0.6245 - tree_binary_dice_metric: 0.4816 - vegetation_task_binary_crossentropy: 0.5604 - vegetation_task_mean_io_u_1: 0.7698 - vegetation_task_dice_metric: 0.9635 - val_loss: 34.9580 - val_tree_height_loss: 0.0156 - val_tree_binary_loss: 68.5485 - val_vegetation_task_loss: 6.7444 - val_tree_height_mae: 0.1029 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8135 - val_tree_binary_mean_io_u: 0.6173 - val_tree_binary_dice_metric: 0.4765 - val_vegetation_task_binary_crossentropy: 0.5362 - val_vegetation_task_mean_io_u_1: 0.8237 - val_vegetation_task_dice_metric: 0.9650\n",
      "Epoch 12/100\n",
      "597/597 [==============================] - 849s 1s/step - loss: 34.7061 - tree_height_loss: 0.0182 - tree_binary_loss: 68.0253 - vegetation_task_loss: 6.8252 - tree_height_mae: 0.0946 - tree_height_accuracy: 0.0764 - tree_binary_binary_crossentropy: 0.8521 - tree_binary_mean_io_u: 0.6257 - tree_binary_dice_metric: 0.4824 - vegetation_task_binary_crossentropy: 0.5502 - vegetation_task_mean_io_u_1: 0.7765 - vegetation_task_dice_metric: 0.9646 - val_loss: 34.3361 - val_tree_height_loss: 0.0146 - val_tree_binary_loss: 67.3697 - val_vegetation_task_loss: 6.4248 - val_tree_height_mae: 0.0750 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.7995 - val_tree_binary_mean_io_u: 0.6274 - val_tree_binary_dice_metric: 0.4902 - val_vegetation_task_binary_crossentropy: 0.5078 - val_vegetation_task_mean_io_u_1: 0.8300 - val_vegetation_task_dice_metric: 0.9667\n",
      "Epoch 13/100\n",
      "597/597 [==============================] - 867s 1s/step - loss: 34.5798 - tree_height_loss: 0.0174 - tree_binary_loss: 67.7776 - vegetation_task_loss: 6.8054 - tree_height_mae: 0.0929 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8491 - tree_binary_mean_io_u: 0.6273 - tree_binary_dice_metric: 0.4854 - vegetation_task_binary_crossentropy: 0.5631 - vegetation_task_mean_io_u_1: 0.7834 - vegetation_task_dice_metric: 0.9647 - val_loss: 34.1577 - val_tree_height_loss: 0.0141 - val_tree_binary_loss: 67.0579 - val_vegetation_task_loss: 6.2022 - val_tree_height_mae: 0.0751 - val_tree_height_accuracy: 0.0767 - val_tree_binary_binary_crossentropy: 0.8153 - val_tree_binary_mean_io_u: 0.6320 - val_tree_binary_dice_metric: 0.4938 - val_vegetation_task_binary_crossentropy: 0.5064 - val_vegetation_task_mean_io_u_1: 0.7888 - val_vegetation_task_dice_metric: 0.9680\n",
      "Epoch 14/100\n",
      "597/597 [==============================] - 926s 2s/step - loss: 34.3499 - tree_height_loss: 0.0168 - tree_binary_loss: 67.3486 - vegetation_task_loss: 6.6554 - tree_height_mae: 0.0916 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8369 - tree_binary_mean_io_u: 0.6305 - tree_binary_dice_metric: 0.4903 - vegetation_task_binary_crossentropy: 0.5598 - vegetation_task_mean_io_u_1: 0.7933 - vegetation_task_dice_metric: 0.9655 - val_loss: 35.4585 - val_tree_height_loss: 0.0184 - val_tree_binary_loss: 69.5356 - val_vegetation_task_loss: 6.7970 - val_tree_height_mae: 0.1143 - val_tree_height_accuracy: 0.0767 - val_tree_binary_binary_crossentropy: 1.0294 - val_tree_binary_mean_io_u: 0.6173 - val_tree_binary_dice_metric: 0.4653 - val_vegetation_task_binary_crossentropy: 0.6041 - val_vegetation_task_mean_io_u_1: 0.8289 - val_vegetation_task_dice_metric: 0.9648\n",
      "Epoch 15/100\n",
      "597/597 [==============================] - 935s 2s/step - loss: 34.8783 - tree_height_loss: 0.0202 - tree_binary_loss: 68.2740 - vegetation_task_loss: 7.2915 - tree_height_mae: 0.0981 - tree_height_accuracy: 0.0764 - tree_binary_binary_crossentropy: 0.8766 - tree_binary_mean_io_u: 0.6249 - tree_binary_dice_metric: 0.4795 - vegetation_task_binary_crossentropy: 0.6452 - vegetation_task_mean_io_u_1: 0.7925 - vegetation_task_dice_metric: 0.9620 - val_loss: 34.0574 - val_tree_height_loss: 0.0138 - val_tree_binary_loss: 66.8668 - val_vegetation_task_loss: 6.1575 - val_tree_height_mae: 0.0762 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8554 - val_tree_binary_mean_io_u: 0.6341 - val_tree_binary_dice_metric: 0.4960 - val_vegetation_task_binary_crossentropy: 0.5223 - val_vegetation_task_mean_io_u_1: 0.8223 - val_vegetation_task_dice_metric: 0.9682\n",
      "Epoch 16/100\n",
      "597/597 [==============================] - 907s 2s/step - loss: 34.3659 - tree_height_loss: 0.0181 - tree_binary_loss: 67.3586 - vegetation_task_loss: 6.7571 - tree_height_mae: 0.0955 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8510 - tree_binary_mean_io_u: 0.6306 - tree_binary_dice_metric: 0.4901 - vegetation_task_binary_crossentropy: 0.5894 - vegetation_task_mean_io_u_1: 0.7978 - vegetation_task_dice_metric: 0.9650 - val_loss: 34.1841 - val_tree_height_loss: 0.0151 - val_tree_binary_loss: 67.0814 - val_vegetation_task_loss: 6.3438 - val_tree_height_mae: 0.0788 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8016 - val_tree_binary_mean_io_u: 0.6306 - val_tree_binary_dice_metric: 0.4934 - val_vegetation_task_binary_crossentropy: 0.5519 - val_vegetation_task_mean_io_u_1: 0.7830 - val_vegetation_task_dice_metric: 0.9672\n",
      "Epoch 17/100\n",
      "597/597 [==============================] - 903s 2s/step - loss: 34.0456 - tree_height_loss: 0.0162 - tree_binary_loss: 66.7646 - vegetation_task_loss: 6.5357 - tree_height_mae: 0.0899 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8293 - tree_binary_mean_io_u: 0.6343 - tree_binary_dice_metric: 0.4969 - vegetation_task_binary_crossentropy: 0.5733 - vegetation_task_mean_io_u_1: 0.8026 - vegetation_task_dice_metric: 0.9662 - val_loss: 33.8421 - val_tree_height_loss: 0.0144 - val_tree_binary_loss: 66.4186 - val_vegetation_task_loss: 6.2415 - val_tree_height_mae: 0.0938 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8226 - val_tree_binary_mean_io_u: 0.6367 - val_tree_binary_dice_metric: 0.5011 - val_vegetation_task_binary_crossentropy: 0.5674 - val_vegetation_task_mean_io_u_1: 0.8411 - val_vegetation_task_dice_metric: 0.9677\n",
      "Epoch 18/100\n",
      "597/597 [==============================] - 917s 2s/step - loss: 34.1288 - tree_height_loss: 0.0191 - tree_binary_loss: 66.8894 - vegetation_task_loss: 6.7263 - tree_height_mae: 0.0960 - tree_height_accuracy: 0.0764 - tree_binary_binary_crossentropy: 0.8372 - tree_binary_mean_io_u: 0.6338 - tree_binary_dice_metric: 0.4955 - vegetation_task_binary_crossentropy: 0.6067 - vegetation_task_mean_io_u_1: 0.7875 - vegetation_task_dice_metric: 0.9651 - val_loss: 34.5768 - val_tree_height_loss: 0.0147 - val_tree_binary_loss: 67.6853 - val_vegetation_task_loss: 7.2537 - val_tree_height_mae: 0.0755 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8456 - val_tree_binary_mean_io_u: 0.6301 - val_tree_binary_dice_metric: 0.4866 - val_vegetation_task_binary_crossentropy: 0.6680 - val_vegetation_task_mean_io_u_1: 0.8176 - val_vegetation_task_dice_metric: 0.9623\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597/597 [==============================] - 955s 2s/step - loss: 34.0339 - tree_height_loss: 0.0160 - tree_binary_loss: 66.7293 - vegetation_task_loss: 6.5964 - tree_height_mae: 0.0887 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8368 - tree_binary_mean_io_u: 0.6349 - tree_binary_dice_metric: 0.4973 - vegetation_task_binary_crossentropy: 0.6003 - vegetation_task_mean_io_u_1: 0.8241 - vegetation_task_dice_metric: 0.9658 - val_loss: 33.8434 - val_tree_height_loss: 0.0152 - val_tree_binary_loss: 66.4545 - val_vegetation_task_loss: 6.0712 - val_tree_height_mae: 0.0971 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8183 - val_tree_binary_mean_io_u: 0.6357 - val_tree_binary_dice_metric: 0.5006 - val_vegetation_task_binary_crossentropy: 0.5428 - val_vegetation_task_mean_io_u_1: 0.8499 - val_vegetation_task_dice_metric: 0.9686\n",
      "Epoch 20/100\n",
      "597/597 [==============================] - 935s 2s/step - loss: 33.8669 - tree_height_loss: 0.0184 - tree_binary_loss: 66.4302 - vegetation_task_loss: 6.4083 - tree_height_mae: 0.0944 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8281 - tree_binary_mean_io_u: 0.6368 - tree_binary_dice_metric: 0.5008 - vegetation_task_binary_crossentropy: 0.5830 - vegetation_task_mean_io_u_1: 0.8147 - vegetation_task_dice_metric: 0.9668 - val_loss: 33.8466 - val_tree_height_loss: 0.0130 - val_tree_binary_loss: 66.4696 - val_vegetation_task_loss: 6.0402 - val_tree_height_mae: 0.0757 - val_tree_height_accuracy: 0.0767 - val_tree_binary_binary_crossentropy: 0.8656 - val_tree_binary_mean_io_u: 0.6366 - val_tree_binary_dice_metric: 0.5005 - val_vegetation_task_binary_crossentropy: 0.5527 - val_vegetation_task_mean_io_u_1: 0.8445 - val_vegetation_task_dice_metric: 0.9688\n",
      "Epoch 21/100\n",
      "597/597 [==============================] - 901s 2s/step - loss: 33.8525 - tree_height_loss: 0.0172 - tree_binary_loss: 66.3925 - vegetation_task_loss: 6.4591 - tree_height_mae: 0.0928 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8308 - tree_binary_mean_io_u: 0.6372 - tree_binary_dice_metric: 0.5012 - vegetation_task_binary_crossentropy: 0.5994 - vegetation_task_mean_io_u_1: 0.8333 - vegetation_task_dice_metric: 0.9666 - val_loss: 33.7825 - val_tree_height_loss: 0.0191 - val_tree_binary_loss: 66.3057 - val_vegetation_task_loss: 6.1820 - val_tree_height_mae: 0.1157 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8074 - val_tree_binary_mean_io_u: 0.6371 - val_tree_binary_dice_metric: 0.5022 - val_vegetation_task_binary_crossentropy: 0.5951 - val_vegetation_task_mean_io_u_1: 0.8719 - val_vegetation_task_dice_metric: 0.9681\n",
      "Epoch 22/100\n",
      "597/597 [==============================] - 898s 2s/step - loss: 33.8554 - tree_height_loss: 0.0179 - tree_binary_loss: 66.3999 - vegetation_task_loss: 6.4471 - tree_height_mae: 0.0950 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8370 - tree_binary_mean_io_u: 0.6369 - tree_binary_dice_metric: 0.5010 - vegetation_task_binary_crossentropy: 0.6094 - vegetation_task_mean_io_u_1: 0.8343 - vegetation_task_dice_metric: 0.9666 - val_loss: 33.8444 - val_tree_height_loss: 0.0135 - val_tree_binary_loss: 66.4301 - val_vegetation_task_loss: 6.2125 - val_tree_height_mae: 0.0735 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.7958 - val_tree_binary_mean_io_u: 0.6365 - val_tree_binary_dice_metric: 0.5008 - val_vegetation_task_binary_crossentropy: 0.5812 - val_vegetation_task_mean_io_u_1: 0.8355 - val_vegetation_task_dice_metric: 0.9679\n",
      "Epoch 23/100\n",
      "597/597 [==============================] - 940s 2s/step - loss: 33.8539 - tree_height_loss: 0.0180 - tree_binary_loss: 66.3740 - vegetation_task_loss: 6.5608 - tree_height_mae: 0.0957 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8362 - tree_binary_mean_io_u: 0.6368 - tree_binary_dice_metric: 0.5013 - vegetation_task_binary_crossentropy: 0.6215 - vegetation_task_mean_io_u_1: 0.8451 - vegetation_task_dice_metric: 0.9660 - val_loss: 33.8798 - val_tree_height_loss: 0.0165 - val_tree_binary_loss: 66.5312 - val_vegetation_task_loss: 6.0431 - val_tree_height_mae: 0.0780 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.7936 - val_tree_binary_mean_io_u: 0.6355 - val_tree_binary_dice_metric: 0.4997 - val_vegetation_task_binary_crossentropy: 0.5678 - val_vegetation_task_mean_io_u_1: 0.8530 - val_vegetation_task_dice_metric: 0.9688\n",
      "Epoch 24/100\n",
      "597/597 [==============================] - 958s 2s/step - loss: 33.8183 - tree_height_loss: 0.0213 - tree_binary_loss: 66.3023 - vegetation_task_loss: 6.5438 - tree_height_mae: 0.0988 - tree_height_accuracy: 0.0763 - tree_binary_binary_crossentropy: 0.8387 - tree_binary_mean_io_u: 0.6374 - tree_binary_dice_metric: 0.5021 - vegetation_task_binary_crossentropy: 0.6285 - vegetation_task_mean_io_u_1: 0.8423 - vegetation_task_dice_metric: 0.9661 - val_loss: 33.6364 - val_tree_height_loss: 0.0160 - val_tree_binary_loss: 66.0435 - val_vegetation_task_loss: 6.0509 - val_tree_height_mae: 0.0804 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8677 - val_tree_binary_mean_io_u: 0.6390 - val_tree_binary_dice_metric: 0.5053 - val_vegetation_task_binary_crossentropy: 0.5736 - val_vegetation_task_mean_io_u_1: 0.8580 - val_vegetation_task_dice_metric: 0.9688\n",
      "Epoch 25/100\n",
      "597/597 [==============================] - 915s 2s/step - loss: 33.8702 - tree_height_loss: 0.0164 - tree_binary_loss: 66.4162 - vegetation_task_loss: 6.5223 - tree_height_mae: 0.0893 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8461 - tree_binary_mean_io_u: 0.6368 - tree_binary_dice_metric: 0.5008 - vegetation_task_binary_crossentropy: 0.6345 - vegetation_task_mean_io_u_1: 0.8538 - vegetation_task_dice_metric: 0.9662 - val_loss: 33.7835 - val_tree_height_loss: 0.0195 - val_tree_binary_loss: 66.3626 - val_vegetation_task_loss: 5.9048 - val_tree_height_mae: 0.0948 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8176 - val_tree_binary_mean_io_u: 0.6369 - val_tree_binary_dice_metric: 0.5015 - val_vegetation_task_binary_crossentropy: 0.5625 - val_vegetation_task_mean_io_u_1: 0.8568 - val_vegetation_task_dice_metric: 0.9695\n",
      "Epoch 26/100\n",
      "597/597 [==============================] - 932s 2s/step - loss: 33.7034 - tree_height_loss: 0.0182 - tree_binary_loss: 66.1053 - vegetation_task_loss: 6.3986 - tree_height_mae: 0.0949 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8389 - tree_binary_mean_io_u: 0.6387 - tree_binary_dice_metric: 0.5043 - vegetation_task_binary_crossentropy: 0.6196 - vegetation_task_mean_io_u_1: 0.8569 - vegetation_task_dice_metric: 0.9669 - val_loss: 33.9788 - val_tree_height_loss: 0.0160 - val_tree_binary_loss: 66.5321 - val_vegetation_task_loss: 7.0311 - val_tree_height_mae: 0.0772 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8344 - val_tree_binary_mean_io_u: 0.6354 - val_tree_binary_dice_metric: 0.4997 - val_vegetation_task_binary_crossentropy: 0.7029 - val_vegetation_task_mean_io_u_1: 0.8741 - val_vegetation_task_dice_metric: 0.9635\n",
      "Epoch 27/100\n",
      "597/597 [==============================] - 951s 2s/step - loss: 33.7381 - tree_height_loss: 0.0206 - tree_binary_loss: 66.1544 - vegetation_task_loss: 6.4855 - tree_height_mae: 0.1000 - tree_height_accuracy: 0.0763 - tree_binary_binary_crossentropy: 0.8417 - tree_binary_mean_io_u: 0.6383 - tree_binary_dice_metric: 0.5038 - vegetation_task_binary_crossentropy: 0.6302 - vegetation_task_mean_io_u_1: 0.8572 - vegetation_task_dice_metric: 0.9664 - val_loss: 33.7982 - val_tree_height_loss: 0.0177 - val_tree_binary_loss: 66.3476 - val_vegetation_task_loss: 6.1383 - val_tree_height_mae: 0.0981 - val_tree_height_accuracy: 0.0767 - val_tree_binary_binary_crossentropy: 0.8648 - val_tree_binary_mean_io_u: 0.6382 - val_tree_binary_dice_metric: 0.5019 - val_vegetation_task_binary_crossentropy: 0.6008 - val_vegetation_task_mean_io_u_1: 0.8322 - val_vegetation_task_dice_metric: 0.9683\n",
      "Epoch 28/100\n",
      "597/597 [==============================] - 956s 2s/step - loss: 33.5712 - tree_height_loss: 0.0214 - tree_binary_loss: 65.8613 - vegetation_task_loss: 6.2768 - tree_height_mae: 0.0940 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8337 - tree_binary_mean_io_u: 0.6403 - tree_binary_dice_metric: 0.5071 - vegetation_task_binary_crossentropy: 0.6148 - vegetation_task_mean_io_u_1: 0.8570 - vegetation_task_dice_metric: 0.9676 - val_loss: 33.7579 - val_tree_height_loss: 0.0125 - val_tree_binary_loss: 66.3051 - val_vegetation_task_loss: 5.9778 - val_tree_height_mae: 0.0803 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8281 - val_tree_binary_mean_io_u: 0.6370 - val_tree_binary_dice_metric: 0.5023 - val_vegetation_task_binary_crossentropy: 0.5904 - val_vegetation_task_mean_io_u_1: 0.8788 - val_vegetation_task_dice_metric: 0.9691\n",
      "Epoch 29/100\n",
      "597/597 [==============================] - 941s 2s/step - loss: 33.5421 - tree_height_loss: 0.0157 - tree_binary_loss: 65.8004 - vegetation_task_loss: 6.3242 - tree_height_mae: 0.0882 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8332 - tree_binary_mean_io_u: 0.6406 - tree_binary_dice_metric: 0.5077 - vegetation_task_binary_crossentropy: 0.6220 - vegetation_task_mean_io_u_1: 0.8643 - vegetation_task_dice_metric: 0.9673 - val_loss: 33.5530 - val_tree_height_loss: 0.0153 - val_tree_binary_loss: 65.8531 - val_vegetation_task_loss: 6.1728 - val_tree_height_mae: 0.0780 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8197 - val_tree_binary_mean_io_u: 0.6398 - val_tree_binary_dice_metric: 0.5073 - val_vegetation_task_binary_crossentropy: 0.6223 - val_vegetation_task_mean_io_u_1: 0.8843 - val_vegetation_task_dice_metric: 0.9681\n",
      "Epoch 30/100\n",
      "597/597 [==============================] - 943s 2s/step - loss: 33.5133 - tree_height_loss: 0.0163 - tree_binary_loss: 65.7404 - vegetation_task_loss: 6.3335 - tree_height_mae: 0.0904 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8322 - tree_binary_mean_io_u: 0.6410 - tree_binary_dice_metric: 0.5084 - vegetation_task_binary_crossentropy: 0.6266 - vegetation_task_mean_io_u_1: 0.8616 - vegetation_task_dice_metric: 0.9672 - val_loss: 33.6740 - val_tree_height_loss: 0.0163 - val_tree_binary_loss: 66.0675 - val_vegetation_task_loss: 6.3045 - val_tree_height_mae: 0.0831 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8582 - val_tree_binary_mean_io_u: 0.6387 - val_tree_binary_dice_metric: 0.5050 - val_vegetation_task_binary_crossentropy: 0.6194 - val_vegetation_task_mean_io_u_1: 0.8666 - val_vegetation_task_dice_metric: 0.9674\n",
      "Epoch 31/100\n",
      "597/597 [==============================] - 944s 2s/step - loss: 33.5935 - tree_height_loss: 0.0191 - tree_binary_loss: 65.8749 - vegetation_task_loss: 6.4459 - tree_height_mae: 0.0976 - tree_height_accuracy: 0.0764 - tree_binary_binary_crossentropy: 0.8404 - tree_binary_mean_io_u: 0.6403 - tree_binary_dice_metric: 0.5069 - vegetation_task_binary_crossentropy: 0.6448 - vegetation_task_mean_io_u_1: 0.8609 - vegetation_task_dice_metric: 0.9666 - val_loss: 33.6055 - val_tree_height_loss: 0.0142 - val_tree_binary_loss: 66.0231 - val_vegetation_task_loss: 5.8543 - val_tree_height_mae: 0.0955 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8436 - val_tree_binary_mean_io_u: 0.6389 - val_tree_binary_dice_metric: 0.5055 - val_vegetation_task_binary_crossentropy: 0.5797 - val_vegetation_task_mean_io_u_1: 0.8793 - val_vegetation_task_dice_metric: 0.9698\n",
      "Epoch 32/100\n",
      "597/597 [==============================] - 979s 2s/step - loss: 33.4888 - tree_height_loss: 0.0165 - tree_binary_loss: 65.6946 - vegetation_task_loss: 6.3153 - tree_height_mae: 0.0907 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8335 - tree_binary_mean_io_u: 0.6414 - tree_binary_dice_metric: 0.5090 - vegetation_task_binary_crossentropy: 0.6307 - vegetation_task_mean_io_u_1: 0.8702 - vegetation_task_dice_metric: 0.9673 - val_loss: 34.2719 - val_tree_height_loss: 0.0142 - val_tree_binary_loss: 66.9761 - val_vegetation_task_loss: 7.7541 - val_tree_height_mae: 0.0829 - val_tree_height_accuracy: 0.0767 - val_tree_binary_binary_crossentropy: 0.9455 - val_tree_binary_mean_io_u: 0.6342 - val_tree_binary_dice_metric: 0.4949 - val_vegetation_task_binary_crossentropy: 0.8002 - val_vegetation_task_mean_io_u_1: 0.8284 - val_vegetation_task_dice_metric: 0.9596\n",
      "Epoch 33/100\n",
      "597/597 [==============================] - 942s 2s/step - loss: 33.4999 - tree_height_loss: 0.0189 - tree_binary_loss: 65.7191 - vegetation_task_loss: 6.2905 - tree_height_mae: 0.0956 - tree_height_accuracy: 0.0764 - tree_binary_binary_crossentropy: 0.8385 - tree_binary_mean_io_u: 0.6412 - tree_binary_dice_metric: 0.5086 - vegetation_task_binary_crossentropy: 0.6329 - vegetation_task_mean_io_u_1: 0.8557 - vegetation_task_dice_metric: 0.9675 - val_loss: 34.1179 - val_tree_height_loss: 0.0150 - val_tree_binary_loss: 66.9250 - val_vegetation_task_loss: 6.4650 - val_tree_height_mae: 0.0930 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8788 - val_tree_binary_mean_io_u: 0.6329 - val_tree_binary_dice_metric: 0.4954 - val_vegetation_task_binary_crossentropy: 0.6560 - val_vegetation_task_mean_io_u_1: 0.8716 - val_vegetation_task_dice_metric: 0.9665\n",
      "Epoch 34/100\n",
      "597/597 [==============================] - 927s 2s/step - loss: 33.4770 - tree_height_loss: 0.0167 - tree_binary_loss: 65.6784 - vegetation_task_loss: 6.2778 - tree_height_mae: 0.0903 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8405 - tree_binary_mean_io_u: 0.6413 - tree_binary_dice_metric: 0.5091 - vegetation_task_binary_crossentropy: 0.6330 - vegetation_task_mean_io_u_1: 0.8715 - vegetation_task_dice_metric: 0.9675 - val_loss: 33.8914 - val_tree_height_loss: 0.0142 - val_tree_binary_loss: 66.5017 - val_vegetation_task_loss: 6.3208 - val_tree_height_mae: 0.0734 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.7708 - val_tree_binary_mean_io_u: 0.6366 - val_tree_binary_dice_metric: 0.4999 - val_vegetation_task_binary_crossentropy: 0.6551 - val_vegetation_task_mean_io_u_1: 0.8841 - val_vegetation_task_dice_metric: 0.9673\n",
      "Epoch 35/100\n",
      "597/597 [==============================] - 956s 2s/step - loss: 34.0752 - tree_height_loss: 0.0229 - tree_binary_loss: 66.6993 - vegetation_task_loss: 7.1180 - tree_height_mae: 0.1020 - tree_height_accuracy: 0.0763 - tree_binary_binary_crossentropy: 0.8705 - tree_binary_mean_io_u: 0.6347 - tree_binary_dice_metric: 0.4973 - vegetation_task_binary_crossentropy: 0.7379 - vegetation_task_mean_io_u_1: 0.8542 - vegetation_task_dice_metric: 0.9629 - val_loss: 33.5895 - val_tree_height_loss: 0.0126 - val_tree_binary_loss: 65.9608 - val_vegetation_task_loss: 6.0157 - val_tree_height_mae: 0.0725 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8731 - val_tree_binary_mean_io_u: 0.6396 - val_tree_binary_dice_metric: 0.5063 - val_vegetation_task_binary_crossentropy: 0.6105 - val_vegetation_task_mean_io_u_1: 0.8827 - val_vegetation_task_dice_metric: 0.9689\n",
      "Epoch 36/100\n",
      "597/597 [==============================] - 966s 2s/step - loss: 33.6210 - tree_height_loss: 0.0179 - tree_binary_loss: 65.9452 - vegetation_task_loss: 6.3762 - tree_height_mae: 0.0938 - tree_height_accuracy: 0.0764 - tree_binary_binary_crossentropy: 0.8482 - tree_binary_mean_io_u: 0.6398 - tree_binary_dice_metric: 0.5062 - vegetation_task_binary_crossentropy: 0.6501 - vegetation_task_mean_io_u_1: 0.8639 - vegetation_task_dice_metric: 0.9670 - val_loss: 33.5798 - val_tree_height_loss: 0.0134 - val_tree_binary_loss: 65.9489 - val_vegetation_task_loss: 5.9738 - val_tree_height_mae: 0.0881 - val_tree_height_accuracy: 0.0767 - val_tree_binary_binary_crossentropy: 0.8300 - val_tree_binary_mean_io_u: 0.6399 - val_tree_binary_dice_metric: 0.5063 - val_vegetation_task_binary_crossentropy: 0.6070 - val_vegetation_task_mean_io_u_1: 0.8777 - val_vegetation_task_dice_metric: 0.9692\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597/597 [==============================] - 970s 2s/step - loss: 33.4168 - tree_height_loss: 0.0176 - tree_binary_loss: 65.5585 - vegetation_task_loss: 6.2704 - tree_height_mae: 0.0929 - tree_height_accuracy: 0.0764 - tree_binary_binary_crossentropy: 0.8395 - tree_binary_mean_io_u: 0.6423 - tree_binary_dice_metric: 0.5105 - vegetation_task_binary_crossentropy: 0.6397 - vegetation_task_mean_io_u_1: 0.8712 - vegetation_task_dice_metric: 0.9676 - val_loss: 33.4470 - val_tree_height_loss: 0.0219 - val_tree_binary_loss: 65.6734 - val_vegetation_task_loss: 5.9715 - val_tree_height_mae: 0.1040 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8057 - val_tree_binary_mean_io_u: 0.6412 - val_tree_binary_dice_metric: 0.5093 - val_vegetation_task_binary_crossentropy: 0.6212 - val_vegetation_task_mean_io_u_1: 0.8873 - val_vegetation_task_dice_metric: 0.9692\n",
      "Epoch 38/100\n",
      "597/597 [==============================] - 941s 2s/step - loss: 33.4397 - tree_height_loss: 0.0188 - tree_binary_loss: 65.5992 - vegetation_task_loss: 6.2884 - tree_height_mae: 0.0956 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8388 - tree_binary_mean_io_u: 0.6421 - tree_binary_dice_metric: 0.5101 - vegetation_task_binary_crossentropy: 0.6485 - vegetation_task_mean_io_u_1: 0.8754 - vegetation_task_dice_metric: 0.9675 - val_loss: 33.7079 - val_tree_height_loss: 0.0170 - val_tree_binary_loss: 66.1932 - val_vegetation_task_loss: 6.0111 - val_tree_height_mae: 0.1019 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8536 - val_tree_binary_mean_io_u: 0.6383 - val_tree_binary_dice_metric: 0.5036 - val_vegetation_task_binary_crossentropy: 0.6200 - val_vegetation_task_mean_io_u_1: 0.8761 - val_vegetation_task_dice_metric: 0.9690\n",
      "Epoch 39/100\n",
      "597/597 [==============================] - 953s 2s/step - loss: 33.5225 - tree_height_loss: 0.0182 - tree_binary_loss: 65.7557 - vegetation_task_loss: 6.3375 - tree_height_mae: 0.0936 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8462 - tree_binary_mean_io_u: 0.6408 - tree_binary_dice_metric: 0.5082 - vegetation_task_binary_crossentropy: 0.6593 - vegetation_task_mean_io_u_1: 0.8742 - vegetation_task_dice_metric: 0.9672 - val_loss: 33.6005 - val_tree_height_loss: 0.0136 - val_tree_binary_loss: 65.9487 - val_vegetation_task_loss: 6.1796 - val_tree_height_mae: 0.0806 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8919 - val_tree_binary_mean_io_u: 0.6387 - val_tree_binary_dice_metric: 0.5064 - val_vegetation_task_binary_crossentropy: 0.6557 - val_vegetation_task_mean_io_u_1: 0.8832 - val_vegetation_task_dice_metric: 0.9681\n",
      "Epoch 40/100\n",
      "597/597 [==============================] - 964s 2s/step - loss: 33.4885 - tree_height_loss: 0.0211 - tree_binary_loss: 65.6880 - vegetation_task_loss: 6.3186 - tree_height_mae: 0.1012 - tree_height_accuracy: 0.0764 - tree_binary_binary_crossentropy: 0.8489 - tree_binary_mean_io_u: 0.6415 - tree_binary_dice_metric: 0.5090 - vegetation_task_binary_crossentropy: 0.6570 - vegetation_task_mean_io_u_1: 0.8733 - vegetation_task_dice_metric: 0.9673 - val_loss: 33.5068 - val_tree_height_loss: 0.0160 - val_tree_binary_loss: 65.8134 - val_vegetation_task_loss: 5.9046 - val_tree_height_mae: 0.0789 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8210 - val_tree_binary_mean_io_u: 0.6408 - val_tree_binary_dice_metric: 0.5077 - val_vegetation_task_binary_crossentropy: 0.6114 - val_vegetation_task_mean_io_u_1: 0.8835 - val_vegetation_task_dice_metric: 0.9695\n",
      "Epoch 41/100\n",
      "597/597 [==============================] - 951s 2s/step - loss: 33.3660 - tree_height_loss: 0.0170 - tree_binary_loss: 65.4608 - vegetation_task_loss: 6.2546 - tree_height_mae: 0.0916 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8401 - tree_binary_mean_io_u: 0.6427 - tree_binary_dice_metric: 0.5115 - vegetation_task_binary_crossentropy: 0.6506 - vegetation_task_mean_io_u_1: 0.8738 - vegetation_task_dice_metric: 0.9677 - val_loss: 33.7776 - val_tree_height_loss: 0.0205 - val_tree_binary_loss: 66.3379 - val_vegetation_task_loss: 5.9638 - val_tree_height_mae: 0.0973 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8419 - val_tree_binary_mean_io_u: 0.6374 - val_tree_binary_dice_metric: 0.5020 - val_vegetation_task_binary_crossentropy: 0.6339 - val_vegetation_task_mean_io_u_1: 0.8863 - val_vegetation_task_dice_metric: 0.9692\n",
      "Epoch 42/100\n",
      "597/597 [==============================] - 1245s 2s/step - loss: 33.5601 - tree_height_loss: 0.0206 - tree_binary_loss: 65.8076 - vegetation_task_loss: 6.4396 - tree_height_mae: 0.1002 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8545 - tree_binary_mean_io_u: 0.6404 - tree_binary_dice_metric: 0.5076 - vegetation_task_binary_crossentropy: 0.6771 - vegetation_task_mean_io_u_1: 0.8741 - vegetation_task_dice_metric: 0.9667 - val_loss: 34.0390 - val_tree_height_loss: 0.0158 - val_tree_binary_loss: 66.8417 - val_vegetation_task_loss: 6.0863 - val_tree_height_mae: 0.0974 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.9147 - val_tree_binary_mean_io_u: 0.6335 - val_tree_binary_dice_metric: 0.4964 - val_vegetation_task_binary_crossentropy: 0.6465 - val_vegetation_task_mean_io_u_1: 0.8842 - val_vegetation_task_dice_metric: 0.9686\n",
      "Epoch 43/100\n",
      "597/597 [==============================] - 960s 2s/step - loss: 33.5407 - tree_height_loss: 0.0197 - tree_binary_loss: 65.7686 - vegetation_task_loss: 6.4456 - tree_height_mae: 0.0984 - tree_height_accuracy: 0.0764 - tree_binary_binary_crossentropy: 0.8531 - tree_binary_mean_io_u: 0.6407 - tree_binary_dice_metric: 0.5081 - vegetation_task_binary_crossentropy: 0.6808 - vegetation_task_mean_io_u_1: 0.8751 - vegetation_task_dice_metric: 0.9666 - val_loss: 33.4956 - val_tree_height_loss: 0.0145 - val_tree_binary_loss: 65.7950 - val_vegetation_task_loss: 5.8947 - val_tree_height_mae: 0.0935 - val_tree_height_accuracy: 0.0767 - val_tree_binary_binary_crossentropy: 0.8685 - val_tree_binary_mean_io_u: 0.6402 - val_tree_binary_dice_metric: 0.5081 - val_vegetation_task_binary_crossentropy: 0.6152 - val_vegetation_task_mean_io_u_1: 0.8864 - val_vegetation_task_dice_metric: 0.9696\n",
      "Epoch 44/100\n",
      "597/597 [==============================] - 956s 2s/step - loss: 33.5322 - tree_height_loss: 0.0183 - tree_binary_loss: 65.7639 - vegetation_task_loss: 6.3934 - tree_height_mae: 0.0939 - tree_height_accuracy: 0.0764 - tree_binary_binary_crossentropy: 0.8518 - tree_binary_mean_io_u: 0.6406 - tree_binary_dice_metric: 0.5081 - vegetation_task_binary_crossentropy: 0.6705 - vegetation_task_mean_io_u_1: 0.8754 - vegetation_task_dice_metric: 0.9669 - val_loss: 34.1534 - val_tree_height_loss: 0.0124 - val_tree_binary_loss: 66.8001 - val_vegetation_task_loss: 7.4589 - val_tree_height_mae: 0.0722 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8708 - val_tree_binary_mean_io_u: 0.6355 - val_tree_binary_dice_metric: 0.4967 - val_vegetation_task_binary_crossentropy: 0.7861 - val_vegetation_task_mean_io_u_1: 0.8499 - val_vegetation_task_dice_metric: 0.9612\n",
      "Epoch 45/100\n",
      "597/597 [==============================] - 945s 2s/step - loss: 33.3905 - tree_height_loss: 0.0173 - tree_binary_loss: 65.5083 - vegetation_task_loss: 6.2604 - tree_height_mae: 0.0916 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8427 - tree_binary_mean_io_u: 0.6427 - tree_binary_dice_metric: 0.5110 - vegetation_task_binary_crossentropy: 0.6622 - vegetation_task_mean_io_u_1: 0.8782 - vegetation_task_dice_metric: 0.9676 - val_loss: 33.8378 - val_tree_height_loss: 0.0132 - val_tree_binary_loss: 66.2589 - val_vegetation_task_loss: 7.0042 - val_tree_height_mae: 0.0746 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.7895 - val_tree_binary_mean_io_u: 0.6378 - val_tree_binary_dice_metric: 0.5026 - val_vegetation_task_binary_crossentropy: 0.7742 - val_vegetation_task_mean_io_u_1: 0.8712 - val_vegetation_task_dice_metric: 0.9636\n",
      "Epoch 46/100\n",
      "597/597 [==============================] - 937s 2s/step - loss: 33.4464 - tree_height_loss: 0.0185 - tree_binary_loss: 65.6018 - vegetation_task_loss: 6.3447 - tree_height_mae: 0.0946 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8475 - tree_binary_mean_io_u: 0.6421 - tree_binary_dice_metric: 0.5100 - vegetation_task_binary_crossentropy: 0.6758 - vegetation_task_mean_io_u_1: 0.8762 - vegetation_task_dice_metric: 0.9672 - val_loss: 33.4187 - val_tree_height_loss: 0.0158 - val_tree_binary_loss: 65.6477 - val_vegetation_task_loss: 5.8540 - val_tree_height_mae: 0.0795 - val_tree_height_accuracy: 0.0769 - val_tree_binary_binary_crossentropy: 0.8604 - val_tree_binary_mean_io_u: 0.6415 - val_tree_binary_dice_metric: 0.5098 - val_vegetation_task_binary_crossentropy: 0.6236 - val_vegetation_task_mean_io_u_1: 0.8872 - val_vegetation_task_dice_metric: 0.9698\n",
      "Epoch 47/100\n",
      "597/597 [==============================] - 934s 2s/step - loss: 33.3692 - tree_height_loss: 0.0200 - tree_binary_loss: 65.4706 - vegetation_task_loss: 6.2189 - tree_height_mae: 0.0991 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.8480 - tree_binary_mean_io_u: 0.6425 - tree_binary_dice_metric: 0.5114 - vegetation_task_binary_crossentropy: 0.6574 - vegetation_task_mean_io_u_1: 0.8790 - vegetation_task_dice_metric: 0.9679 - val_loss: 33.6069 - val_tree_height_loss: 0.0143 - val_tree_binary_loss: 66.0047 - val_vegetation_task_loss: 5.9601 - val_tree_height_mae: 0.0738 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8524 - val_tree_binary_mean_io_u: 0.6389 - val_tree_binary_dice_metric: 0.5056 - val_vegetation_task_binary_crossentropy: 0.6291 - val_vegetation_task_mean_io_u_1: 0.8853 - val_vegetation_task_dice_metric: 0.9692\n",
      "Epoch 48/100\n",
      "597/597 [==============================] - ETA: 0s - loss: 33.3954 - tree_height_loss: 0.0214 - tree_binary_loss: 65.5173 - vegetation_task_loss: 6.2394 - tree_height_mae: 0.0990 - tree_height_accuracy: 0.0763 - tree_binary_binary_crossentropy: 0.8456 - tree_binary_mean_io_u: 0.6426 - tree_binary_dice_metric: 0.5110 - vegetation_task_binary_crossentropy: 0.6622 - vegetation_task_mean_io_u_1: 0.8784 - vegetation_task_dice_metric: 0.9678"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\keras\\engine\\training.py:1445\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1432\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1433\u001b[0m       x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1434\u001b[0m       y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1443\u001b[0m       model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1444\u001b[0m       steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution)\n\u001b[1;32m-> 1445\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1447\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1450\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1454\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1457\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m   1458\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\keras\\engine\\training.py:1756\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1755\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 1756\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1757\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1758\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_tree_binary_mean_io_u\", patience=5, restore_best_weights=True,\n",
    "                                           mode=\"max\", verbose=1)\n",
    "\n",
    "# Train our model\n",
    "history=model_multi_all_shared.fit(\n",
    "    training_generator,\n",
    "    epochs=100,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callback\n",
    ")\n",
    "\n",
    "# model_multi_all_shared.save('C:/Users/johnf/Documents/UCL/thesis/code/models/multi_all_shared')\n",
    "\n",
    "# hist_df = pd.DataFrame(history.history)\n",
    "\n",
    "# hist_df\n",
    "# hist_df.to_csv('C:/Users/johnf/Documents/UCL/thesis/code/models/multi_all_shared_hist.csv')  \n",
    "\n",
    "# hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b4aafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tf.reduce_sum(tf.random.normal([1000, 1000])))\n",
    "# print(tf.config.list_physical_devices('GPU'))\n",
    "# len(tf.config.list_physical_devices('GPU')) > 0 \n",
    "\n",
    "\n",
    "\n",
    "params = {'batch_size':8,\n",
    "         'shuffle': True}\n",
    "\n",
    "# need a dictionary which has a list of training paths and a list of validations paths\n",
    "filelist_temp = filelist\n",
    "np.random.seed(14)\n",
    "mask = np.random.rand(len(filelist_temp)) <=.75\n",
    "\n",
    "training_data = np.array(filelist_temp)[mask]\n",
    "val_data = np.array(filelist_temp)[~mask]\n",
    "\n",
    "\n",
    "mydict = {}\n",
    "mydict[\"training\"] = training_data\n",
    "mydict[\"validation\"] = val_data\n",
    "\n",
    "# generators\n",
    "training_generator = DataGenerator(mydict[\"training\"], **params)\n",
    "val_generator = DataGenerator(mydict[\"validation\"], **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8bad97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# define U-Net model architecture - proof_concept_2\n",
    "def build_unet(img_shape):\n",
    "    # input layer shape is equal to patch image size\n",
    "    inputs = layers.Input(shape=img_shape)\n",
    "\n",
    "    # rescale images from (0, 255) to (0, 1)\n",
    " #   rescale = Rescaling(scale=1. / 255, input_shape=(img_height, img_width, img_channels))(inputs)\n",
    " #   previous_block_activation = rescale  # Set aside residual\n",
    "    previous_block_activation = inputs\n",
    "\n",
    "    contraction = {}\n",
    "    # # Contraction path: Blocks 1 through 5 are identical apart from the feature depth\n",
    "    for f in [32, 64]:\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(previous_block_activation)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        contraction[f'conv{f}'] = x\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        previous_block_activation = x\n",
    "\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(previous_block_activation)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    contraction[f'conv{128}'] = x\n",
    "    x = layers.MaxPooling2D((3, 3))(x)\n",
    "    previous_block_activation = x\n",
    "        \n",
    "    c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(previous_block_activation)\n",
    "    c5 = layers.Dropout(0.2)(c5)\n",
    "    c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "    previous_block_activation0 = c5\n",
    "    \n",
    "    x = layers.Conv2DTranspose(128, (2, 2), strides=(3, 3), padding='same')(previous_block_activation0)\n",
    "    x = layers.concatenate([x, contraction[f'conv{128}']])\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    previous_block_activation_1 = x\n",
    "    \n",
    "    x = layers.Conv2DTranspose(128, (2, 2), strides=(3, 3), padding='same')(previous_block_activation0)\n",
    "    x = layers.concatenate([x, contraction[f'conv{128}']])\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    previous_block_activation_2 = x\n",
    "    \n",
    "    x = layers.Conv2DTranspose(128, (2, 2), strides=(3, 3), padding='same')(previous_block_activation0)\n",
    "    x = layers.concatenate([x, contraction[f'conv{128}']])\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    previous_block_activation_3 = x  \n",
    "\n",
    "    # Expansive path: Second half of the network: upsampling inputs\n",
    "    # could we use upsampling layers here instead of Conv2dTRanspose layers? might that help\n",
    "    for f in reversed([32, 64]):\n",
    "        x = layers.Conv2DTranspose(f, (2, 2), strides=(2, 2), padding='same')(previous_block_activation_1)\n",
    "        x = layers.concatenate([x, contraction[f'conv{f}']])\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        previous_block_activation_1 = x\n",
    "\n",
    "            # Expansive path: Second half of the network: upsampling inputs\n",
    "    # could we use upsampling layers here instead of Conv2dTRanspose layers? might that help\n",
    "    for f in reversed([32, 64]):\n",
    "        x = layers.Conv2DTranspose(f, (2, 2), strides=(2, 2), padding='same')(previous_block_activation_2)\n",
    "        x = layers.concatenate([x, contraction[f'conv{f}']])\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        previous_block_activation_2 = x\n",
    "\n",
    "            # Expansive path: Second half of the network: upsampling inputs\n",
    "    # could we use upsampling layers here instead of Conv2dTRanspose layers? might that help\n",
    "    for f in reversed([32, 64]):\n",
    "        x = layers.Conv2DTranspose(f, (2, 2), strides=(2, 2), padding='same')(previous_block_activation_3)\n",
    "        x = layers.concatenate([x, contraction[f'conv{f}']])\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        previous_block_activation_3 = x\n",
    "        \n",
    "    output_tree_height = layers.Conv2D(filters=1, kernel_size=(1, 1), activation=\"linear\", name=\"tree_height\")(previous_block_activation_1)\n",
    "    output_tree_binary = layers.Conv2D(filters=1, kernel_size=(1, 1), activation=\"sigmoid\", name=\"tree_binary\")(previous_block_activation_2)    \n",
    "    output_vegetation_task = layers.Conv2D(filters=1, kernel_size=(1, 1), activation=\"sigmoid\", name=\"vegetation_task\")(previous_block_activation_3)\n",
    "\n",
    "\n",
    "    return Model(inputs=inputs, outputs=[output_tree_height,output_tree_binary,output_vegetation_task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de98183b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 240, 240, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 240, 240, 32  896         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 240, 240, 32  0           ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 240, 240, 32  9248        ['dropout[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 120, 120, 32  0           ['conv2d_1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 120, 120, 64  18496       ['max_pooling2d[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 120, 120, 64  0           ['conv2d_2[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 120, 120, 64  36928       ['dropout_1[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 60, 60, 64)  0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 60, 60, 128)  73856       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 60, 60, 128)  0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 60, 60, 128)  147584      ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 20, 20, 128)  0          ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 20, 20, 256)  295168      ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 20, 20, 256)  0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 20, 20, 256)  590080      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 60, 60, 128)  131200     ['conv2d_7[0][0]']               \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 60, 60, 128)  131200     ['conv2d_7[0][0]']               \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 60, 60, 128)  131200     ['conv2d_7[0][0]']               \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 60, 60, 256)  0           ['conv2d_transpose[0][0]',       \n",
      "                                                                  'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 60, 60, 256)  0           ['conv2d_transpose_1[0][0]',     \n",
      "                                                                  'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 60, 60, 256)  0           ['conv2d_transpose_2[0][0]',     \n",
      "                                                                  'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 60, 60, 128)  295040      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 60, 60, 128)  295040      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 60, 60, 128)  295040      ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 60, 60, 128)  0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 60, 60, 128)  0           ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 60, 60, 128)  0           ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 60, 60, 128)  147584      ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 60, 60, 128)  147584      ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 60, 60, 128)  147584      ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 120, 120, 64  32832      ['conv2d_9[0][0]']               \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_5 (Conv2DTran  (None, 120, 120, 64  32832      ['conv2d_11[0][0]']              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_7 (Conv2DTran  (None, 120, 120, 64  32832      ['conv2d_13[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 120, 120, 12  0           ['conv2d_transpose_3[0][0]',     \n",
      "                                8)                                'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 120, 120, 12  0           ['conv2d_transpose_5[0][0]',     \n",
      "                                8)                                'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 120, 120, 12  0           ['conv2d_transpose_7[0][0]',     \n",
      "                                8)                                'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 120, 120, 64  73792       ['concatenate_3[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 120, 120, 64  73792       ['concatenate_5[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 120, 120, 64  73792       ['concatenate_7[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 120, 120, 64  0           ['conv2d_14[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 120, 120, 64  0           ['conv2d_18[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 120, 120, 64  0           ['conv2d_22[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 120, 120, 64  36928       ['dropout_7[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 120, 120, 64  36928       ['dropout_9[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 120, 120, 64  36928       ['dropout_11[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2DTran  (None, 240, 240, 32  8224       ['conv2d_15[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_6 (Conv2DTran  (None, 240, 240, 32  8224       ['conv2d_19[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_8 (Conv2DTran  (None, 240, 240, 32  8224       ['conv2d_23[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 240, 240, 64  0           ['conv2d_transpose_4[0][0]',     \n",
      "                                )                                 'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 240, 240, 64  0           ['conv2d_transpose_6[0][0]',     \n",
      "                                )                                 'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 240, 240, 64  0           ['conv2d_transpose_8[0][0]',     \n",
      "                                )                                 'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 240, 240, 32  18464       ['concatenate_4[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 240, 240, 32  18464       ['concatenate_6[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 240, 240, 32  18464       ['concatenate_8[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 240, 240, 32  0           ['conv2d_16[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 240, 240, 32  0           ['conv2d_20[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 240, 240, 32  0           ['conv2d_24[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 240, 240, 32  9248        ['dropout_8[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 240, 240, 32  9248        ['dropout_10[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d_25 (Conv2D)             (None, 240, 240, 32  9248        ['dropout_12[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tree_height (Conv2D)           (None, 240, 240, 1)  33          ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " tree_binary (Conv2D)           (None, 240, 240, 1)  33          ['conv2d_21[0][0]']              \n",
      "                                                                                                  \n",
      " vegetation_task (Conv2D)       (None, 240, 240, 1)  33          ['conv2d_25[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,432,291\n",
      "Trainable params: 3,432,291\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model_multi_partial_shared= build_unet(img_shape=(240, 240, 3))\n",
    "model_multi_partial_shared.summary()\n",
    "# plot_model(model_unet_multitask_noweights,\"multi_task_model_fuller_attempt.png\" , show_shapes=True, show_layer_names=True)\n",
    "model_multi_partial_shared.compile(optimizer=\"adam\", \n",
    "              loss={'tree_height': 'mse',\n",
    "                    'tree_binary': jaccard_distance_loss,\n",
    "                    'vegetation_task': jaccard_distance_loss,},\n",
    "               loss_weights={'tree_binary': .6, \n",
    "                             'tree_height': .6,\n",
    "                             'vegetation_task': .1,},\n",
    "              metrics={'tree_height': [\"mae\", 'accuracy'],\n",
    "                       'tree_binary': [tf.keras.metrics.BinaryCrossentropy(),tf.keras.metrics.MeanIoU(num_classes=2),dice_metric], \n",
    "                       'vegetation_task': [tf.keras.metrics.BinaryCrossentropy(),tf.keras.metrics.MeanIoU(num_classes=2),dice_metric]}\n",
    ")\n",
    "#print(model_multi_all_shared.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "440c44c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "896/896 [==============================] - 868s 960ms/step - loss: 59.5797 - tree_height_loss: 0.0189 - tree_binary_loss: 95.1461 - vegetation_task_loss: 24.8071 - tree_height_mae: 0.1009 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 14.0768 - tree_binary_mean_io_u: 0.0400 - tree_binary_dice_metric: 0.0920 - vegetation_task_binary_crossentropy: 3.7666 - vegetation_task_mean_io_u_1: 0.3766 - vegetation_task_dice_metric: 0.8570 - val_loss: 59.5899 - val_tree_height_loss: 0.0186 - val_tree_binary_loss: 95.1574 - val_vegetation_task_loss: 24.8439 - val_tree_height_mae: 0.0974 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 14.5139 - val_tree_binary_mean_io_u: 0.0241 - val_tree_binary_dice_metric: 0.0918 - val_vegetation_task_binary_crossentropy: 3.7893 - val_vegetation_task_mean_io_u_1: 0.3758 - val_vegetation_task_dice_metric: 0.8567\n",
      "Epoch 2/100\n",
      "896/896 [==============================] - 911s 1s/step - loss: 49.2869 - tree_height_loss: 0.0176 - tree_binary_loss: 79.9361 - vegetation_task_loss: 13.1470 - tree_height_mae: 0.0982 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 3.6031 - tree_binary_mean_io_u: 0.4257 - tree_binary_dice_metric: 0.3243 - vegetation_task_binary_crossentropy: 1.2848 - vegetation_task_mean_io_u_1: 0.6771 - vegetation_task_dice_metric: 0.9281 - val_loss: 45.3327 - val_tree_height_loss: 0.0164 - val_tree_binary_loss: 74.1546 - val_vegetation_task_loss: 8.3010 - val_tree_height_mae: 0.0909 - val_tree_height_accuracy: 0.0769 - val_tree_binary_binary_crossentropy: 0.9045 - val_tree_binary_mean_io_u: 0.5843 - val_tree_binary_dice_metric: 0.4079 - val_vegetation_task_binary_crossentropy: 0.5585 - val_vegetation_task_mean_io_u_1: 0.7518 - val_vegetation_task_dice_metric: 0.9566\n",
      "Epoch 3/100\n",
      "896/896 [==============================] - 919s 1s/step - loss: 44.8886 - tree_height_loss: 0.0158 - tree_binary_loss: 73.3745 - vegetation_task_loss: 8.5439 - tree_height_mae: 0.0905 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 1.0152 - tree_binary_mean_io_u: 0.5923 - tree_binary_dice_metric: 0.4177 - vegetation_task_binary_crossentropy: 0.6025 - vegetation_task_mean_io_u_1: 0.7710 - vegetation_task_dice_metric: 0.9552 - val_loss: 44.4292 - val_tree_height_loss: 0.0145 - val_tree_binary_loss: 72.7520 - val_vegetation_task_loss: 7.6924 - val_tree_height_mae: 0.0833 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8925 - val_tree_binary_mean_io_u: 0.5950 - val_tree_binary_dice_metric: 0.4255 - val_vegetation_task_binary_crossentropy: 0.5494 - val_vegetation_task_mean_io_u_1: 0.7846 - val_vegetation_task_dice_metric: 0.9599\n",
      "Epoch 4/100\n",
      "896/896 [==============================] - 921s 1s/step - loss: 44.0910 - tree_height_loss: 0.0149 - tree_binary_loss: 72.1477 - vegetation_task_loss: 7.9351 - tree_height_mae: 0.0869 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9787 - tree_binary_mean_io_u: 0.6002 - tree_binary_dice_metric: 0.4328 - vegetation_task_binary_crossentropy: 0.6282 - vegetation_task_mean_io_u_1: 0.8103 - vegetation_task_dice_metric: 0.9586 - val_loss: 43.6356 - val_tree_height_loss: 0.0142 - val_tree_binary_loss: 71.4822 - val_vegetation_task_loss: 7.3778 - val_tree_height_mae: 0.0825 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 1.0007 - val_tree_binary_mean_io_u: 0.6043 - val_tree_binary_dice_metric: 0.4413 - val_vegetation_task_binary_crossentropy: 0.6064 - val_vegetation_task_mean_io_u_1: 0.8121 - val_vegetation_task_dice_metric: 0.9616\n",
      "Epoch 5/100\n",
      "896/896 [==============================] - 904s 1s/step - loss: 43.6678 - tree_height_loss: 0.0144 - tree_binary_loss: 71.4887 - vegetation_task_loss: 7.6599 - tree_height_mae: 0.0841 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9700 - tree_binary_mean_io_u: 0.6048 - tree_binary_dice_metric: 0.4408 - vegetation_task_binary_crossentropy: 0.6540 - vegetation_task_mean_io_u_1: 0.8284 - vegetation_task_dice_metric: 0.9601 - val_loss: 43.2535 - val_tree_height_loss: 0.0136 - val_tree_binary_loss: 70.9035 - val_vegetation_task_loss: 7.0324 - val_tree_height_mae: 0.0817 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.9332 - val_tree_binary_mean_io_u: 0.6086 - val_tree_binary_dice_metric: 0.4482 - val_vegetation_task_binary_crossentropy: 0.6066 - val_vegetation_task_mean_io_u_1: 0.8500 - val_vegetation_task_dice_metric: 0.9635\n",
      "Epoch 6/100\n",
      "896/896 [==============================] - 922s 1s/step - loss: 43.2782 - tree_height_loss: 0.0135 - tree_binary_loss: 70.8797 - vegetation_task_loss: 7.4226 - tree_height_mae: 0.0810 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9686 - tree_binary_mean_io_u: 0.6090 - tree_binary_dice_metric: 0.4481 - vegetation_task_binary_crossentropy: 0.6564 - vegetation_task_mean_io_u_1: 0.8380 - vegetation_task_dice_metric: 0.9614 - val_loss: 43.0575 - val_tree_height_loss: 0.0140 - val_tree_binary_loss: 70.6022 - val_vegetation_task_loss: 6.8773 - val_tree_height_mae: 0.0764 - val_tree_height_accuracy: 0.0769 - val_tree_binary_binary_crossentropy: 0.9139 - val_tree_binary_mean_io_u: 0.6098 - val_tree_binary_dice_metric: 0.4517 - val_vegetation_task_binary_crossentropy: 0.6082 - val_vegetation_task_mean_io_u_1: 0.8572 - val_vegetation_task_dice_metric: 0.9643\n",
      "Epoch 7/100\n",
      "896/896 [==============================] - 933s 1s/step - loss: 42.9979 - tree_height_loss: 0.0137 - tree_binary_loss: 70.4472 - vegetation_task_loss: 7.2132 - tree_height_mae: 0.0809 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9753 - tree_binary_mean_io_u: 0.6113 - tree_binary_dice_metric: 0.4534 - vegetation_task_binary_crossentropy: 0.6578 - vegetation_task_mean_io_u_1: 0.8463 - vegetation_task_dice_metric: 0.9625 - val_loss: 42.4890 - val_tree_height_loss: 0.0127 - val_tree_binary_loss: 69.6890 - val_vegetation_task_loss: 6.6794 - val_tree_height_mae: 0.0758 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.9714 - val_tree_binary_mean_io_u: 0.6160 - val_tree_binary_dice_metric: 0.4626 - val_vegetation_task_binary_crossentropy: 0.6119 - val_vegetation_task_mean_io_u_1: 0.8629 - val_vegetation_task_dice_metric: 0.9654\n",
      "Epoch 8/100\n",
      "896/896 [==============================] - 928s 1s/step - loss: 42.6863 - tree_height_loss: 0.0133 - tree_binary_loss: 69.9353 - vegetation_task_loss: 7.1708 - tree_height_mae: 0.0791 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9643 - tree_binary_mean_io_u: 0.6147 - tree_binary_dice_metric: 0.4593 - vegetation_task_binary_crossentropy: 0.6726 - vegetation_task_mean_io_u_1: 0.8508 - vegetation_task_dice_metric: 0.9627 - val_loss: 42.0085 - val_tree_height_loss: 0.0121 - val_tree_binary_loss: 68.9317 - val_vegetation_task_loss: 6.4223 - val_tree_height_mae: 0.0763 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.9267 - val_tree_binary_mean_io_u: 0.6208 - val_tree_binary_dice_metric: 0.4715 - val_vegetation_task_binary_crossentropy: 0.5966 - val_vegetation_task_mean_io_u_1: 0.8608 - val_vegetation_task_dice_metric: 0.9668\n",
      "Epoch 9/100\n",
      "896/896 [==============================] - 913s 1s/step - loss: 42.4010 - tree_height_loss: 0.0132 - tree_binary_loss: 69.4787 - vegetation_task_loss: 7.0588 - tree_height_mae: 0.0781 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9501 - tree_binary_mean_io_u: 0.6178 - tree_binary_dice_metric: 0.4648 - vegetation_task_binary_crossentropy: 0.6798 - vegetation_task_mean_io_u_1: 0.8560 - vegetation_task_dice_metric: 0.9633 - val_loss: 42.5898 - val_tree_height_loss: 0.0130 - val_tree_binary_loss: 69.7955 - val_vegetation_task_loss: 7.0470 - val_tree_height_mae: 0.0706 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.8939 - val_tree_binary_mean_io_u: 0.6159 - val_tree_binary_dice_metric: 0.4611 - val_vegetation_task_binary_crossentropy: 0.7046 - val_vegetation_task_mean_io_u_1: 0.8731 - val_vegetation_task_dice_metric: 0.9634\n",
      "Epoch 10/100\n",
      "896/896 [==============================] - 922s 1s/step - loss: 42.1824 - tree_height_loss: 0.0127 - tree_binary_loss: 69.1419 - vegetation_task_loss: 6.8963 - tree_height_mae: 0.0758 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9497 - tree_binary_mean_io_u: 0.6197 - tree_binary_dice_metric: 0.4687 - vegetation_task_binary_crossentropy: 0.6716 - vegetation_task_mean_io_u_1: 0.8603 - vegetation_task_dice_metric: 0.9642 - val_loss: 41.8858 - val_tree_height_loss: 0.0127 - val_tree_binary_loss: 68.7189 - val_vegetation_task_loss: 6.4676 - val_tree_height_mae: 0.0715 - val_tree_height_accuracy: 0.0767 - val_tree_binary_binary_crossentropy: 0.8961 - val_tree_binary_mean_io_u: 0.6227 - val_tree_binary_dice_metric: 0.4738 - val_vegetation_task_binary_crossentropy: 0.6143 - val_vegetation_task_mean_io_u_1: 0.8698 - val_vegetation_task_dice_metric: 0.9665\n",
      "Epoch 11/100\n",
      "896/896 [==============================] - 929s 1s/step - loss: 42.0689 - tree_height_loss: 0.0124 - tree_binary_loss: 68.9597 - vegetation_task_loss: 6.8562 - tree_height_mae: 0.0744 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9416 - tree_binary_mean_io_u: 0.6211 - tree_binary_dice_metric: 0.4709 - vegetation_task_binary_crossentropy: 0.6738 - vegetation_task_mean_io_u_1: 0.8624 - vegetation_task_dice_metric: 0.9644 - val_loss: 41.6127 - val_tree_height_loss: 0.0118 - val_tree_binary_loss: 68.2378 - val_vegetation_task_loss: 6.6296 - val_tree_height_mae: 0.0744 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.9499 - val_tree_binary_mean_io_u: 0.6249 - val_tree_binary_dice_metric: 0.4795 - val_vegetation_task_binary_crossentropy: 0.6493 - val_vegetation_task_mean_io_u_1: 0.8575 - val_vegetation_task_dice_metric: 0.9656\n",
      "Epoch 12/100\n",
      "896/896 [==============================] - 936s 1s/step - loss: 41.9817 - tree_height_loss: 0.0124 - tree_binary_loss: 68.8246 - vegetation_task_loss: 6.7953 - tree_height_mae: 0.0748 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9514 - tree_binary_mean_io_u: 0.6215 - tree_binary_dice_metric: 0.4724 - vegetation_task_binary_crossentropy: 0.6756 - vegetation_task_mean_io_u_1: 0.8645 - vegetation_task_dice_metric: 0.9647 - val_loss: 41.6799 - val_tree_height_loss: 0.0122 - val_tree_binary_loss: 68.3648 - val_vegetation_task_loss: 6.5369 - val_tree_height_mae: 0.0743 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.9540 - val_tree_binary_mean_io_u: 0.6236 - val_tree_binary_dice_metric: 0.4780 - val_vegetation_task_binary_crossentropy: 0.6486 - val_vegetation_task_mean_io_u_1: 0.8780 - val_vegetation_task_dice_metric: 0.9661\n",
      "Epoch 13/100\n",
      "896/896 [==============================] - 910s 1s/step - loss: 41.8686 - tree_height_loss: 0.0127 - tree_binary_loss: 68.6480 - vegetation_task_loss: 6.7213 - tree_height_mae: 0.0752 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9499 - tree_binary_mean_io_u: 0.6225 - tree_binary_dice_metric: 0.4745 - vegetation_task_binary_crossentropy: 0.6787 - vegetation_task_mean_io_u_1: 0.8672 - vegetation_task_dice_metric: 0.9651 - val_loss: 41.6097 - val_tree_height_loss: 0.0125 - val_tree_binary_loss: 68.2689 - val_vegetation_task_loss: 6.4092 - val_tree_height_mae: 0.0719 - val_tree_height_accuracy: 0.0767 - val_tree_binary_binary_crossentropy: 0.9870 - val_tree_binary_mean_io_u: 0.6238 - val_tree_binary_dice_metric: 0.4793 - val_vegetation_task_binary_crossentropy: 0.6523 - val_vegetation_task_mean_io_u_1: 0.8726 - val_vegetation_task_dice_metric: 0.9668\n",
      "Epoch 14/100\n",
      "896/896 [==============================] - 910s 1s/step - loss: 41.9891 - tree_height_loss: 0.0121 - tree_binary_loss: 68.8259 - vegetation_task_loss: 6.8631 - tree_height_mae: 0.0733 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9578 - tree_binary_mean_io_u: 0.6217 - tree_binary_dice_metric: 0.4725 - vegetation_task_binary_crossentropy: 0.7029 - vegetation_task_mean_io_u_1: 0.8655 - vegetation_task_dice_metric: 0.9644 - val_loss: 41.6844 - val_tree_height_loss: 0.0130 - val_tree_binary_loss: 68.4132 - val_vegetation_task_loss: 6.2869 - val_tree_height_mae: 0.0808 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.9614 - val_tree_binary_mean_io_u: 0.6232 - val_tree_binary_dice_metric: 0.4776 - val_vegetation_task_binary_crossentropy: 0.6443 - val_vegetation_task_mean_io_u_1: 0.8748 - val_vegetation_task_dice_metric: 0.9675\n",
      "Epoch 15/100\n",
      "896/896 [==============================] - 931s 1s/step - loss: 41.7400 - tree_height_loss: 0.0117 - tree_binary_loss: 68.4515 - vegetation_task_loss: 6.6206 - tree_height_mae: 0.0716 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9478 - tree_binary_mean_io_u: 0.6239 - tree_binary_dice_metric: 0.4767 - vegetation_task_binary_crossentropy: 0.6809 - vegetation_task_mean_io_u_1: 0.8708 - vegetation_task_dice_metric: 0.9657 - val_loss: 41.6038 - val_tree_height_loss: 0.0141 - val_tree_binary_loss: 68.2910 - val_vegetation_task_loss: 6.2072 - val_tree_height_mae: 0.0874 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 1.0454 - val_tree_binary_mean_io_u: 0.6223 - val_tree_binary_dice_metric: 0.4791 - val_vegetation_task_binary_crossentropy: 0.6395 - val_vegetation_task_mean_io_u_1: 0.8804 - val_vegetation_task_dice_metric: 0.9679\n",
      "Epoch 16/100\n",
      "896/896 [==============================] - 906s 1s/step - loss: 41.7693 - tree_height_loss: 0.0119 - tree_binary_loss: 68.4901 - vegetation_task_loss: 6.6808 - tree_height_mae: 0.0720 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9553 - tree_binary_mean_io_u: 0.6233 - tree_binary_dice_metric: 0.4763 - vegetation_task_binary_crossentropy: 0.6948 - vegetation_task_mean_io_u_1: 0.8703 - vegetation_task_dice_metric: 0.9654 - val_loss: 41.2609 - val_tree_height_loss: 0.0121 - val_tree_binary_loss: 67.7300 - val_vegetation_task_loss: 6.1562 - val_tree_height_mae: 0.0741 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.9042 - val_tree_binary_mean_io_u: 0.6284 - val_tree_binary_dice_metric: 0.4853 - val_vegetation_task_binary_crossentropy: 0.6248 - val_vegetation_task_mean_io_u_1: 0.8800 - val_vegetation_task_dice_metric: 0.9682\n",
      "Epoch 17/100\n",
      "896/896 [==============================] - 924s 1s/step - loss: 41.6836 - tree_height_loss: 0.0120 - tree_binary_loss: 68.3606 - vegetation_task_loss: 6.6005 - tree_height_mae: 0.0722 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9549 - tree_binary_mean_io_u: 0.6239 - tree_binary_dice_metric: 0.4778 - vegetation_task_binary_crossentropy: 0.6833 - vegetation_task_mean_io_u_1: 0.8716 - vegetation_task_dice_metric: 0.9658 - val_loss: 42.1189 - val_tree_height_loss: 0.0138 - val_tree_binary_loss: 68.9913 - val_vegetation_task_loss: 7.1585 - val_tree_height_mae: 0.0682 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 1.0164 - val_tree_binary_mean_io_u: 0.6193 - val_tree_binary_dice_metric: 0.4708 - val_vegetation_task_binary_crossentropy: 0.7432 - val_vegetation_task_mean_io_u_1: 0.8534 - val_vegetation_task_dice_metric: 0.9628\n",
      "Epoch 18/100\n",
      "896/896 [==============================] - 932s 1s/step - loss: 41.7865 - tree_height_loss: 0.0119 - tree_binary_loss: 68.5183 - vegetation_task_loss: 6.6842 - tree_height_mae: 0.0720 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9543 - tree_binary_mean_io_u: 0.6233 - tree_binary_dice_metric: 0.4759 - vegetation_task_binary_crossentropy: 0.7021 - vegetation_task_mean_io_u_1: 0.8704 - vegetation_task_dice_metric: 0.9653 - val_loss: 41.1973 - val_tree_height_loss: 0.0117 - val_tree_binary_loss: 67.6028 - val_vegetation_task_loss: 6.2868 - val_tree_height_mae: 0.0677 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.9264 - val_tree_binary_mean_io_u: 0.6287 - val_tree_binary_dice_metric: 0.4868 - val_vegetation_task_binary_crossentropy: 0.6612 - val_vegetation_task_mean_io_u_1: 0.8830 - val_vegetation_task_dice_metric: 0.9675\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896/896 [==============================] - 925s 1s/step - loss: 41.7348 - tree_height_loss: 0.0118 - tree_binary_loss: 68.4356 - vegetation_task_loss: 6.6635 - tree_height_mae: 0.0712 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9568 - tree_binary_mean_io_u: 0.6237 - tree_binary_dice_metric: 0.4769 - vegetation_task_binary_crossentropy: 0.7024 - vegetation_task_mean_io_u_1: 0.8711 - vegetation_task_dice_metric: 0.9654 - val_loss: 41.6431 - val_tree_height_loss: 0.0122 - val_tree_binary_loss: 68.2829 - val_vegetation_task_loss: 6.6608 - val_tree_height_mae: 0.0696 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.9947 - val_tree_binary_mean_io_u: 0.6233 - val_tree_binary_dice_metric: 0.4791 - val_vegetation_task_binary_crossentropy: 0.7073 - val_vegetation_task_mean_io_u_1: 0.8780 - val_vegetation_task_dice_metric: 0.9655\n",
      "Epoch 20/100\n",
      "896/896 [==============================] - 932s 1s/step - loss: 41.7497 - tree_height_loss: 0.0117 - tree_binary_loss: 68.4527 - vegetation_task_loss: 6.7111 - tree_height_mae: 0.0711 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9571 - tree_binary_mean_io_u: 0.6238 - tree_binary_dice_metric: 0.4768 - vegetation_task_binary_crossentropy: 0.7112 - vegetation_task_mean_io_u_1: 0.8704 - vegetation_task_dice_metric: 0.9652 - val_loss: 41.4486 - val_tree_height_loss: 0.0112 - val_tree_binary_loss: 67.9848 - val_vegetation_task_loss: 6.5095 - val_tree_height_mae: 0.0689 - val_tree_height_accuracy: 0.0767 - val_tree_binary_binary_crossentropy: 0.9562 - val_tree_binary_mean_io_u: 0.6256 - val_tree_binary_dice_metric: 0.4824 - val_vegetation_task_binary_crossentropy: 0.6908 - val_vegetation_task_mean_io_u_1: 0.8810 - val_vegetation_task_dice_metric: 0.9663\n",
      "Epoch 21/100\n",
      "896/896 [==============================] - 933s 1s/step - loss: 41.6040 - tree_height_loss: 0.0117 - tree_binary_loss: 68.2351 - vegetation_task_loss: 6.5592 - tree_height_mae: 0.0711 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9549 - tree_binary_mean_io_u: 0.6248 - tree_binary_dice_metric: 0.4792 - vegetation_task_binary_crossentropy: 0.6925 - vegetation_task_mean_io_u_1: 0.8732 - vegetation_task_dice_metric: 0.9660 - val_loss: 41.2046 - val_tree_height_loss: 0.0113 - val_tree_binary_loss: 67.6458 - val_vegetation_task_loss: 6.1035 - val_tree_height_mae: 0.0691 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.9443 - val_tree_binary_mean_io_u: 0.6279 - val_tree_binary_dice_metric: 0.4863 - val_vegetation_task_binary_crossentropy: 0.6396 - val_vegetation_task_mean_io_u_1: 0.8832 - val_vegetation_task_dice_metric: 0.9684\n",
      "Epoch 22/100\n",
      "896/896 [==============================] - 921s 1s/step - loss: 41.6090 - tree_height_loss: 0.0117 - tree_binary_loss: 68.2354 - vegetation_task_loss: 6.6077 - tree_height_mae: 0.0711 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9491 - tree_binary_mean_io_u: 0.6251 - tree_binary_dice_metric: 0.4792 - vegetation_task_binary_crossentropy: 0.7008 - vegetation_task_mean_io_u_1: 0.8729 - vegetation_task_dice_metric: 0.9657 - val_loss: 41.5195 - val_tree_height_loss: 0.0109 - val_tree_binary_loss: 68.0746 - val_vegetation_task_loss: 6.6817 - val_tree_height_mae: 0.0685 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.9137 - val_tree_binary_mean_io_u: 0.6267 - val_tree_binary_dice_metric: 0.4813 - val_vegetation_task_binary_crossentropy: 0.7346 - val_vegetation_task_mean_io_u_1: 0.8763 - val_vegetation_task_dice_metric: 0.9654\n",
      "Epoch 23/100\n",
      "896/896 [==============================] - ETA: 0s - loss: 41.6790 - tree_height_loss: 0.0118 - tree_binary_loss: 68.3519 - vegetation_task_loss: 6.6083 - tree_height_mae: 0.0713 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9657 - tree_binary_mean_io_u: 0.6239 - tree_binary_dice_metric: 0.4779 - vegetation_task_binary_crossentropy: 0.7056 - vegetation_task_mean_io_u_1: 0.8730 - vegetation_task_dice_metric: 0.9657Restoring model weights from the end of the best epoch: 18.\n",
      "896/896 [==============================] - 943s 1s/step - loss: 41.6790 - tree_height_loss: 0.0118 - tree_binary_loss: 68.3519 - vegetation_task_loss: 6.6083 - tree_height_mae: 0.0713 - tree_height_accuracy: 0.0765 - tree_binary_binary_crossentropy: 0.9657 - tree_binary_mean_io_u: 0.6239 - tree_binary_dice_metric: 0.4779 - vegetation_task_binary_crossentropy: 0.7056 - vegetation_task_mean_io_u_1: 0.8730 - vegetation_task_dice_metric: 0.9657 - val_loss: 41.5348 - val_tree_height_loss: 0.0114 - val_tree_binary_loss: 68.1915 - val_vegetation_task_loss: 6.1299 - val_tree_height_mae: 0.0665 - val_tree_height_accuracy: 0.0768 - val_tree_binary_binary_crossentropy: 0.9588 - val_tree_binary_mean_io_u: 0.6247 - val_tree_binary_dice_metric: 0.4800 - val_vegetation_task_binary_crossentropy: 0.6531 - val_vegetation_task_mean_io_u_1: 0.8838 - val_vegetation_task_dice_metric: 0.9683\n",
      "Epoch 23: early stopping\n",
      "CPU times: total: 3h 44min 3s\n",
      "Wall time: 5h 52min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Train our model\n",
    "history=model_multi_partial_shared.fit(\n",
    "    training_generator,\n",
    "    epochs=100,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callback\n",
    ")\n",
    "\n",
    "# model_multi_partial_shared.save('C:/Users/johnf/Documents/UCL/thesis/code/models/multi_partial_shared')\n",
    "\n",
    "# hist_df = pd.DataFrame(history.history)\n",
    "\n",
    "# hist_df\n",
    "# hist_df.to_csv('C:/Users/johnf/Documents/UCL/thesis/code/models/multi_partial_shared_hist.csv')  \n",
    "\n",
    "# need to do three separate models with all of the data to compare against..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa20eb0",
   "metadata": {},
   "source": [
    "###### tree yes/no #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cb61ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=25, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        images = []\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            dataset = gdal.Open(ID)\n",
    "            image = dataset.ReadAsArray()  # Returned image is a NumPy array with shape (16, 60, 60) for example.\n",
    "            images.append(image)  # Append the NumPy array to the list.\n",
    "\n",
    "        all_data= np.stack(images, axis= 0)\n",
    "        all_data[all_data < .0000001] = 0\n",
    "        X=all_data[:,:14,:,:] # separate out the band values\n",
    "        X = np.transpose(X, axes=[0, 2, 3, 1])\n",
    "        # normalize values of the input data to 0,1\n",
    "        X = X/X.max(axis=(3),keepdims=1)\n",
    "        # For RGB uncomment this\n",
    "        X = X[:,:,:,:3]\n",
    "        \n",
    "        # canopy_height,tree/not tree,ndvi\n",
    "        all_data= np.stack( images, axis= 0)\n",
    "        Y = all_data[:,14:]\n",
    "        Y[Y  < .0000001] = 0\n",
    "        Y[:,1][Y[:,1]  > 1] = 1\n",
    "        Y=Y[:,1]\n",
    "        self.X=X\n",
    "        self.Y=Y\n",
    "\n",
    "        return X, Y\n",
    "    \n",
    "    def get_true_values_x(self, indexes):\n",
    "        'Generate one batch of data'\n",
    "        # Return validation data for plotting \n",
    "\n",
    "        # Find list of IDs - (give input of validation indexes)\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        \n",
    "        # only need to return the RGB data for plotting\n",
    "        X = X[:,:,:,0:3]\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def get_true_values_y(self, indexes):\n",
    "        'Generate one batch of data'\n",
    "        # Return validation data for plotting \n",
    "\n",
    "        # Find list of IDs - (give input of validation indexes)\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d712806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size':12,\n",
    "         'shuffle': True}\n",
    "\n",
    "# need a dictionary which has a list of training paths and a list of validations paths\n",
    "filelist_temp = filelist\n",
    "np.random.seed(14)\n",
    "mask = np.random.rand(len(filelist_temp)) <=.75\n",
    "\n",
    "training_data = np.array(filelist_temp)[mask]\n",
    "val_data = np.array(filelist_temp)[~mask]\n",
    "\n",
    "\n",
    "mydict = {}\n",
    "mydict[\"training\"] = training_data\n",
    "mydict[\"validation\"] = val_data\n",
    "\n",
    "# generators\n",
    "training_generator = DataGenerator(mydict[\"training\"], **params)\n",
    "val_generator = DataGenerator(mydict[\"validation\"], **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2df8e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# define U-Net model architecture - proof_concept_2\n",
    "\n",
    "def build_unet(img_shape):\n",
    "    # input layer shape is equal to patch image size\n",
    "    inputs = layers.Input(shape=img_shape)\n",
    "\n",
    "    # rescale images from (0, 255) to (0, 1)\n",
    " #   rescale = Rescaling(scale=1. / 255, input_shape=(img_height, img_width, img_channels))(inputs)\n",
    " #   previous_block_activation = rescale  # Set aside residual\n",
    "    previous_block_activation = inputs\n",
    "\n",
    "    contraction = {}\n",
    "    # # Contraction path: Blocks 1 through 5 are identical apart from the feature depth\n",
    "    for f in [32, 64]:\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(previous_block_activation)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        contraction[f'conv{f}'] = x\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        previous_block_activation = x\n",
    "\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(previous_block_activation)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    contraction[f'conv{128}'] = x\n",
    "    x = layers.MaxPooling2D((3, 3))(x)\n",
    "    previous_block_activation = x\n",
    "        \n",
    "    c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(previous_block_activation)\n",
    "    c5 = layers.Dropout(0.2)(c5)\n",
    "    c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "    previous_block_activation = c5\n",
    "    \n",
    "    x = layers.Conv2DTranspose(128, (2, 2), strides=(3, 3), padding='same')(previous_block_activation)\n",
    "    x = layers.concatenate([x, contraction[f'conv{128}']])\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    previous_block_activation = x\n",
    "        \n",
    "    # Expansive path: Second half of the network: upsampling inputs\n",
    "    # could we use upsampling layers here instead of Conv2dTRanspose layers? might that help\n",
    "    for f in reversed([32, 64]):\n",
    "        x = layers.Conv2DTranspose(f, (2, 2), strides=(2, 2), padding='same')(previous_block_activation)\n",
    "        x = layers.concatenate([x, contraction[f'conv{f}']])\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        previous_block_activation = x\n",
    "\n",
    "    outputs = layers.Conv2D(filters=1, kernel_size=(1, 1), activation=\"sigmoid\")(previous_block_activation)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25343152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 240, 240, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 240, 240, 32  896         ['input_2[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 240, 240, 32  0           ['conv2d_26[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 240, 240, 32  9248        ['dropout_13[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 120, 120, 32  0          ['conv2d_27[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 120, 120, 64  18496       ['max_pooling2d_3[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 120, 120, 64  0           ['conv2d_28[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 120, 120, 64  36928       ['dropout_14[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 60, 60, 64)  0           ['conv2d_29[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 60, 60, 128)  73856       ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 60, 60, 128)  0           ['conv2d_30[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 60, 60, 128)  147584      ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 20, 20, 128)  0          ['conv2d_31[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 20, 20, 256)  295168      ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 20, 20, 256)  0           ['conv2d_32[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 20, 20, 256)  590080      ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_transpose_9 (Conv2DTran  (None, 60, 60, 128)  131200     ['conv2d_33[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 60, 60, 256)  0           ['conv2d_transpose_9[0][0]',     \n",
      "                                                                  'conv2d_31[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 60, 60, 128)  295040      ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 60, 60, 128)  0           ['conv2d_34[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 60, 60, 128)  147584      ['dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_transpose_10 (Conv2DTra  (None, 120, 120, 64  32832      ['conv2d_35[0][0]']              \n",
      " nspose)                        )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 120, 120, 12  0           ['conv2d_transpose_10[0][0]',    \n",
      "                                8)                                'conv2d_29[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 120, 120, 64  73792       ['concatenate_10[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 120, 120, 64  0           ['conv2d_36[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 120, 120, 64  36928       ['dropout_18[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_11 (Conv2DTra  (None, 240, 240, 32  8224       ['conv2d_37[0][0]']              \n",
      " nspose)                        )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 240, 240, 64  0           ['conv2d_transpose_11[0][0]',    \n",
      "                                )                                 'conv2d_27[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 240, 240, 32  18464       ['concatenate_11[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 240, 240, 32  0           ['conv2d_38[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 240, 240, 32  9248        ['dropout_19[0][0]']             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 240, 240, 1)  33          ['conv2d_39[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,925,601\n",
      "Trainable params: 1,925,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "597/597 [==============================] - 921s 2s/step - loss: 0.1396 - binary_crossentropy: 0.1396 - accuracy: 0.9518 - mean_io_u_2: 0.4759 - val_loss: 0.1232 - val_binary_crossentropy: 0.1232 - val_accuracy: 0.9518 - val_mean_io_u_2: 0.4759\n",
      "Epoch 2/100\n",
      "597/597 [==============================] - 916s 2s/step - loss: 0.1224 - binary_crossentropy: 0.1224 - accuracy: 0.9521 - mean_io_u_2: 0.4759 - val_loss: 0.1196 - val_binary_crossentropy: 0.1196 - val_accuracy: 0.9529 - val_mean_io_u_2: 0.4759\n",
      "Epoch 3/100\n",
      "597/597 [==============================] - 913s 2s/step - loss: 0.1170 - binary_crossentropy: 0.1170 - accuracy: 0.9532 - mean_io_u_2: 0.4759 - val_loss: 0.1132 - val_binary_crossentropy: 0.1132 - val_accuracy: 0.9539 - val_mean_io_u_2: 0.4759\n",
      "Epoch 4/100\n",
      "597/597 [==============================] - 929s 2s/step - loss: 0.1147 - binary_crossentropy: 0.1147 - accuracy: 0.9537 - mean_io_u_2: 0.4759 - val_loss: 0.1133 - val_binary_crossentropy: 0.1133 - val_accuracy: 0.9536 - val_mean_io_u_2: 0.4759\n",
      "Epoch 5/100\n",
      "597/597 [==============================] - 924s 2s/step - loss: 0.1117 - binary_crossentropy: 0.1117 - accuracy: 0.9542 - mean_io_u_2: 0.4759 - val_loss: 0.1089 - val_binary_crossentropy: 0.1089 - val_accuracy: 0.9549 - val_mean_io_u_2: 0.4759\n",
      "Epoch 6/100\n",
      "597/597 [==============================] - 914s 2s/step - loss: 0.1095 - binary_crossentropy: 0.1095 - accuracy: 0.9546 - mean_io_u_2: 0.4759 - val_loss: 0.1079 - val_binary_crossentropy: 0.1079 - val_accuracy: 0.9553 - val_mean_io_u_2: 0.4759\n",
      "Epoch 7/100\n",
      "597/597 [==============================] - 906s 2s/step - loss: 0.1158 - binary_crossentropy: 0.1158 - accuracy: 0.9540 - mean_io_u_2: 0.4759 - val_loss: 0.1124 - val_binary_crossentropy: 0.1124 - val_accuracy: 0.9545 - val_mean_io_u_2: 0.4759\n",
      "Epoch 8/100\n",
      "597/597 [==============================] - 903s 2s/step - loss: 0.1137 - binary_crossentropy: 0.1137 - accuracy: 0.9542 - mean_io_u_2: 0.4759 - val_loss: 0.1144 - val_binary_crossentropy: 0.1144 - val_accuracy: 0.9543 - val_mean_io_u_2: 0.4759\n",
      "Epoch 9/100\n",
      "597/597 [==============================] - 924s 2s/step - loss: 0.1086 - binary_crossentropy: 0.1086 - accuracy: 0.9550 - mean_io_u_2: 0.4759 - val_loss: 0.1052 - val_binary_crossentropy: 0.1052 - val_accuracy: 0.9556 - val_mean_io_u_2: 0.4759\n",
      "Epoch 10/100\n",
      "597/597 [==============================] - 916s 2s/step - loss: 0.1076 - binary_crossentropy: 0.1076 - accuracy: 0.9551 - mean_io_u_2: 0.4759 - val_loss: 0.1073 - val_binary_crossentropy: 0.1073 - val_accuracy: 0.9550 - val_mean_io_u_2: 0.4759\n",
      "Epoch 11/100\n",
      "597/597 [==============================] - 908s 2s/step - loss: 0.1057 - binary_crossentropy: 0.1057 - accuracy: 0.9556 - mean_io_u_2: 0.4759 - val_loss: 0.1046 - val_binary_crossentropy: 0.1046 - val_accuracy: 0.9559 - val_mean_io_u_2: 0.4759\n",
      "Epoch 12/100\n",
      "597/597 [==============================] - 914s 2s/step - loss: 0.1077 - binary_crossentropy: 0.1077 - accuracy: 0.9554 - mean_io_u_2: 0.4759 - val_loss: 0.1055 - val_binary_crossentropy: 0.1055 - val_accuracy: 0.9556 - val_mean_io_u_2: 0.4759\n",
      "Epoch 13/100\n",
      "597/597 [==============================] - 930s 2s/step - loss: 0.1116 - binary_crossentropy: 0.1116 - accuracy: 0.9548 - mean_io_u_2: 0.4759 - val_loss: 0.1066 - val_binary_crossentropy: 0.1066 - val_accuracy: 0.9557 - val_mean_io_u_2: 0.4759\n",
      "Epoch 14/100\n",
      "597/597 [==============================] - 911s 2s/step - loss: 0.1090 - binary_crossentropy: 0.1090 - accuracy: 0.9552 - mean_io_u_2: 0.4759 - val_loss: 0.1064 - val_binary_crossentropy: 0.1064 - val_accuracy: 0.9556 - val_mean_io_u_2: 0.4759\n",
      "Epoch 15/100\n",
      "597/597 [==============================] - 926s 2s/step - loss: 0.1052 - binary_crossentropy: 0.1052 - accuracy: 0.9558 - mean_io_u_2: 0.4759 - val_loss: 0.1040 - val_binary_crossentropy: 0.1040 - val_accuracy: 0.9563 - val_mean_io_u_2: 0.4759\n",
      "Epoch 16/100\n",
      "597/597 [==============================] - 907s 2s/step - loss: 0.1043 - binary_crossentropy: 0.1043 - accuracy: 0.9560 - mean_io_u_2: 0.4759 - val_loss: 0.1033 - val_binary_crossentropy: 0.1033 - val_accuracy: 0.9562 - val_mean_io_u_2: 0.4759\n",
      "Epoch 17/100\n",
      "597/597 [==============================] - 906s 2s/step - loss: 0.1030 - binary_crossentropy: 0.1030 - accuracy: 0.9563 - mean_io_u_2: 0.4759 - val_loss: 0.1027 - val_binary_crossentropy: 0.1027 - val_accuracy: 0.9566 - val_mean_io_u_2: 0.4759\n",
      "Epoch 18/100\n",
      "597/597 [==============================] - 918s 2s/step - loss: 0.1031 - binary_crossentropy: 0.1031 - accuracy: 0.9564 - mean_io_u_2: 0.4759 - val_loss: 0.1034 - val_binary_crossentropy: 0.1034 - val_accuracy: 0.9563 - val_mean_io_u_2: 0.4759\n",
      "Epoch 19/100\n",
      "597/597 [==============================] - 1097s 2s/step - loss: 0.1093 - binary_crossentropy: 0.1093 - accuracy: 0.9554 - mean_io_u_2: 0.4759 - val_loss: 0.1063 - val_binary_crossentropy: 0.1063 - val_accuracy: 0.9559 - val_mean_io_u_2: 0.4759\n",
      "Epoch 20/100\n",
      "597/597 [==============================] - 1105s 2s/step - loss: 0.1042 - binary_crossentropy: 0.1042 - accuracy: 0.9561 - mean_io_u_2: 0.4759 - val_loss: 0.1033 - val_binary_crossentropy: 0.1033 - val_accuracy: 0.9563 - val_mean_io_u_2: 0.4759\n",
      "Epoch 21/100\n",
      "597/597 [==============================] - 912s 2s/step - loss: 0.1017 - binary_crossentropy: 0.1017 - accuracy: 0.9567 - mean_io_u_2: 0.4759 - val_loss: 0.1019 - val_binary_crossentropy: 0.1019 - val_accuracy: 0.9566 - val_mean_io_u_2: 0.4759\n",
      "Epoch 22/100\n",
      "597/597 [==============================] - 915s 2s/step - loss: 0.1022 - binary_crossentropy: 0.1022 - accuracy: 0.9567 - mean_io_u_2: 0.4759 - val_loss: 0.1024 - val_binary_crossentropy: 0.1024 - val_accuracy: 0.9567 - val_mean_io_u_2: 0.4759\n",
      "Epoch 23/100\n",
      "597/597 [==============================] - 915s 2s/step - loss: 0.1023 - binary_crossentropy: 0.1023 - accuracy: 0.9567 - mean_io_u_2: 0.4759 - val_loss: 0.1066 - val_binary_crossentropy: 0.1066 - val_accuracy: 0.9557 - val_mean_io_u_2: 0.4759\n",
      "Epoch 24/100\n",
      "597/597 [==============================] - 912s 2s/step - loss: 0.1044 - binary_crossentropy: 0.1044 - accuracy: 0.9563 - mean_io_u_2: 0.4759 - val_loss: 0.1030 - val_binary_crossentropy: 0.1030 - val_accuracy: 0.9564 - val_mean_io_u_2: 0.4759\n",
      "Epoch 25/100\n",
      "597/597 [==============================] - 903s 2s/step - loss: 0.1020 - binary_crossentropy: 0.1020 - accuracy: 0.9565 - mean_io_u_2: 0.4759 - val_loss: 0.1016 - val_binary_crossentropy: 0.1016 - val_accuracy: 0.9568 - val_mean_io_u_2: 0.4759\n",
      "Epoch 26/100\n",
      "597/597 [==============================] - 925s 2s/step - loss: 0.1009 - binary_crossentropy: 0.1009 - accuracy: 0.9570 - mean_io_u_2: 0.4759 - val_loss: 0.1016 - val_binary_crossentropy: 0.1016 - val_accuracy: 0.9569 - val_mean_io_u_2: 0.4759\n",
      "Epoch 27/100\n",
      "597/597 [==============================] - 919s 2s/step - loss: 0.1000 - binary_crossentropy: 0.1000 - accuracy: 0.9571 - mean_io_u_2: 0.4759 - val_loss: 0.1007 - val_binary_crossentropy: 0.1007 - val_accuracy: 0.9570 - val_mean_io_u_2: 0.4759\n",
      "Epoch 28/100\n",
      "597/597 [==============================] - 921s 2s/step - loss: 0.1019 - binary_crossentropy: 0.1019 - accuracy: 0.9568 - mean_io_u_2: 0.4759 - val_loss: 0.1023 - val_binary_crossentropy: 0.1023 - val_accuracy: 0.9568 - val_mean_io_u_2: 0.4759\n",
      "Epoch 29/100\n",
      "597/597 [==============================] - 907s 2s/step - loss: 0.1014 - binary_crossentropy: 0.1014 - accuracy: 0.9569 - mean_io_u_2: 0.4759 - val_loss: 0.1012 - val_binary_crossentropy: 0.1012 - val_accuracy: 0.9568 - val_mean_io_u_2: 0.4759\n",
      "Epoch 30/100\n",
      "597/597 [==============================] - 925s 2s/step - loss: 0.1000 - binary_crossentropy: 0.1000 - accuracy: 0.9572 - mean_io_u_2: 0.4759 - val_loss: 0.1008 - val_binary_crossentropy: 0.1008 - val_accuracy: 0.9570 - val_mean_io_u_2: 0.4759\n",
      "Epoch 31/100\n",
      "597/597 [==============================] - 921s 2s/step - loss: 0.0992 - binary_crossentropy: 0.0992 - accuracy: 0.9575 - mean_io_u_2: 0.4759 - val_loss: 0.1020 - val_binary_crossentropy: 0.1020 - val_accuracy: 0.9566 - val_mean_io_u_2: 0.4759\n",
      "Epoch 32/100\n",
      "597/597 [==============================] - 921s 2s/step - loss: 0.0988 - binary_crossentropy: 0.0988 - accuracy: 0.9576 - mean_io_u_2: 0.4759 - val_loss: 0.1014 - val_binary_crossentropy: 0.1014 - val_accuracy: 0.9569 - val_mean_io_u_2: 0.4759\n",
      "CPU times: total: 4h 1min 13s\n",
      "Wall time: 8h 14min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# build model\n",
    "model_unet_tree_footprint_all = build_unet(img_shape=(240, 240, 3))\n",
    "model_unet_tree_footprint_all.summary()\n",
    "\n",
    "# compile model\n",
    "model_unet_tree_footprint_all.compile(optimizer=\"adam\",\n",
    "              loss=\"binary_crossentropy\", \n",
    "              metrics=[tf.keras.metrics.BinaryCrossentropy(),'accuracy',tf.keras.metrics.MeanIoU(num_classes=2)])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train our model\n",
    "history=model_unet_tree_footprint_all.fit(\n",
    "    training_generator,\n",
    "    epochs=100,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callback\n",
    ")\n",
    "\n",
    "\n",
    "# model_unet_tree_footprint_all.save('C:/Users/johnf/Documents/UCL/thesis/code/models/tree_model')\n",
    "\n",
    "# hist_df = pd.DataFrame(history.history)\n",
    "\n",
    "# hist_df\n",
    "#hist_df.to_csv('C:/Users/johnf/Documents/UCL/thesis/code/models/tree_binary_hist.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bca06aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Canopy Height\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=25, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        images = []\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            dataset = gdal.Open(ID)\n",
    "            image = dataset.ReadAsArray()  # Returned image is a NumPy array with shape (16, 60, 60) for example.\n",
    "            images.append(image)  # Append the NumPy array to the list.\n",
    "\n",
    "        all_data= np.stack(images, axis= 0)\n",
    "        all_data[all_data < .0000001] = 0\n",
    "        X=all_data[:,:14,:,:] # separate out the band values\n",
    "        X = np.transpose(X, axes=[0, 2, 3, 1])\n",
    "        # normalize values of the input data to 0,1\n",
    "        X = X/X.max(axis=(3),keepdims=1)\n",
    "        # For RGB uncomment this\n",
    "        X = X[:,:,:,:3]\n",
    "        \n",
    "        # canopy_height,tree/not tree,ndvi\n",
    "        all_data= np.stack( images, axis= 0)\n",
    "        Y = all_data[:,14:]\n",
    "        Y[Y  < .0000001] = 0\n",
    "        #Y[:,0][Y[:,0]  > 1] = 1\n",
    "        Y=Y[:,0]\n",
    "        Y = Y/Y.max()\n",
    "        self.X=X\n",
    "        self.Y=Y\n",
    "\n",
    "        return X, Y\n",
    "    \n",
    "    def get_true_values_x(self, indexes):\n",
    "        'Generate one batch of data'\n",
    "        # Return validation data for plotting \n",
    "\n",
    "        # Find list of IDs - (give input of validation indexes)\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        images = []\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            dataset = gdal.Open(ID)\n",
    "            image = dataset.ReadAsArray()  # Returned image is a NumPy array with shape (16, 60, 60) for example.\n",
    "            images.append(image)  # Append the NumPy array to the list.\n",
    "\n",
    "        all_data= np.stack(images, axis= 0)\n",
    "        all_data[all_data < .0000001] = 0\n",
    "        X=all_data[:,:14,:,:] # separate out the band values\n",
    "        X = np.transpose(X, axes=[0, 2, 3, 1])\n",
    "        \n",
    "        # only need to return the RGB data for plotting\n",
    "        X = X[:,:,:,0:3]\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def get_true_values_y(self, indexes):\n",
    "        'Generate one batch of data'\n",
    "        # Return validation data for plotting \n",
    "\n",
    "        # Find list of IDs - (give input of validation indexes)\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, Y1 = self.__data_generation(list_IDs_temp)\n",
    "        y=[Y1]\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c61936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size':15,\n",
    "         'shuffle': True}\n",
    "\n",
    "# need a dictionary which has a list of training paths and a list of validations paths\n",
    "filelist_temp = filelist\n",
    "np.random.seed(14)\n",
    "mask = np.random.rand(len(filelist_temp)) <=.75\n",
    "\n",
    "training_data = np.array(filelist_temp)[mask]\n",
    "val_data = np.array(filelist_temp)[~mask]\n",
    "\n",
    "\n",
    "mydict = {}\n",
    "mydict[\"training\"] = training_data\n",
    "mydict[\"validation\"] = val_data\n",
    "\n",
    "# generators\n",
    "training_generator = DataGenerator(mydict[\"training\"], **params)\n",
    "val_generator = DataGenerator(mydict[\"validation\"], **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce446d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# define U-Net model architecture - proof_concept_2\n",
    "\n",
    "def build_unet(img_shape):\n",
    "    # input layer shape is equal to patch image size\n",
    "    inputs = layers.Input(shape=img_shape)\n",
    "\n",
    "    # rescale images from (0, 255) to (0, 1)\n",
    " #   rescale = Rescaling(scale=1. / 255, input_shape=(img_height, img_width, img_channels))(inputs)\n",
    " #   previous_block_activation = rescale  # Set aside residual\n",
    "    previous_block_activation = inputs\n",
    "\n",
    "    contraction = {}\n",
    "    # # Contraction path: Blocks 1 through 5 are identical apart from the feature depth\n",
    "    for f in [32, 64]:\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(previous_block_activation)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        contraction[f'conv{f}'] = x\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        previous_block_activation = x\n",
    "\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(previous_block_activation)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    contraction[f'conv{128}'] = x\n",
    "    x = layers.MaxPooling2D((3, 3))(x)\n",
    "    previous_block_activation = x\n",
    "        \n",
    "    c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(previous_block_activation)\n",
    "    c5 = layers.Dropout(0.2)(c5)\n",
    "    c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "    previous_block_activation = c5\n",
    "    \n",
    "    x = layers.Conv2DTranspose(128, (2, 2), strides=(3, 3), padding='same')(previous_block_activation)\n",
    "    x = layers.concatenate([x, contraction[f'conv{128}']])\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    previous_block_activation = x\n",
    "        \n",
    "    # Expansive path: Second half of the network: upsampling inputs\n",
    "    # could we use upsampling layers here instead of Conv2dTRanspose layers? might that help\n",
    "    for f in reversed([32, 64]):\n",
    "        x = layers.Conv2DTranspose(f, (2, 2), strides=(2, 2), padding='same')(previous_block_activation)\n",
    "        x = layers.concatenate([x, contraction[f'conv{f}']])\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        previous_block_activation = x\n",
    "\n",
    "    outputs = layers.Conv2D(filters=1, kernel_size=(1, 1), activation=\"linear\")(previous_block_activation)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f17ce9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 240, 240, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 240, 240, 32  896         ['input_3[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 240, 240, 32  0           ['conv2d_41[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 240, 240, 32  9248        ['dropout_20[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 120, 120, 32  0          ['conv2d_42[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 120, 120, 64  18496       ['max_pooling2d_6[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 120, 120, 64  0           ['conv2d_43[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 120, 120, 64  36928       ['dropout_21[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 60, 60, 64)  0           ['conv2d_44[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 60, 60, 128)  73856       ['max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 60, 60, 128)  0           ['conv2d_45[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 60, 60, 128)  147584      ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPooling2D)  (None, 20, 20, 128)  0          ['conv2d_46[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 20, 20, 256)  295168      ['max_pooling2d_8[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 20, 20, 256)  0           ['conv2d_47[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 20, 20, 256)  590080      ['dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_transpose_12 (Conv2DTra  (None, 60, 60, 128)  131200     ['conv2d_48[0][0]']              \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 60, 60, 256)  0           ['conv2d_transpose_12[0][0]',    \n",
      "                                                                  'conv2d_46[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 60, 60, 128)  295040      ['concatenate_12[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 60, 60, 128)  0           ['conv2d_49[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 60, 60, 128)  147584      ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_transpose_13 (Conv2DTra  (None, 120, 120, 64  32832      ['conv2d_50[0][0]']              \n",
      " nspose)                        )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 120, 120, 12  0           ['conv2d_transpose_13[0][0]',    \n",
      "                                8)                                'conv2d_44[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 120, 120, 64  73792       ['concatenate_13[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 120, 120, 64  0           ['conv2d_51[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 120, 120, 64  36928       ['dropout_25[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_14 (Conv2DTra  (None, 240, 240, 32  8224       ['conv2d_52[0][0]']              \n",
      " nspose)                        )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 240, 240, 64  0           ['conv2d_transpose_14[0][0]',    \n",
      "                                )                                 'conv2d_42[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 240, 240, 32  18464       ['concatenate_14[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 240, 240, 32  0           ['conv2d_53[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 240, 240, 32  9248        ['dropout_26[0][0]']             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 240, 240, 1)  33          ['conv2d_54[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,925,601\n",
      "Trainable params: 1,925,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model_unet_canopy_ht_all = build_unet(img_shape=(240, 240, 3))\n",
    "model_unet_canopy_ht_all.summary()\n",
    "\n",
    "# compile model\n",
    "model_unet_canopy_ht_all.compile(optimizer=\"adam\",\n",
    "              loss=\"mse\", \n",
    "              metrics=[\"mae\", 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86fb0635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "478/478 [==============================] - 927s 2s/step - loss: 0.0133 - mae: 0.0831 - accuracy: 0.0765 - val_loss: 0.0116 - val_mae: 0.0795 - val_accuracy: 0.0768\n",
      "Epoch 2/100\n",
      "478/478 [==============================] - 939s 2s/step - loss: 0.0128 - mae: 0.0819 - accuracy: 0.0765 - val_loss: 0.0117 - val_mae: 0.0778 - val_accuracy: 0.0768\n",
      "Epoch 3/100\n",
      "478/478 [==============================] - 932s 2s/step - loss: 0.0123 - mae: 0.0787 - accuracy: 0.0765 - val_loss: 0.0114 - val_mae: 0.0805 - val_accuracy: 0.0768\n",
      "Epoch 4/100\n",
      "478/478 [==============================] - 937s 2s/step - loss: 0.0115 - mae: 0.0754 - accuracy: 0.0765 - val_loss: 0.0110 - val_mae: 0.0726 - val_accuracy: 0.0768\n",
      "Epoch 5/100\n",
      "478/478 [==============================] - 941s 2s/step - loss: 0.0117 - mae: 0.0746 - accuracy: 0.0765 - val_loss: 0.0118 - val_mae: 0.0796 - val_accuracy: 0.0768\n",
      "Epoch 6/100\n",
      "478/478 [==============================] - 948s 2s/step - loss: 0.0107 - mae: 0.0712 - accuracy: 0.0765 - val_loss: 0.0100 - val_mae: 0.0680 - val_accuracy: 0.0769\n",
      "Epoch 7/100\n",
      "478/478 [==============================] - 951s 2s/step - loss: 0.0105 - mae: 0.0695 - accuracy: 0.0765 - val_loss: 0.0144 - val_mae: 0.0665 - val_accuracy: 0.0768\n",
      "Epoch 8/100\n",
      "478/478 [==============================] - 941s 2s/step - loss: 0.0105 - mae: 0.0693 - accuracy: 0.0765 - val_loss: 0.0098 - val_mae: 0.0637 - val_accuracy: 0.0768\n",
      "Epoch 9/100\n",
      "478/478 [==============================] - 942s 2s/step - loss: 0.0100 - mae: 0.0670 - accuracy: 0.0765 - val_loss: 0.0111 - val_mae: 0.0784 - val_accuracy: 0.0768\n",
      "Epoch 10/100\n",
      "478/478 [==============================] - 942s 2s/step - loss: 0.0101 - mae: 0.0672 - accuracy: 0.0765 - val_loss: 0.0098 - val_mae: 0.0686 - val_accuracy: 0.0767\n",
      "Epoch 11/100\n",
      "478/478 [==============================] - 932s 2s/step - loss: 0.0094 - mae: 0.0633 - accuracy: 0.0765 - val_loss: 0.0087 - val_mae: 0.0595 - val_accuracy: 0.0768\n",
      "Epoch 12/100\n",
      "478/478 [==============================] - 942s 2s/step - loss: 0.0092 - mae: 0.0630 - accuracy: 0.0765 - val_loss: 0.0087 - val_mae: 0.0598 - val_accuracy: 0.0768\n",
      "Epoch 13/100\n",
      "478/478 [==============================] - 937s 2s/step - loss: 0.0092 - mae: 0.0627 - accuracy: 0.0765 - val_loss: 0.0088 - val_mae: 0.0565 - val_accuracy: 0.0768\n",
      "Epoch 14/100\n",
      "478/478 [==============================] - 931s 2s/step - loss: 0.0086 - mae: 0.0603 - accuracy: 0.0765 - val_loss: 0.0090 - val_mae: 0.0546 - val_accuracy: 0.0768\n",
      "Epoch 15/100\n",
      "478/478 [==============================] - 947s 2s/step - loss: 0.0081 - mae: 0.0574 - accuracy: 0.0765 - val_loss: 0.0077 - val_mae: 0.0570 - val_accuracy: 0.0768\n",
      "Epoch 16/100\n",
      "478/478 [==============================] - 943s 2s/step - loss: 0.0084 - mae: 0.0586 - accuracy: 0.0765 - val_loss: 0.0085 - val_mae: 0.0616 - val_accuracy: 0.0768\n",
      "Epoch 17/100\n",
      "478/478 [==============================] - 941s 2s/step - loss: 0.0080 - mae: 0.0572 - accuracy: 0.0765 - val_loss: 0.0078 - val_mae: 0.0560 - val_accuracy: 0.0768\n",
      "Epoch 18/100\n",
      "478/478 [==============================] - 952s 2s/step - loss: 0.0080 - mae: 0.0570 - accuracy: 0.0765 - val_loss: 0.0079 - val_mae: 0.0558 - val_accuracy: 0.0768\n",
      "Epoch 19/100\n",
      "478/478 [==============================] - 953s 2s/step - loss: 0.0075 - mae: 0.0546 - accuracy: 0.0765 - val_loss: 0.0083 - val_mae: 0.0610 - val_accuracy: 0.0768\n",
      "Epoch 20/100\n",
      "478/478 [==============================] - 961s 2s/step - loss: 0.0078 - mae: 0.0557 - accuracy: 0.0765 - val_loss: 0.0073 - val_mae: 0.0517 - val_accuracy: 0.0767\n",
      "Epoch 21/100\n",
      "478/478 [==============================] - 968s 2s/step - loss: 0.0084 - mae: 0.0589 - accuracy: 0.0765 - val_loss: 0.0083 - val_mae: 0.0554 - val_accuracy: 0.0768\n",
      "Epoch 22/100\n",
      "478/478 [==============================] - 970s 2s/step - loss: 0.0079 - mae: 0.0562 - accuracy: 0.0765 - val_loss: 0.0077 - val_mae: 0.0551 - val_accuracy: 0.0768\n",
      "Epoch 23/100\n",
      "478/478 [==============================] - 970s 2s/step - loss: 0.0073 - mae: 0.0536 - accuracy: 0.0765 - val_loss: 0.0072 - val_mae: 0.0527 - val_accuracy: 0.0768\n",
      "Epoch 24/100\n",
      "478/478 [==============================] - 967s 2s/step - loss: 0.0074 - mae: 0.0544 - accuracy: 0.0765 - val_loss: 0.0075 - val_mae: 0.0522 - val_accuracy: 0.0768\n",
      "Epoch 25/100\n",
      "478/478 [==============================] - 983s 2s/step - loss: 0.0072 - mae: 0.0529 - accuracy: 0.0765 - val_loss: 0.0077 - val_mae: 0.0524 - val_accuracy: 0.0768\n",
      "Epoch 26/100\n",
      "478/478 [==============================] - 967s 2s/step - loss: 0.0076 - mae: 0.0545 - accuracy: 0.0765 - val_loss: 0.0070 - val_mae: 0.0496 - val_accuracy: 0.0768\n",
      "Epoch 27/100\n",
      "478/478 [==============================] - 979s 2s/step - loss: 0.0075 - mae: 0.0540 - accuracy: 0.0765 - val_loss: 0.0072 - val_mae: 0.0517 - val_accuracy: 0.0768\n",
      "Epoch 28/100\n",
      "478/478 [==============================] - 971s 2s/step - loss: 0.0074 - mae: 0.0540 - accuracy: 0.0765 - val_loss: 0.0084 - val_mae: 0.0508 - val_accuracy: 0.0768\n",
      "Epoch 29/100\n",
      "478/478 [==============================] - 988s 2s/step - loss: 0.0072 - mae: 0.0528 - accuracy: 0.0765 - val_loss: 0.0073 - val_mae: 0.0517 - val_accuracy: 0.0768\n",
      "Epoch 30/100\n",
      "478/478 [==============================] - 982s 2s/step - loss: 0.0070 - mae: 0.0518 - accuracy: 0.0765 - val_loss: 0.0067 - val_mae: 0.0535 - val_accuracy: 0.0768\n",
      "Epoch 31/100\n",
      "478/478 [==============================] - ETA: 0s - loss: 0.0070 - mae: 0.0523 - accuracy: 0.0765"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:4\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\keras\\engine\\training.py:1445\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1432\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1433\u001b[0m       x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1434\u001b[0m       y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1443\u001b[0m       model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1444\u001b[0m       steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution)\n\u001b[1;32m-> 1445\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1447\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1450\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1454\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1457\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m   1458\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\keras\\engine\\training.py:1756\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1755\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 1756\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1757\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1758\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\envTHESIS\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train our model\n",
    "history=model_unet_canopy_ht_all.fit(\n",
    "    training_generator,\n",
    "    epochs=100,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callback\n",
    ")\n",
    "\n",
    "# model_unet_canopy_ht_all.save('C:/Users/johnf/Documents/UCL/thesis/code/models/height_model')\n",
    "\n",
    "# hist_df = pd.DataFrame(history.history)\n",
    "\n",
    "# hist_df\n",
    "# hist_df.to_csv('C:/Users/johnf/Documents/UCL/thesis/code/models/tree_height_hist.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "374aebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vegetation mask ###\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=25, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        images = []\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            dataset = gdal.Open(ID)\n",
    "            image = dataset.ReadAsArray()  # Returned image is a NumPy array with shape (16, 60, 60) for example.\n",
    "            images.append(image)  # Append the NumPy array to the list.\n",
    "\n",
    "        all_data= np.stack(images, axis= 0)\n",
    "        all_data[all_data < .0000001] = 0\n",
    "        X=all_data[:,:14,:,:] # separate out the band values\n",
    "        X = np.transpose(X, axes=[0, 2, 3, 1])\n",
    "        # normalize values of the input data to 0,1\n",
    "        X = X/X.max(axis=(3),keepdims=1)\n",
    "        # For RGB uncomment this\n",
    "        X = X[:,:,:,:3]\n",
    "        \n",
    "        # canopy_height,tree/not tree,ndvi\n",
    "        all_data= np.stack( images, axis= 0)\n",
    "        Y = all_data[:,14:]\n",
    "        Y[(Y < .0000001) & (Y >= 0) ] = 1\n",
    "        #Y[:,0][Y[:,0]  > 1] = 1\n",
    "        Y=Y[:,2] # 0 for height, 1 for tree/not, 2 for NDVI \n",
    "        Y[Y  >0 ] = 0\n",
    "        Y[Y  <0 ] = 1\n",
    "        #Y = Y/Y.max()\n",
    "        self.X=X\n",
    "        self.Y=Y\n",
    "\n",
    "        return X, Y\n",
    "    \n",
    "    def get_true_values_x(self, indexes):\n",
    "        'Generate one batch of data'\n",
    "        # Return validation data for plotting \n",
    "\n",
    "        # Find list of IDs - (give input of validation indexes)\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        \n",
    "        # only need to return the RGB data for plotting\n",
    "        X = X[:,:,:,0:3]\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def get_true_values_y(self, indexes):\n",
    "        'Generate one batch of data'\n",
    "        # Return validation data for plotting \n",
    "\n",
    "        # Find list of IDs - (give input of validation indexes)\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8566a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size':15,\n",
    "         'shuffle': True}\n",
    "\n",
    "# need a dictionary which has a list of training paths and a list of validations paths\n",
    "filelist_temp = filelist\n",
    "np.random.seed(14)\n",
    "mask = np.random.rand(len(filelist_temp)) <=.75\n",
    "\n",
    "training_data = np.array(filelist_temp)[mask]\n",
    "val_data = np.array(filelist_temp)[~mask]\n",
    "\n",
    "\n",
    "mydict = {}\n",
    "mydict[\"training\"] = training_data\n",
    "mydict[\"validation\"] = val_data\n",
    "\n",
    "# generators\n",
    "training_generator = DataGenerator(mydict[\"training\"], **params)\n",
    "val_generator = DataGenerator(mydict[\"validation\"], **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45907f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# define U-Net model architecture - proof_concept_2\n",
    "\n",
    "def build_unet(img_shape):\n",
    "    # input layer shape is equal to patch image size\n",
    "    inputs = layers.Input(shape=img_shape)\n",
    "\n",
    "    # rescale images from (0, 255) to (0, 1)\n",
    " #   rescale = Rescaling(scale=1. / 255, input_shape=(img_height, img_width, img_channels))(inputs)\n",
    " #   previous_block_activation = rescale  # Set aside residual\n",
    "    previous_block_activation = inputs\n",
    "\n",
    "    contraction = {}\n",
    "    # # Contraction path: Blocks 1 through 5 are identical apart from the feature depth\n",
    "    for f in [32, 64]:\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(previous_block_activation)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        contraction[f'conv{f}'] = x\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        previous_block_activation = x\n",
    "\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(previous_block_activation)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    contraction[f'conv{128}'] = x\n",
    "    x = layers.MaxPooling2D((3, 3))(x)\n",
    "    previous_block_activation = x\n",
    "        \n",
    "    c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(previous_block_activation)\n",
    "    c5 = layers.Dropout(0.2)(c5)\n",
    "    c5 = layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "    previous_block_activation = c5\n",
    "    \n",
    "    x = layers.Conv2DTranspose(128, (2, 2), strides=(3, 3), padding='same')(previous_block_activation)\n",
    "    x = layers.concatenate([x, contraction[f'conv{128}']])\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "    previous_block_activation = x\n",
    "        \n",
    "    # Expansive path: Second half of the network: upsampling inputs\n",
    "    # could we use upsampling layers here instead of Conv2dTRanspose layers? might that help\n",
    "    for f in reversed([32, 64]):\n",
    "        x = layers.Conv2DTranspose(f, (2, 2), strides=(2, 2), padding='same')(previous_block_activation)\n",
    "        x = layers.concatenate([x, contraction[f'conv{f}']])\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = layers.Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        previous_block_activation = x\n",
    "\n",
    "    outputs = layers.Conv2D(filters=1, kernel_size=(1, 1), activation=\"sigmoid\")(previous_block_activation)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22675f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 240, 240, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 240, 240, 32  896         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 240, 240, 32  0           ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 240, 240, 32  9248        ['dropout[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 120, 120, 32  0           ['conv2d_1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 120, 120, 64  18496       ['max_pooling2d[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 120, 120, 64  0           ['conv2d_2[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 120, 120, 64  36928       ['dropout_1[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 60, 60, 64)  0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 60, 60, 128)  73856       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 60, 60, 128)  0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 60, 60, 128)  147584      ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 20, 20, 128)  0          ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 20, 20, 256)  295168      ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 20, 20, 256)  0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 20, 20, 256)  590080      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 60, 60, 128)  131200     ['conv2d_7[0][0]']               \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 60, 60, 256)  0           ['conv2d_transpose[0][0]',       \n",
      "                                                                  'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 60, 60, 128)  295040      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 60, 60, 128)  0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 60, 60, 128)  147584      ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 120, 120, 64  32832      ['conv2d_9[0][0]']               \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 120, 120, 12  0           ['conv2d_transpose_1[0][0]',     \n",
      "                                8)                                'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 120, 120, 64  73792       ['concatenate_1[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 120, 120, 64  0           ['conv2d_10[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 120, 120, 64  36928       ['dropout_5[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 240, 240, 32  8224       ['conv2d_11[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 240, 240, 64  0           ['conv2d_transpose_2[0][0]',     \n",
      "                                )                                 'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 240, 240, 32  18464       ['concatenate_2[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 240, 240, 32  0           ['conv2d_12[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 240, 240, 32  9248        ['dropout_6[0][0]']              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 240, 240, 1)  33          ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,925,601\n",
      "Trainable params: 1,925,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "478/478 [==============================] - 1577s 3s/step - loss: 0.2281 - binary_crossentropy: 0.2281 - accuracy: 0.9036 - mean_io_u: 0.1256 - val_loss: 0.1669 - val_binary_crossentropy: 0.1669 - val_accuracy: 0.9345 - val_mean_io_u: 0.1264\n",
      "Epoch 2/20\n",
      "478/478 [==============================] - 931s 2s/step - loss: 0.1582 - binary_crossentropy: 0.1582 - accuracy: 0.9366 - mean_io_u: 0.1253 - val_loss: 0.1413 - val_binary_crossentropy: 0.1413 - val_accuracy: 0.9421 - val_mean_io_u: 0.1256\n",
      "Epoch 3/20\n",
      "478/478 [==============================] - 935s 2s/step - loss: 0.1391 - binary_crossentropy: 0.1391 - accuracy: 0.9432 - mean_io_u: 0.1244 - val_loss: 0.1273 - val_binary_crossentropy: 0.1273 - val_accuracy: 0.9485 - val_mean_io_u: 0.1253\n",
      "Epoch 4/20\n",
      "478/478 [==============================] - 936s 2s/step - loss: 0.1324 - binary_crossentropy: 0.1324 - accuracy: 0.9457 - mean_io_u: 0.1249 - val_loss: 0.1192 - val_binary_crossentropy: 0.1192 - val_accuracy: 0.9508 - val_mean_io_u: 0.1258\n",
      "Epoch 5/20\n",
      "478/478 [==============================] - 991s 2s/step - loss: 0.1254 - binary_crossentropy: 0.1254 - accuracy: 0.9481 - mean_io_u: 0.1250 - val_loss: 0.1156 - val_binary_crossentropy: 0.1156 - val_accuracy: 0.9525 - val_mean_io_u: 0.1278\n",
      "Epoch 6/20\n",
      "478/478 [==============================] - 933s 2s/step - loss: 0.1213 - binary_crossentropy: 0.1213 - accuracy: 0.9498 - mean_io_u: 0.1246 - val_loss: 0.1144 - val_binary_crossentropy: 0.1144 - val_accuracy: 0.9523 - val_mean_io_u: 0.1247\n",
      "Epoch 7/20\n",
      "478/478 [==============================] - 893s 2s/step - loss: 0.1189 - binary_crossentropy: 0.1189 - accuracy: 0.9507 - mean_io_u: 0.1247 - val_loss: 0.1132 - val_binary_crossentropy: 0.1132 - val_accuracy: 0.9532 - val_mean_io_u: 0.1251\n",
      "Epoch 8/20\n",
      "478/478 [==============================] - 903s 2s/step - loss: 0.1162 - binary_crossentropy: 0.1162 - accuracy: 0.9518 - mean_io_u: 0.1248 - val_loss: 0.1106 - val_binary_crossentropy: 0.1106 - val_accuracy: 0.9543 - val_mean_io_u: 0.1245\n",
      "Epoch 9/20\n",
      "478/478 [==============================] - 967s 2s/step - loss: 0.1138 - binary_crossentropy: 0.1138 - accuracy: 0.9526 - mean_io_u: 0.1248 - val_loss: 0.1066 - val_binary_crossentropy: 0.1066 - val_accuracy: 0.9558 - val_mean_io_u: 0.1281\n",
      "Epoch 10/20\n",
      "478/478 [==============================] - 949s 2s/step - loss: 0.1109 - binary_crossentropy: 0.1109 - accuracy: 0.9538 - mean_io_u: 0.1250 - val_loss: 0.1078 - val_binary_crossentropy: 0.1078 - val_accuracy: 0.9549 - val_mean_io_u: 0.1252\n",
      "Epoch 11/20\n",
      "478/478 [==============================] - 907s 2s/step - loss: 0.1098 - binary_crossentropy: 0.1098 - accuracy: 0.9543 - mean_io_u: 0.1257 - val_loss: 0.1069 - val_binary_crossentropy: 0.1069 - val_accuracy: 0.9557 - val_mean_io_u: 0.1353\n",
      "Epoch 12/20\n",
      "478/478 [==============================] - 910s 2s/step - loss: 0.1087 - binary_crossentropy: 0.1087 - accuracy: 0.9548 - mean_io_u: 0.1271 - val_loss: 0.1059 - val_binary_crossentropy: 0.1059 - val_accuracy: 0.9562 - val_mean_io_u: 0.1245\n",
      "Epoch 13/20\n",
      "478/478 [==============================] - 953s 2s/step - loss: 0.1062 - binary_crossentropy: 0.1062 - accuracy: 0.9557 - mean_io_u: 0.1275 - val_loss: 0.1018 - val_binary_crossentropy: 0.1018 - val_accuracy: 0.9575 - val_mean_io_u: 0.1245\n",
      "Epoch 14/20\n",
      "478/478 [==============================] - 973s 2s/step - loss: 0.1061 - binary_crossentropy: 0.1061 - accuracy: 0.9557 - mean_io_u: 0.1284 - val_loss: 0.1061 - val_binary_crossentropy: 0.1061 - val_accuracy: 0.9555 - val_mean_io_u: 0.1281\n",
      "Epoch 15/20\n",
      "478/478 [==============================] - 930s 2s/step - loss: 0.1041 - binary_crossentropy: 0.1041 - accuracy: 0.9566 - mean_io_u: 0.1278 - val_loss: 0.1003 - val_binary_crossentropy: 0.1003 - val_accuracy: 0.9584 - val_mean_io_u: 0.1298\n",
      "Epoch 16/20\n",
      "478/478 [==============================] - 926s 2s/step - loss: 0.1026 - binary_crossentropy: 0.1026 - accuracy: 0.9571 - mean_io_u: 0.1292 - val_loss: 0.1022 - val_binary_crossentropy: 0.1022 - val_accuracy: 0.9574 - val_mean_io_u: 0.1313\n",
      "Epoch 17/20\n",
      "478/478 [==============================] - 972s 2s/step - loss: 0.1015 - binary_crossentropy: 0.1015 - accuracy: 0.9576 - mean_io_u: 0.1292 - val_loss: 0.0973 - val_binary_crossentropy: 0.0973 - val_accuracy: 0.9594 - val_mean_io_u: 0.1309\n",
      "Epoch 18/20\n",
      "478/478 [==============================] - 974s 2s/step - loss: 0.1010 - binary_crossentropy: 0.1010 - accuracy: 0.9578 - mean_io_u: 0.1301 - val_loss: 0.0973 - val_binary_crossentropy: 0.0973 - val_accuracy: 0.9594 - val_mean_io_u: 0.1314\n",
      "Epoch 19/20\n",
      "478/478 [==============================] - 954s 2s/step - loss: 0.0993 - binary_crossentropy: 0.0993 - accuracy: 0.9585 - mean_io_u: 0.1282 - val_loss: 0.0996 - val_binary_crossentropy: 0.0996 - val_accuracy: 0.9585 - val_mean_io_u: 0.1295\n",
      "Epoch 20/20\n",
      "478/478 [==============================] - 930s 2s/step - loss: 0.0988 - binary_crossentropy: 0.0988 - accuracy: 0.9586 - mean_io_u: 0.1279 - val_loss: 0.0961 - val_binary_crossentropy: 0.0961 - val_accuracy: 0.9599 - val_mean_io_u: 0.1272\n",
      "CPU times: total: 2h 34min 56s\n",
      "Wall time: 5h 24min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# build model\n",
    "model_unet_ndvi_ht_all = build_unet(img_shape=(240, 240, 3))\n",
    "model_unet_ndvi_ht_all.summary()\n",
    "\n",
    "# compile model\n",
    "\n",
    "\n",
    "# model_unet_ndvi_ht_all.compile(optimizer=\"adam\",\n",
    "#               loss=\"mse\", \n",
    "#               metrics=[\"mae\", 'accuracy'])\n",
    "model_unet_ndvi_ht_all.compile(optimizer=\"adam\",\n",
    "              loss=\"binary_crossentropy\", \n",
    "              metrics=[tf.keras.metrics.BinaryCrossentropy(),'accuracy',tf.keras.metrics.MeanIoU(num_classes=2)])\n",
    "\n",
    "\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train our model\n",
    "history=model_unet_ndvi_ht_all.fit(\n",
    "    training_generator,\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callback\n",
    ")\n",
    "\n",
    "# model_unet_ndvi_ht_all.save('C:/Users/johnf/Documents/UCL/thesis/code/models/vegetation_model')\n",
    "\n",
    "# hist_df = pd.DataFrame(history.history)\n",
    "\n",
    "# hist_df\n",
    "# hist_df.to_csv('C:/Users/johnf/Documents/UCL/thesis/code/models/vegetation_hist.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db403b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc8e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9af72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d251ec9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babd5e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
